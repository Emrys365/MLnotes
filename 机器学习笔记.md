![封面](封面.png)

<div STYLE="page-break-after: always;"></div>

#机器学习笔记

[TOC]

<div STYLE="page-break-after: always;"></div>

##一、线性代数相关

###1.矩阵的求导 

|               表达形式               |             公式$\quad$`分子布局`              |
| :------------------------------: | :--------------------------------------: |
| $\dfrac{\partial向量}{\partial标量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\qquad\dfrac{\partial \vec{y}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x}\\\dfrac{\partial y_2}{\partial x}\\...\\\dfrac{\partial y_m}{\partial x} \end{pmatrix}$ |
| $\dfrac{\partial标量}{\partial向量}$ | $\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\qquad\dfrac{\partial y}{\partial \vec{x}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_1}&\dfrac{\partial y}{\partial x_2}&...&\dfrac{\partial y}{\partial x_n} \end{pmatrix}$ |
|         标量函数关于空间向量的方向导数          | $\nabla_{\vec{u}}f(x)\mathop{=}\limits^{\triangle}\nabla f(x)\cdot{\vec{u}}=\dfrac{\partial f(x)}{\partial x}\cdot{\vec{u}}$ |
| $\dfrac{\partial向量}{\partial向量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\quad\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\\\dfrac{\partial \vec{y}}{\partial \vec{x}}=\begin{pmatrix}\dfrac{\partial y_1}{\partial \vec{x}}\\\dfrac{\partial y_2}{\partial \vec{x}}\\...\\\dfrac{\partial y_m}{\partial \vec{x}}\end{pmatrix}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x_1}&\dfrac{\partial y_1}{\partial x_2}&...&\dfrac{\partial y_1}{\partial x_n}\\\dfrac{\partial y_2}{\partial x_1}&\dfrac{\partial y_2}{\partial x_2}&...&\dfrac{\partial y_2}{\partial x_n}\\...&...&...&...\\\dfrac{\partial y_m}{\partial x_1}&\dfrac{\partial y_m}{\partial x_2}&...&\dfrac{\partial y_m}{\partial x_n} \end{pmatrix}_{m\times{n}}$ |
| $\dfrac{\partial矩阵}{\partial标量}$ | $\dfrac{\partial \bf{Y}_{m\times{n}}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_{11}}{\partial x}&\dfrac{\partial y_{12}}{\partial x}&...&\dfrac{\partial y_{1n}}{\partial x}\\\dfrac{\partial y_{21}}{\partial x}&\dfrac{\partial y_{22}}{\partial x}&...&\dfrac{\partial y_{2n}}{\partial x}\\...&...&...&...\\\dfrac{\partial y_{m1}}{\partial x}&\dfrac{\partial y_{m2}}{\partial x}&...&\dfrac{\partial y_{mn}}{\partial x} \end{pmatrix}_{m\times{n}}$ |

<div STYLE="page-break-after: always;"></div>

|               表达形式               |             公式$\quad$`分子布局`              |
| :------------------------------: | :--------------------------------------: |
| $\dfrac{\partial标量}{\partial矩阵}$ | $\dfrac{\partial y}{\partial \bf{X}_{p\times{q}}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_{11}}&\dfrac{\partial y}{\partial x_{21}}&...&\dfrac{\partial y}{\partial x_{p1}}\\\dfrac{\partial y}{\partial x_{12}}&\dfrac{\partial y}{\partial x_{22}}&...&\dfrac{\partial y}{\partial x_{p2}}\\...&...&...&...\\\dfrac{\partial y}{\partial x_{1q}}&\dfrac{\partial y}{\partial x_{2q}}&...&\dfrac{\partial y}{\partial x_{pq}} \end{pmatrix}_{q\times{p}}\\\quad\\通常写作\nabla_{\bf{X}}y(\bf{X})=\dfrac{\partial y(\bf{X})}{\partial \bf{X}}$ |
|          标量函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}f=tr\pmatrix{\dfrac{\partial f}{\partial \bf{X}}\bf{Y}}$ |
| $\dfrac{\partial矩阵}{\partial矩阵}$ | $\dfrac{\partial \bf{F}_{p\times{q}}}{\partial \bf{X}_{n\times{m}}}=\begin{pmatrix} \dfrac{\partial \bf{F}}{\partial \bf{X_{1,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,n}}}\\\dfrac{\partial \bf{F}}{\partial \bf{X_{2,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,n}}}\\...&...&...&...\\\dfrac{\partial \bf{F}}{\partial \bf{X_{m,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,n}}} \end{pmatrix}_{mp\times{nq}}\\\quad\\其中每一项\dfrac{\partial \bf{F}}{\partial \bf{X_{i,j}}}都是p\times{q}的矩阵$ |
|          矩阵函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}\bf{F}=tr\pmatrix{\dfrac{\partial \bf{F}}{\partial \bf{X}}\bf{Y}}$ |

#### · 通用公式

$\quad(\bf{X}、\bf{Y}均为矩阵)$

> $\dfrac{\partial \bf{X}}{\partial \bf{X}}={\bf{I}}$
>
> $\dfrac{\partial a{\bf{Y}}}{\partial \bf{X}}=a\dfrac{\partial {\bf{Y}}}{\partial \bf{X}}\quad(a不是\bf{X}的函数)$
>
> $\dfrac{\partial f(g(u))}{\partial{\bf{X}}}=\dfrac{\partial f(g)}{\partial g}\dfrac{\partial g(u)}{\partial u}\dfrac{\partial u}{\partial{\bf{X}}}\quad(f,g,u均为标量函数)$
>
> $\dfrac{\partial^2 \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}^2}={\bf{A}}+{\bf{A}}^T\quad({\bf{A}}不是\vec{x}的函数，\vec{x}^T{\bf{A}}\vec{x}为标量)$
>
> $\dfrac{\partial\vec{u}^T}{\partial x}=\pmatrix{\dfrac{\partial \vec{u}}{\partial x}}^T$

`分子布局`

$\qquad$根据$\bf Y$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad$根据${\bf X}^T$来布局$\dfrac{\partial y}{\partial\bf{X}}$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为列向量，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为行向量

`分母布局`

$\qquad$根据${\bf Y}^T$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad​$根据$\bf{X}​$来布局$\dfrac{\partial y}{\partial\bf{X}}​$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为列向量，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为行向量

|          分子布局(Numerator Layout)          |         分母布局(Denominator Layout)         |
| :--------------------------------------: | :--------------------------------------: |
| $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ m\times{n}$ | $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ n\times{m}$ |
| $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\vec{u}\dfrac{\partial a}{\partial\vec{x}}$ | $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\dfrac{\partial a}{\partial\vec{x}}\vec{u}^T$ |
| $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}={\bf{A}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}({\bf{A}}_{p\times{q}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}{\bf{A}}^T$<br>$\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\vec{u}^T\dfrac{\partial \vec{v}}{\partial \vec{x}}+\vec{v}^T\dfrac{\partial \vec{u}}{\partial \vec{x}}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ | $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\dfrac{\partial \vec{v}}{\partial \vec{x}}\vec{u}+\dfrac{\partial \vec{u}}{\partial \vec{x}}\vec{v}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ |
| $\dfrac{\partial\vec{u}^T{\bf{A}}{\vec{v}}}{\partial \vec{x}}=\vec{u}^T{\bf{A}}\dfrac{\partial \vec{v}}{\partial \vec{x}}+\vec{v}^T{\bf{A}}^T\dfrac{\partial \vec{u}}{\partial \vec{x}}$<br>$\color{blueviolet}(\vec{u}_{m\times{1}}是\vec{x}的函数，\vec{v}_{n\times{1}}是\vec{x}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}的函数)$ | $\dfrac{\partial\vec{u}^T{\bf{A}}{\vec{v}}}{\partial \vec{x}}=\dfrac{\partial \vec{v}}{\partial \vec{x}}{\bf{A}}^T\vec{u}+\dfrac{\partial \vec{u}}{\partial \vec{x}}{\bf{A}}\vec{v}$<br>$\color{blueviolet}(\vec{u}^T\vec{v}是标量，{\bf{A}}不是\vec{x}的函数)$ |
| $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=\vec{x}^T({\bf{A}}+{\bf{A}}^T)\quad\color{blueviolet}({\bf{A}}_{n\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=({\bf{A}}+{\bf{A}}^T)\vec{x}\quad\color{blueviolet}({\bf{A}}_{n\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |

<div STYLE="page-break-after: always;"></div>

|          分子布局(Numerator Layout)          |         分母布局(Denominator Layout)         |
| :--------------------------------------: | :--------------------------------------: |
| $\dfrac{\partial \vec{b}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=\vec{b}^T{\bf{A}}$<br>$\color{blueviolet}(\vec{b}_{m\times{1}}不是\vec{x}_{n\times{1}}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{b}^T{\bf{A}}\vec{x}}{\partial \vec{x}}={\bf{A}}^T\vec{b}$<br>$\color{blueviolet}(\vec{b}_{m\times{1}}不是\vec{x}_{n\times{1}}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial \vec{x}^T\vec{x}}{\partial \vec{x}}=2\vec{x}^T$ | $\dfrac{\partial \vec{x}^T\vec{x}}{\partial \vec{x}}=2\vec{x}$ |
| $\dfrac{\partial \vec{a}^T\vec{x}\vec{x}^T\vec{b}}{\partial \vec{x}}=\vec{x}^T(\vec{a}\vec{b}^T+\vec{b}\vec{a}^T)$<br>$\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数，\vec{b}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{a}^T\vec{x}\vec{x}^T\vec{b}}{\partial \vec{x}}=(\vec{a}\vec{b}^T+\vec{b}\vec{a}^T)\vec{x}$<br>$\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数，\vec{b}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\ \Vert\vec{x}-\vec{a}\Vert}{\partial \vec{x}}=\dfrac{(\vec{x}-\vec{a})^T}{\Vert\vec{x}-\vec{a}\Vert}\quad\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\ \Vert\vec{x}-\vec{a}\Vert}{\partial \vec{x}}=\dfrac{\vec{x}-\vec{a}}{\Vert\vec{x}-\vec{a}\Vert}\quad\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\ \vec{x}^T\mathbf{A}\vec{y}}{\partial\mathbf{A}}=\vec{y}\vec{x}^T$<br>$\color{blueviolet}(\vec{x}_{m\times{1}}和\vec{y}_{n\times{1}}不是{\bf{A}}_{m\times{n}}的函数)$ | $\dfrac{\partial\ \vec{x}^T\mathbf{A}\vec{y}}{\partial\mathbf{A}}=\vec{x}\vec{y}^T$<br>$\color{blueviolet}(\vec{x}_{m\times{1}}和\vec{y}_{n\times{1}}不是{\bf{A}}_{m\times{n}}的函数)$ |

对${\bf{f}}({\bf{X}}):\mathbb{R}^{m\times{n}}\rightarrow\mathbb{R}^{p\times{q}}$求导**不存在**与标量函数完全等同的乘积法则$(f\cdot{g})'=f'\cdot{g}+f\cdot{g'}$和链式法则$\dfrac{dz}{dx}=\dfrac{dz}{dy}\cdot{\dfrac{dy}{dx}}$

> *$\ $维度相容原则$\quad$`Trick`
>
> 通过**前后换序**、**转置** 使求导结果满足矩阵乘法且结果维数满足下式，那么结果就是正确的：
>
> $\qquad如果\vec{x}\in \mathbb{R}^{m\times{n}},f(\vec{x})\in \mathbb{R}^1，那么\dfrac{\partial f(\vec{x})}{\partial\vec{x}}\in \mathbb{R}^{m\times{n}}$

###2. 雅各比矩阵(Jacobian Matrix)

> $\vec{f}(\vec{x}):\mathbb{R}^n\rightarrow\mathbb{R}^m是向量，那么其雅各比矩阵{\bf{J}}是一个m\times{n}的矩阵，定义如下:$

$\quad{\bf{J}}=\begin{pmatrix}\dfrac{\partial\bf{f}}{\partial x_1}&\dfrac{\partial\bf{f}}{\partial x_2}&...&\dfrac{\partial\bf{f}}{\partial x_n}\end{pmatrix}=\begin{pmatrix}\dfrac{\partial f_1}{\partial x_1}&\dfrac{\partial f_1}{\partial x_2}&...&\dfrac{\partial f_1}{\partial x_n}\\\dfrac{\partial f_2}{\partial x_1}&\dfrac{\partial f_2}{\partial x_2}&...&\dfrac{\partial f_2}{\partial x_n}\\...&...&...&...\\\dfrac{\partial f_m}{\partial x_1}&\dfrac{\partial f_m}{\partial x_2}&...&\dfrac{\partial f_m}{\partial x_n}\end{pmatrix}$

$\quad{\bf{J}}_{i,j}=\dfrac{\partial f_i}{\partial x_j}$

> 如果$\vec{f}$在$\vec{x}$点处可微，则雅各比矩阵${\bf{J}}$定义了一个线性映射$\mathbb{R}^n\rightarrow\mathbb{R}^m$，这是对函数$\vec{f}$在$\vec{x}$点处最优（逐点的）线性估计，被称作$\vec{f}$在$\vec{x}$点处的导数或微分

如果$m=n$，那么雅各比矩阵${\bf{J}}$为一个方阵，其行列式称为**雅各比行列式 (Jacobian Determinant)**

+ 当且仅当雅各比行列式在$\vec{x}$点处不等于零时，函数$\vec{f}$在$\vec{x}$点附近有反函数 (inverse function)`反函数定理(Inverse function theorem)`
+ 若雅各比行列式的值为正，则$\vec{f}$在$\vec{x}$点附近的取向不变；若为负，则取向相反
+ 若雅各比行列式恒等于零，则函数组$\pmatrix{f_1&f_2&...&f_m}$是函数相关的，其中至少有一个函数是其余函数的一个连续可微的函数

###3. 海森矩阵(Hessian Matrix)

> $f(\vec{x}):\mathbb{R}^n\rightarrow\mathbb{R}是标量，如果f的所有二阶导数存在且在函数定义域内连续，则f的海森矩阵{\bf{H}}是一个n\times{n}的方阵，定义如下:$

$\quad{\bf{H}}=\begin{pmatrix}\dfrac{\partial^2f}{\partial x_1^2}&\dfrac{\partial^2f}{\partial x_1\partial x_2}&...&\dfrac{\partial^2f}{\partial x_1\partial x_n}\\\dfrac{\partial^2f}{\partial x_2\partial x_1}&\dfrac{\partial^2f}{\partial x_2^2}&...&\dfrac{\partial^2f}{\partial x_2\partial x_n}\\...&...&...&...\\\dfrac{\partial^2f}{\partial x_n\partial x_1}&\dfrac{\partial^2f}{\partial x_n\partial x_2}&...&\dfrac{\partial^2f}{\partial x_n^2}\end{pmatrix}$

$\quad{\bf{H}}_{i,j}=\dfrac{\partial^2f}{\partial x_i\partial x_j}$

> https://www.yangzhou301.com/2016/03/14/826442654/
>
> 当且仅当对于定义域上任意自变量$x_0$的海森矩阵${\bf{H}}(x_0)$均为半正定矩阵时，函数$y=f(⋅)$是**凸函数(convex)**

### 4. 矩阵的迹(trace)

$\mbox{tr}(\mathbf{A}_n)=\sum\limits_{i=1}^na_{ii}=\sum\limits_{i=1}^n\lambda_{i}$

其中$\lambda_i$是n阶方阵$\mathbf{A}$的[特征值](#6. 矩阵的特征值(eigenvalue)与特征向量(eigenvector))



### 5. 矩阵的秩(rank)



### 6. 矩阵的特征值(eigenvalue)与特征向量(eigenvector)

设$\mathbf{A}$为n阶**方阵**，若存在**非零**n维向量$\vec{x}$和常数$\lambda$使

$\qquad\mathbf{A}\vec{x}=\lambda\vec{x}$

成立，则称$\lambda$为$\mathbf{A}$的**特征值**，$\vec{x}$为$\mathbf{A}$的**特征向量**

$\quad{\color{maroon}\mathop{\Longrightarrow}\limits^{变形}}\ (\lambda\mathbf{I}-\mathbf{A})\vec{x}=\vec{0}$

$\qquad$上述齐次线性方程组有非零解的充要条件是$f(\lambda)=|\lambda\mathbf{I}-\mathbf{A}|=0$，称$f(\lambda)$为$\mathbf{A}$的**特征多项式**

$\qquad f(\lambda)$是一个n次多项式，在复数域内必有n个根（可能有重根），它们就是$\mathbf{A}$的全部特征值

$\qquad\color{maroon}\Longrightarrow\ $n阶方阵在复数域内有n个特征值$\lambda_1,\lambda_2,...,\lambda_n$

$\qquad\qquad\left\{\begin{array}{lcl}\lambda_1\lambda_2\cdot\cdot\cdot\lambda_n=|\mathbf{A}|\\\sum\limits_{i=1}^n\lambda_i=\sum\limits_{i=1}^na_{ii}=\mbox{tr}(\mathbf{A})\end{array}\right.$

> + $\mathbf{A}$的一个特征值对应无穷多个特征向量
>
> + 不可逆方阵必有零特征值，即$\lambda=0$
>
> + $\mathbf{A}$的不同特征值对应的特征向量是线性无关的
>
> + 对角矩阵的特征值是其对角线上的各个元素
>
> +  有n个不同的特征值$\;\;{\color{Bittersweet}\Longrightarrow}\;\;$必有n个线性无关的特征向量
>
>   反之不成立，如单位矩阵$\mathbf{I}_n$有n个相同的特征值1，但有n个线性无关的特征向量

特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多

#### · 特征值分解(Eigendecomposition)

$\mathbf{A}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}$

其中$\ \mathbf{\Lambda}$是对角阵（每个对角线元素就是一个特征值，且由大到小排列）

$\qquad\mathbf{Q}$是由$\mathbf{A}$的特征向量组成的矩阵（$\mathbf{Q}$的列向量是单位化的特征向量）

> 可用于数据降维
>
> + 最大特征值对应的特征向量方向上包含最多的信息量，如果某几个特征值很小，说明这几个方向信息量很小，可以用来降维，也就是删除小特征值对应方向的数据，只保留大特征值方向对应的数据，这样做以后数据量减小，但有用信息量变化不大

[如何理解矩阵特征值和特征向量？](http://www.matongxue.com/madocs/228.html#/madoc)

### 7. 可逆(invertible)矩阵

对于n阶方阵$\mathbf{A}$，若存在一个n阶方阵$\mathbf{B}$使得$\mathbf{A}\mathbf{B}=\mathbf{B}\mathbf{A}=\mathbf{I}_n$，则称$\mathbf{A}$为可逆/非奇异(nonsingular)矩阵，$\mathbf{B}$为$\mathbf{A}$的逆矩阵，即$\mathbf{A}^{-1}=\mathbf{B}$

+ 奇异(singular)矩阵

  `行列式为0的方阵`

  若$\mathbf{A}$为奇异矩阵，则方程$\mathbf{AX}=\mathbf{0}$有无穷组解

+ 非方阵(如$\mathbf{A}_{m\times{n}}$)不存在逆矩阵，但在有些情况下可能存在左逆矩阵(left inverse)$\mathbf{B}_{n\times{m}}$使$\mathbf{B}\mathbf{A}=\mathbf{I}_n$或右逆矩阵(right inverse)$\mathbf{B}_{n\times{m}}$使$\mathbf{A}\mathbf{B}=\mathbf{I}_m$

+ n阶方阵$\mathbf{A}$可逆$\ \Longleftrightarrow\ \mathbf{A}$的n个特征值均不为0

`性质`

> + 对于标量$k\neq0$，有$(kA)^{−1} = k^{−1}A^{−1} $
> + $\left(\mathbf{A}^T\right)^{-1}=\left(\mathbf{A}^{-1}\right)^T$

### 8. 正交(orthogonal)矩阵

若n阶实方阵$\mathbf{A}$满足$\mathbf{A}^T\mathbf{A}=\mathbf{I}$或$\mathbf{A}\mathbf{A}^T=\mathbf{I}$，则称其为正交矩阵

`性质`

> + $\mathbf{A}^T=\mathbf{A}^{-1}$也是正交矩阵
> + $\mathbf{A}$的各行是单位向量且两两正交
> + $\mathbf{A}$的各列是单位向量且两两正交
> + $|\mathbf{A}|=1\mbox{ or }-1$
> + 两个正交矩阵$\mathbf{A}$、$\mathbf{B}$的乘积矩阵$\mathbf{AB}$也是正交矩阵

### 9. 对称(symmetric)矩阵

若方阵$\mathbf{A}$满足$\mathbf{A}^T=\mathbf{A}$，则称其为对称矩阵

`性质`

> + 所有对角方阵都是对称矩阵
> + 当且仅当$\mathbf{A}\mathbf{B}=\mathbf{B}\mathbf{A}$时，方阵$\mathbf{A}$、$\mathbf{B}$的积$\mathbf{A}\mathbf{B}$为方阵
> + 任何对称矩阵$\mathbf{A}$可以通过一个正交矩阵$\mathbf{S}$转换为对角(diagonal)矩阵$\mathbf{B}$，即
>
> $\qquad\mathbf{B}=\begin{pmatrix}\lambda_1&0&...&0\\0&\lambda_2&...&0\\...&...&...&...\\0&0&...&\lambda_n\end{pmatrix}=\mathbf{S}\mathbf{A}\mathbf{S}^T$
>
> + 对称矩阵不同特征值对应的特征向量之间相互正交
> + 实对称矩阵的特征值都是实数

#### · 反对称(skew-symmetric)阵

若方阵$\mathbf{A}$满足$\mathbf{A}^T=-\mathbf{A}$，则称其为反对称矩阵

`性质`

> **主对角线上的元素全为零**，而位于主对角线两侧对称的元素反号
>
> 奇数阶反对称矩阵的行列式必为0
>
> 反对称矩阵的特征值是0或纯虚数，并且对应于纯虚数的特征向量的实部和虚部形成的实向量等长且互相正交

### 10. 正定(positive definite)矩阵

> 当对称的n阶实矩阵$\bf{M}$满足下列条件时称其为正定的：
>
> $\qquad\forall\vec{z}\neq\vec{0},\quad\vec{z}^T{\bf{M}}\vec{z}\gt0\qquad(\vec{z}\in\mathbb{R}^{n})$

+ 单位矩阵$\bf{I}$是正定的，因为$\vec{z}^T{\bf{I}}\vec{z}=|{\bf{I}}|^2\gt0$

+ 对于任意可逆实矩阵$\bf{A}$，其内积${\bf{A}}^T{\bf{A}}$是一个正定矩阵，因为$\vec{z}^T({\bf{A}}^T{\bf{A}})\vec{z}=|{\bf{A}}\vec{z}|^2\gt0$

+ 正定矩阵的对角元均大于0

  $\quad{\color{gray}\forall\vec{z}\neq\vec{0},\quad\vec{z}^T{\bf{A}}\vec{z}\gt0\quad(\vec{z}\in\mathbb{R}^{n})}\quad{\color{Bittersweet}\Rightarrow}\quad{\color{gray}取\vec{z}为仅第i个元素为1，其余元素为0的向量，则\vec{z}^T{\bf{A}}\vec{z}=a_{ii}\gt0}$

+ 正定矩阵的特征值均大于0

  $\quad{\color{gray}\mathbf{A}\vec{v}=\lambda\vec{v}}\quad{\color{Bittersweet}\Rightarrow}\quad{\color{gray}\vec{v}^T\mathbf{A}\vec{v}=\lambda\vec{v}^T\vec{v}\gt0}\quad{\color{Bittersweet}\Rightarrow}\quad{\color{gray}\lambda\gt0}$

+ 正定矩阵的行列式大于0

####· 半正定(positive semi-definite)矩阵

> 当对称的n阶实矩阵$\bf{M}$满足下列条件时称其为半正定的：
>
> $\qquad\forall\vec{z}\neq\vec{0},\quad\vec{z}^T{\bf{M}}\vec{z}\ge0\qquad(\vec{z}\in\mathbb{R}^{n})$

+ 半正定矩阵的对角元均非负

#### · 负定(negative definite)矩阵

> 当对称的n阶实矩阵$\bf{M}$满足下列条件时称其为负定的：
>
> $\qquad\forall\vec{z}\neq\vec{0},\quad\vec{z}^T{\bf{M}}\vec{z}\lt0\qquad(\vec{z}\in\mathbb{R}^{n})$

#### · 半负定(negative semi-definite)矩阵

> 当对称的n阶实矩阵$\bf{M}$满足下列条件时称其为半负定的：
>
> $\qquad\forall\vec{z}\neq\vec{0},\quad\vec{z}^T{\bf{M}}\vec{z}\le0\qquad(\vec{z}\in\mathbb{R}^{n})$

####· 不定(indefinite)矩阵

> 不属于以上四种情况的矩阵称为不定矩阵
>
> + 若实对称矩阵$\mathbf{A}$的主对角线上元素有正有负，则$\mathbf{A}$一定是不定矩阵

### 11. 二次型(Quadratic Form)

`定义`

> n元二次**齐次(homogeneous)**多项式，即多项式中所有非零项有相同的次数(=2)

一元(unary)：$q(x) = ax^2$

二元(binary)：$q(x,y) = ax^2 + bxy + cy^2$

三元(ternary)：$q(x,y,z) = ax^2 + by^2 + cz^2 + dxy + exz + fyz$

$\quad$其中$a,b,...,f$是系数

`性质`

> + 任何二次型都可以由一个实对称矩阵$\mathbf{A}$表示
>
>   $\qquad q_A(x_1,\ldots,x_n) = \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}a_{ij}{x_i}{x_j} = \mathbf{x}^T\mathbf{A} \mathbf x$
>
> + 符合平行四边形定律，即$q(x+y)+q(x-y)=2q(x)+2q(y)$
>
>   若$x$和$y$是向量且正交，则$q(x+y)=q(x)+q(y)$
>

###12. 求解线性方程组 

设线性方程组${\bf{A}}\vec{x}=\vec{b}$，其中${\bf{A}}$为已知的对称[正定](#4. 正定(positive definite)矩阵)方阵，$\vec{b}$为已知的列向量

> `理解矩阵A对于方程的影响`
>
> 对于二次方程$f(\vec{x})=\dfrac{1}{2}\vec{x}^T\mathbf{A}\vec{x}-\vec{b}^T\vec{x}+c$，它的导数为$f'(\vec{x})=\begin{pmatrix}\dfrac{\partial f}{\partial x_1}\\\dfrac{\partial f}{\partial x_2}\\...\\\dfrac{\partial f}{\partial x_n}\end{pmatrix}=\dfrac{1}{2}\mathbf{A}^T\vec{x}+\dfrac{1}{2}\mathbf{A}\vec{x}-\vec{b}$
>
> 若$\mathbf{A}$为[对称矩阵](#9. 对称(symmetric)矩阵)，则$f'(\vec{x})=\mathbf{A}\vec{x}-\vec{b}\qquad$`分母布局`
>
> 设点$\vec{x}$为方程${\bf{A}}\vec{x}=\vec{b}$的解，则$\vec{b}^T\vec{x}=\vec{x}^T\mathbf{A}\vec{x}$，$f(\vec{x})=-\dfrac{1}{2}\vec{x}^T\mathbf{A}\vec{x}+c$
>
> 若$\mathbf{A}$为对称矩阵，对于任意点$\vec{p}$，有
>
> $\qquad\begin{equation}\begin{aligned}f(\vec{p})&=f(\vec{x})+\dfrac{1}{2}\vec{p}^T\mathbf{A}\vec{p}-\vec{b}^T\vec{p}+\dfrac{1}{2}\vec{x}^T\mathbf{A}\vec{x}\\&=f(\vec{x})+\dfrac{1}{2}\vec{p}^T\mathbf{A}\vec{p}-\vec{x}^T\mathbf{A}\vec{p}+\dfrac{1}{2}\vec{x}^T\mathbf{A}\vec{x}\\&=f(\vec{x})+\dfrac{1}{2}\vec{p}^T\mathbf{A}\vec{p}-\dfrac{1}{2}\vec{x}^T\mathbf{A}\vec{p}-\dfrac{1}{2}\vec{p}^T\mathbf{A}\vec{x}+\dfrac{1}{2}\vec{x}^T\mathbf{A}\vec{x}\\&=f(\vec{x})+\dfrac{1}{2}(\vec{p}-\vec{x})^T\mathbf{A}(\vec{p}-\vec{x})\end{aligned}\end{equation}$
>
> - 若$\mathbf{A}$为对称[正定](#10. 正定(positive definite)矩阵)矩阵，则方程的解对应于$f(\vec{x})$的最小值
>
> - 若$\mathbf{A}$为对称[负定](#· 负定(negative definite)矩阵)矩阵，则方程的解对应于$f(\vec{x})$的最大值
>
>   ![正定矩阵与负定矩阵](正定矩阵与负定矩阵.png)
>
> - 若$\mathbf{A}$为[奇异](#7. 可逆(invertible)矩阵)正定矩阵，则方程的解对应于$f(\vec{x})$曲面峡谷底部的一条线
>
> - 若$\mathbf{A}$为[不定](#· 不定(indefinite)矩阵)矩阵，则方程的解对应于$f(\vec{x})$曲面上的一个鞍点(saddle point)，梯度下降和共轭梯度法都将失效。在三维或更高维时，奇异矩阵也可能有鞍点
>
> ![不定矩阵](不定矩阵.png)

Ⅰ$\quad\ $一般当**A**为低阶稠密矩阵时，用主元消去法解此方程组是有效方法

Ⅱ$\quad$当**A**为高阶稀疏矩阵时，利用迭代法求解更合适

> 迭代法通常在一个向量$\vec{x}$上不断应用矩阵$\mathbf{B}$，当$\mathbf{B}$反复作用于其特征向量$\vec{v}$时($\vec{x}=\vec{v}$)：
>
> - 若对应的特征值$|\lambda|\lt1$，则$\mathbf{B}^i\vec{v}=\lambda^i\vec{v}$将随着迭代次数$i$趋于无穷而消失(vanish)
> - 若对应的特征值$|\lambda|\gt1$，则$\mathbf{B}^i\vec{v}=\lambda^i\vec{v}$将随着迭代次数$i$趋于无穷而发散
> - 当$\mathbf{B}$有n个线性无关的特征向量时，它们就可以构成n维空间的一组基{$\vec{v}_1,\vec{v}_2,...,\vec{v}_n$}，即任何n维向量$\vec{x}$都可由这n个特征向量的线性组合表示，假设$\mathbf{B}^{i}\vec{x}=\mathbf{B}^{i}\vec{v}_1+\mathbf{B}^{i}\vec{v}_2+...+\mathbf{B}^{i}\vec{v}_n=\lambda^i_1\vec{v}_1+\lambda^i_2\vec{v}_2+...$$+\lambda^i_n\vec{v}_n$
>   + 只要所有特征值的幅值均小于1，$\vec{x}$将随着迭代次数$i$趋于无穷而收敛到0
>   + 若有一个特征值的幅值大于1，则$\vec{x}$将随着迭代次数$i$趋于无穷而发散

####· 雅各比迭代法(Jacobi Iterations)

将矩阵$\mathbf{A}$分解为对角矩阵$\mathbf{D}$和另一个矩阵$\mathbf{E}$：

$\qquad\mathbf{A}=\mathbf{D}+\mathbf{E}$

其中$\mathbf{D}$对角线上的元素与$\mathbf{A}$相同，$\mathbf{E}$对角线上元素均为0，其余部分与$\mathbf{A}$相同

$\qquad\mathbf{A}\vec{x}=\vec{b}\quad{\color{Bittersweet}\Rightarrow}\quad\mathbf{D}\vec{x}=-\mathbf{E}\vec{x}+\vec{b}\quad{\color{Bittersweet}\Rightarrow}\quad\vec{x}=-\mathbf{D}^{-1}\mathbf{E}\vec{x}+\mathbf{D}^{-1}\vec{b}$

$\qquad$令$\mathbf{B}=-\mathbf{D}^{-1}\mathbf{E}$，$\vec{z}=-\mathbf{D}^{-1}\vec{b}$，则$\vec{x}=\mathbf{B}\vec{x}+\vec{z}\qquad$（$\mathbf{D}^{-1}$易求）

$\quad{\color{maroon}\mathop{\Longrightarrow}\limits^{迭代公式}}\quad\vec{x}_{(i+1)}=\mathbf{B}\vec{x}_{(i)}+\vec{z}$

`收敛条件`$\quad$矩阵$\mathbf{A}$具有**严格**对角优势(strictly diagonally dominant)，即其每一行的非对角元的模之和都**小于**这一行的对角元的模$\quad{\color{Bittersweet}\Rightarrow}\quad\mathbf{A}$是非奇异矩阵

> 给定一个初始值$\vec{x}_{(0)}$，即可开始迭代，当迭代结果等于方程的解$\vec{x}$（驻点）时，由于$\vec{x}=\mathbf{B}\vec{x}+\vec{z}$，迭代的结果将不会变化
>
> 设第$i$次迭代的误差为$e_{(i)}$，则由$\vec{x}_{(i+1)}=\mathbf{B}\vec{x}_{(i)}+\vec{z}=\mathbf{B}\left(\vec{x}+\vec{e}_{(i)}\right)+\vec{z}=\mathbf{B}\vec{x}+\vec{z}+\mathbf{B}\vec{e}_{(i)}=\vec{x}+\mathbf{B}\vec{e}_{(i)}$可得，$e_{(i+1)}=\mathbf{B}e_{(i)}$
>
> 那么只要$\mathbf{B}$的特征值$|\lambda|$均小于1，那么误差将随着不断迭代收敛到0

####· 最速下降法( Steepest Descent)

> 每一步迭代的方向都是$f$减少最快的方向，即$f'(\vec{x}_{(i)})$相反的方向：$-f'(\vec{x}_{(i)})=b-\mathbf{A}\vec{x}_{(i)}$

定义误差向量$\vec{e}_{(i)}=\vec{x}_{(i)}-\vec{x}$表示距离方程的解有多远，**残差(residual)**向量$\vec{r}_{(i)}=b-\mathbf{A}\vec{x}_{(i)}=-f'(\vec{x}_{(i)})$是**最速下降的方向**，同时也表示表示距离正确的b值有多远（对于线性问题）

$\qquad{\color{maroon}\Longrightarrow}\ \vec{r}_{(i)}=-\mathbf{A}\vec{e}_{(i)}$

`迭代公式`$\qquad\vec{x}_{(i+1)}=\vec{x}_{(i)}+\alpha_{(i)}\vec{r}_{(i)}\qquad{\color{maroon}\Longrightarrow}\quad$如何选取$\alpha_{(i)}$?

令$\dfrac{d}{d\alpha_{(i)}}f(\vec{x}_{(i)})=0\quad{\color{Bittersweet}\Rightarrow}\quad f'(\vec{x}_{(i)})^T\dfrac{d\vec{x}_{(i)}}{d\alpha_{(i)}}=0\quad{\color{Bittersweet}\Rightarrow}\quad f'(\vec{x}_{(i)})^T\vec{r}_{(i-1)}=0\qquad$`分子布局`

$\qquad\qquad\qquad\qquad\quad{\color{Bittersweet}\Rightarrow}\quad -\vec{r}_{(i)}^T\vec{r}_{(i-1)}=0\quad{\color{Bittersweet}\Rightarrow}\quad -\left(\vec{b}-\mathbf{A}\vec{x}_{(i)}\right)^T\vec{r}_{(i-1)}=0$

$\qquad\qquad\qquad\qquad\quad{\color{Bittersweet}\Rightarrow}\quad-\left(\vec{b}-\mathbf{A}\left(\vec{x}_{(i-1)}+\alpha_{(i-1)}\vec{r}_{(i-1)}\right)\right)^T\vec{r}_{(i-1)}=0$

$\qquad\qquad\qquad\qquad\quad{\color{Bittersweet}\Rightarrow}\quad-\left(\vec{b}-\mathbf{A}\vec{x}_{(i-1)}\right)^T\vec{r}_{(i-1)}+\alpha_{(i-1)}\left(\mathbf{A}\vec{r}_{(i-1)}\right)^T\vec{r}_{(i-1)}=0$

$\qquad\qquad\qquad\qquad\quad{\color{Bittersweet}\Rightarrow}\quad\left(\vec{b}-\mathbf{A}\vec{x}_{(i-1)}\right)^T\vec{r}_{(i-1)}=\alpha_{(i-1)}\left(\mathbf{A}\vec{r}_{(i-1)}\right)^T\vec{r}_{(i-1)}$

$\qquad\qquad\qquad\qquad\quad{\color{Bittersweet}\Rightarrow}\quad\vec{r}_{(i-1)}^T\vec{r}_{(i-1)}=\alpha_{(i-1)}\vec{r}_{(i-1)}^T\left(\mathbf{A}\vec{r}_{(i-1)}\right)$

$\qquad\qquad\qquad\qquad\quad{\color{Bittersweet}\Rightarrow}\quad\alpha_{(i)}=\dfrac{\vec{r}_{(i)}^T\vec{r}_{(i)}}{\vec{r}_{(i)}^T\mathbf{A}\vec{r}_{(i)}}$

> $\vec{x}_{(i+1)}=\vec{x}_{(i)}+\dfrac{\vec{r}_{(i)}^T\vec{r}_{(i)}}{\vec{r}_{(i)}^T\mathbf{A}\vec{r}_{(i)}}\vec{r}_{(i)}$
>
> $\vec{e}_{(i+1)}-\vec{e}_{(i)}=\vec{x}_{(i+1)}-\vec{x}_{(i)}=\alpha_{(i)}\vec{r}_{(i)}=\dfrac{\vec{r}_{(i)}^T\vec{r}_{(i)}}{\vec{r}_{(i)}^T\mathbf{A}\vec{r}_{(i)}}\vec{r}_{(i)}$

+ 收敛性分析

设误差向量$\vec{e}_{(i)}$是$\mathbf{A}$的特征向量，对应的特征值是$\lambda_e$，那么残差$\ \vec{r}_{(i)}=-\mathbf{A}\vec{e}_{(i)}=-\lambda_e\vec{e}_{(i)}$也是特征向量

 $\vec{e}_{(i+1)}=\vec{e}_{(i)}+\dfrac{\vec{r}_{(i)}^T\vec{r}_{(i)}}{\vec{r}_{(i)}^T\mathbf{A}\vec{r}_{(i)}}\vec{r}_{(i)}=\vec{e}_{(i)}+\dfrac{\vec{r}_{(i)}^T\vec{r}_{(i)}}{\lambda_e\vec{r}_{(i)}^T\vec{r}_{(i)}}\left(-\lambda_e\vec{e}_{(i)}\right)=0$

因此当误差项为$\mathbf{A}$的**特征向量**时，$\alpha=\dfrac{1}{\lambda_e}$，最速下降法仅需**一次迭代**即可收敛到最优解

###14. 协方差矩阵(covariance matrix)

> ####协方差(Covariance)
>
> 表示两个变量的总体误差
>
> $\left\{\begin{array}{lcl}如果两个随机变量的变化趋势一致，那么两个变量之间的协方差就是正值\\\\如果两个变量的变化趋势相反，那么两个随机变量之间的协方差就是负值\end{array}\right.$
>
> $\text{cov}(X,Y)=\text{E}\left[\left(X-\text{E}[X]\right)\left(Y-\text{E}[Y]\right)\right]=\text{E}[XY]-\text{E}[X]\text{E}[Y]$
>
> $\ \ \color{maroon}\Longrightarrow\ \ $对于离散随机变量，$\text{cov}(X,Y)=\dfrac{1}{m}\sum\limits_{i=1}^m\left(x_i-\text{E}[X]\right)\left(y_i-\text{E}[Y]\right)$
>
> $\qquad\quad\ $若$n$组随机变量的均值$\text{E}[X_i]\ (i=1,2,\cdots,n)$未知，可根据$m$组观察值计算协方差的估计值
> $\qquad\quad\ $变量$X_i,X_j$的协方差的估计值为$\ q_{ij}=\dfrac{1}{m-1}\sum\limits_{k=1}^m\left(X_{ki}-\overline{X}_i\right)\left(X_{kj}-\overline{X}_j\right)$
>
> `性质`
>
> 设$X,Y,Z,W,V$为实值随机变量，$a,b,c,d$为常数（非随机变量）
>
> 1. 当$X=Y$时，两者的协方差即为变量$X$的方差，即$\text{cov}(X,X)=\text{var}(X)$
> 2. $\text{cov}(X,Y)=\operatorname {cov}(Y,X)$
> 3. $\text{cov}(X,a)=0$
> 4. $\text{cov}(aX,bY)=ab\,\operatorname {cov}(X,Y)$
> 5. $\text{cov}(X+a,Y+b)=\operatorname {cov}(X,Y)$
> 6. $\text{cov}(aX + bY, Z) = a\,\operatorname {cov}(X, Z) + b\,\operatorname {cov}(Y, Z)$
> 7. $\operatorname {cov} (aX+bY,cW+dV)=ac\,\operatorname {cov} (X,W)+ad\,\operatorname {cov} (X,V)+bc\,\operatorname {cov} (Y,W)+bd\,\operatorname {cov} (Y,V)$

$n$维随机向量$\vec{X}$的协方差矩阵为$\Sigma=\begin{pmatrix}\text{cov}(X_1,X_1)&\text{cov}(X_1,X_2)&\cdots&\text{cov}(X_1,X_n)\\\text{cov}(X_2,X_1)&\text{cov}(X_2,X_2)&\cdots&\text{cov}(X_2,X_n)\\\cdots&\cdots&\cdots&\cdots\\\text{cov}(X_n,X_1)&\text{cov}(X_n,X_2)&\cdots&\text{cov}(X_n,X_n)\end{pmatrix}_{n\times{n}}$

$\quad\Sigma_{ij}=\text{cov}(X_i,X_j)=\text{E}[(X_i-\mu_i)(X_j-\mu_j)]=\text{E}[X_iX_j]-\mu_i\mu_j$

$\quad\Sigma=\text{E}\left[(\vec{X}-\text{E}[\vec{X}])(\vec{X}-\text{E}[\vec{X}])^T\right]=\text{E}[\vec{X}\vec{X}^T]-\vec\mu\vec\mu^T$

$\quad$其中$\mu_i=\text{E}[X_i],\mu_j=\text{E}[X_j],\vec\mu=\text{E}[\vec{X}]$

`性质`

设$\vec{X},\vec{X}_1,\vec{X}_2$为$p$维随机向量，$\vec{Y}$为$q$维随机向量，$\vec{a}$为$q\times{1}$的向量，$\vec{b}$为$p\times{1}$的向量，$\mathbf{A},\mathbf{B}$为$q\times{p}$的常量矩阵

1. $\text{cov}(\vec{X},\vec{Y})=\text{cov}(\vec{Y},\vec{X})^T$

2. 若$\vec{X}$的每一个随机变量都与$\vec{Y}$的随机变量不相关，则$\text{cov}(\vec{X},\vec{Y})=0$

3. 若$p=q$，则$\text{var}(\vec{X}+\vec{Y})=\text{var}(\vec{X})+\text{cov}(\vec{X},\vec{Y})+\text{cov}(\vec{Y},\vec{X})+\text{var}(\vec{Y})$

4. $\operatorname {cov} (\vec{X} _{1}+\vec{X} _{2},\vec{Y} )=\operatorname {cov} (\vec{X} _{1},\vec{Y} )+\operatorname {cov} (\vec{X} _{2},\vec{Y} )$

5. $\operatorname {cov} (\mathbf {A}\vec{X} +\vec{a} ,\mathbf {B} ^T\vec{Y} +\vec{b} )=\mathbf {A} \,\operatorname {cov} (\vec{X} ,\vec{Y} )\,\mathbf {B} $

6. 协方差矩阵一定是**对称**半正定矩阵

   $\begin{equation}\begin{aligned}对于任意向量\vec{y},有\ \ \vec{y}^T\Sigma\vec{y}&=\vec{y}^T\text{E}\left[(\vec{X}-\vec{\mu})(\vec{X}-\vec{\mu})^T\right]\vec{y}=\text{E}\left[\vec{y}^T(\vec{X}-\vec{\mu})(\vec{X}-\vec{\mu})^T\vec{y}\right]\\&=\text{E}\left[\left((\vec{X}-\vec{\mu})^T\vec{y}\right)^T\left((\vec{X}-\vec{\mu})^T\vec{y}\right)\right]=\text{E}\left[\Vert (\vec{X}-\vec{\mu})^T\vec{y}\Vert^2\right]\ge0\end{aligned}\end{equation}$

###15. 奇异值分解(Singular value decomposition, SVD)

设$\mathbf{M}$是一个$m\times{n}$的矩阵，它的每个元素属于数域$K$（$K=$实数域$\mathbb{R}$或复数域$\mathbb{C}$），那么$\mathbf{M}$的奇异值分解表示为：

$\qquad\mathbf{M}=\mathbf{U\Sigma V}^*$

其中$\ \mathbf{U}$$是m\times{m}$的[酉矩阵](#17. 酉矩阵(Unitary Matrix))（$K=\mathbb{R}$时，酉矩阵即为正交矩阵）

$\qquad\Sigma$是$m\times{n}$的对角矩阵，其对角线上元素均为非负实数

$\qquad\mathbf{V}$是$n\times{n}$的酉矩阵，$\mathbf{V}^*$是$\mathbf{V}$的[共轭转置](#16. 正规矩阵(Normal Matrix))

> `奇异值(sigular value)`
>
> 对于一个非负实数$\sigma$和矩阵$\mathbf{M}_{m\times{n}}$，若存在满足以下条件的单位向量$\vec{u}\in K^m$和$\vec{v}\in K^n$ ：
>
> $\qquad\mathbf {M} {\vec {v}}=\sigma {\vec {u}}\,{\text{ 和 }}\mathbf {M} ^{*}{\vec {u}}=\sigma {\vec {v}}$
>
> 则称$\sigma$为矩阵$\mathbf{M}$的**奇异值**
> $\vec{u}$称为$\mathbf{M}$的**左奇异向量(left-singular vector)**，$\vec{v}$称为$\mathbf{M}$的**右奇异向量(right-singular vector)**
>
> + $m\times{n}$的矩阵最多有$p=\min(m,n)$个不同的奇异值

$\Sigma$的对角线元素$\sigma_i$就是$\mathbf{M}$的奇异值，通常约定按降序排列

$\mathbf{U}$的前$p=\min(m,n)$列是$\mathbf{M}$对应奇异值的左奇异向量，$\mathbf{V}$的前$p=\min(m,n)$列是$\mathbf{M}$对应奇异值的右奇异向量

###16. 正规矩阵(Normal Matrix)

若复方阵(complex square matrix)$\mathbf{A}$满足$\mathbf{A}^*\mathbf{A}=\mathbf{A}\mathbf{A}^*$，则称$\mathbf{A}$为正规矩阵

其中$\mathbf{A}^*$为$\mathbf{A}$的共轭转置

> `共轭转置(Conjugate Transpose)`
>
> 对于复矩阵$\mathbf{A}_{m\times{n}}$，其共轭转置$\mathbf{A}^*_{n\times{m}}=\overline{\mathbf{A}}^T=\overline{\mathbf{A}^T}$，即对$\mathbf{A}$取转置之后再对每个元素取其共轭
>
> 对于实方阵$\mathbf{A}$，有$\mathbf{A}^*=\mathbf{A}^T$



###17. 酉矩阵(Unitary Matrix)



<div STYLE="page-break-after: always;"></div>

##二、线性回归(Linear Regression)

###1. 模型表示

​	假设函数

​		$ h_θ(x)=θ_0+θ_1x\qquad\qquad\qquad\qquad\qquad$`单个特征`

​		$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\quad\ $`多个特征`


![线性回归](线性回归.jpg)

###2. 代价函数(Cost function)

   $J(\theta_0,\theta_1)=\dfrac{1}{2m}\sum_{i=1}\limits^n(h_\theta(x^{(i)})^2-y^{(i)})^2\quad$`最小二乘代价函数`

   + [为什么线性回归使用最小二乘代价函数$J(\theta_0,\theta_1)?$](http://blog.csdn.net/Eric2016_Lv/article/details/52836182?locationNum=3&fps=1)

###3. 最小二乘法(LSE)

   $\hat{y}=\hat{a}+\hat{b}x$

   $\hat{b}=\dfrac{\sum_{i=1}\limits^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}\limits^n(x_i-\bar{x})^2}=\dfrac{\sum_{i=1}\limits^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}\limits^nx_i^2-n\bar{x}^2}$

   $\hat{a}=\bar{y}-\hat{b}\bar{x}$

   $其中\bar{x}、\bar{y}为x_i、y_i的均值$

   + [ 求回归直线方程的推导过程](http://blog.csdn.net/marsjohn/article/details/54911788)
   + [最小二乘法？为神马不是差的绝对值](http://blog.sciencenet.cn/blog-430956-621997.html)

一般地，$\theta=\left(X^TX\right)^{-1}X^Ty$

+ 加权最小二乘：$\theta=\left(X^TWX\right)^{-1}X^TWy$


   + `求解全局最优`
   + `数据量很大时，计算量大`

###4. 梯度下降(Gradient Descent) / 最速下降法(Steepest Descent)

   + 目标：$\mathop{minimize}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)$


   + `迭代法`

     + $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\quad$`α为学习率`

       $\mathop{\Longrightarrow}\limits^{线性回归}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_1:=\theta_1-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x^{(i)}}\end{array}\right.\quad$`同时更新θ₀和θ₁`

   + `每一步更新未知量，逐渐逼近局部最优解`

        ![梯度下降1](梯度下降1.png)

        ![梯度下降2](梯度下降2.png)

   + 如何确保梯度下降能正常工作

         + J(θ)应随着迭代不断减小
         + 使用较小的学习率*α*

   + 如何选取学习率*α*

         + 尝试：

              ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...

        + 取值范围

             $0<\alpha<2(|X^TX|^{-1})$

   + 分类

        + Batch Gradient Descent

          $\qquad$每一步都使用**整个**训练集的数据计算梯度

        + Stochastic Gradient Descent

          $\Downarrow\quad\ $每一步仅用训练集中的**单个**计算梯度

        + Mini-batch Gradient Descent

          $\qquad$每一步仅用训练集中的**一小部分**（32/64/128/256）计算梯度

          > The way to go is therefore to use some acceptable (but not necessarily optimal) values for the other hyper-parameters, and then trial a number of different mini-batch sizes, scaling α as above. Plot the validation accuracy versus time (as in, real elapsed time, not epoch!), and choose whichever mini-batch size gives you the most rapid improvement in performance. With the mini-batch size chosen you can then proceed to optimize the other hyper-parameters.

          + 适用于存在较多局部最优解的情况，能跳出局部最优解

          + 计算量更小，计算速度更快

          + [BGD与SGD的不同](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent)

          > The applicability of batch or stochastic gradient descent really depends on the error manifold expected.
          >
          > Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.
          >
          > Stochastic gradient descent (SGD) computes the gradient using a single sample. Most applications of SGD actually use a minibatch of several samples, for reasons that will be explained a bit later. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. Single samples are really noisy, while minibatches tend to average a little of the noise out. Thus, the amount of jerk is reduced when using minibatches. A good balance is struck when the minibatch size is small enough to avoid some of the poor local minima, but large enough that it doesn't avoid the global minima or better-performing local minima. (Incidently, this assumes that the best minima have a larger and deeper basin of attraction, and are therefore easier to fall into.)
          >
          > One benefit of SGD is that it's computationally a whole lot faster. Large datasets often can't be held in RAM, which makes vectorization much less efficient. Rather, each sample or batch of samples must be loaded, worked with, the results stored, and so on. Minibatch SGD, on the other hand, is usually intentionally made small enough to be computationally tractable.
          >
          > Usually, this computational advantage is leveraged by performing many more iterations of SGD, making many more steps than conventional batch gradient descent. This usually results in a model that is very close to that which would be found via batch gradient descent, or better.
          >
          > The way I like to think of how SGD works is to imagine that I have one point that represents my input distribution. My model is attempting to learn that input distribution. Surrounding the input distribution is a shaded area that represents the input distributions of all of the possible minibatches I could sample. It's usually a fair assumption that the minibatch input distributions are close in proximity to the true input distribution. Batch gradient descent, at all steps, takes the steepest route to reach the true input distribution. SGD, on the other hand, chooses a random point within the shaded area, and takes the steepest route towards this point. At each iteration, though, it chooses a new point. The average of all of these steps will approximate the true input distribution, usually quite well.

###5. 向量化

|     变量     | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------: | ---------------------------------------- |
|  feature   | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|   output   | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}$ |
|     θ      | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
| hypothesis | $h=X\theta=\begin{pmatrix}  \sum\limits_{i=0}^n\theta_ix_i^{(1)}\\ \sum\limits_{i=0}^n\theta_ix_i^{(2)}\\...\\ \sum\limits_{i=0}^n\theta_ix_i^{(m)} \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |

<div STYLE="page-break-after: always;"></div>

|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | ---------------------------------------- |
|  cost function   | $J=\dfrac{1}{2m}(h-y)^T(h-y)$            |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |
###6. 特征缩放(Feature Scaling) 

+ 确保所有特征在相似的规模上（$-1\leq{x_i}\leq{1}$）

+ 提升梯度下降速度

  ![特征缩放](特征缩放.png)

+ 方法

  + **mean normalization**

    $x_i'=\dfrac{x_i-\mu_i}{max(x)-min(x)}$

    $其中\mu_i为x的均值$

  + **Standardization**

    $x_i'=\dfrac{x-\bar{x}}{\sigma}$

    $其中\sigma为x的标准差$
###7. 正规方程(Normal Equation) 

+ 公式

  $\theta=(X^TX)^{-1}X^Ty$

+ 推导过程

  > 为了使$J=\dfrac{1}{2m}(h-y)^T(h-y)$最小，令$J$等于某一极小值，即
  >
  > $\begin{equation}\begin{aligned}\dfrac{\partial J}{\partial\theta}&=\dfrac{1}{2m}\dfrac{\partial (h-y)^T(h-y)}{\partial\theta}=\dfrac{1}{2m}\dfrac{\partial\left(h^Th-h^Ty-y^Th+y^Ty\right)}{\partial\theta}\\\\&=\dfrac{1}{2m}\dfrac{\partial\left(h^Th-2h^Ty\right)}{\partial\theta}=\dfrac{1}{2m}\left[\dfrac{\partial h^Th}{\partial\theta}-2\dfrac{\partial h^Ty}{\partial\theta}\right]\\\\&\xlongequal{\color{salmon}分母布局}\dfrac{1}{2m}\left[\left(\dfrac{\partial h}{\partial\theta}h+\dfrac{\partial h}{\partial\theta}h\right)-2\left(\dfrac{\partial y}{\partial\theta}h+\dfrac{\partial h}{\partial\theta}y\right)\right]\\\\&=\dfrac{1}{m}\left[\dfrac{\partial h}{\partial\theta}(h-y)-\dfrac{\partial y}{\partial\theta}h\right]=\dfrac{1}{m}\left[\dfrac{\partial X\theta}{\partial\theta}(X\theta-y)-\dfrac{\partial y}{\partial\theta}(X\theta)\right]\\\\&\xlongequal{\color{salmon}分母布局}\dfrac{1}{m}\left[X^T(X\theta-y)\right]{\color{violet}=\bf{0}}\end{aligned}\end{equation}$
  >
  > $\quad{\color{salmon}\Longrightarrow}\ X^TX\theta=X^Ty$
  >
  > $\quad{\color{salmon}\xrightarrow{X^TX可逆}}\ \theta=(X^TX)^{-1}X^Ty$

+ 直接求解参数*θ*的最优值，不需要多次迭代 $\quad$`解析方法`

+ 不需要特征缩放

+ 不需要选择学习率*α*



> `问题`
>
> A.  特征数很大时计算量大（n≥10000) $\quad$`求解逆矩阵的时间复杂度O(n³)`
>
> B.  只适用于线性模型
>
> C.  $X^TX$可能不可逆
>
> + 冗余特征：存在线性相关的特征
>
> + 太多特征：特征数>样本数，导致过拟合
>
>   $\mathop{\Longrightarrow}\limits^{处理方法}\left\{\begin{array}{lcl}删除部分特征\left\{\begin{array}{lcl}手动选择\\模型选择算法\end{array}\right.\\正则化(\mbox{Regularization})\end{array}\right.$

### 8. 正则化(Normalization)

`针对过拟合提出`

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=\dfrac{1}{2m}[\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2{\color{salmon}+\lambda\sum\limits_{j=1}^n\theta_j^2}]$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

  $\qquad\qquad\qquad\qquad\qquad\qquad\Downarrow$

  $\qquad\qquad\theta_j:=\theta_j{\color{salmon}(1-\alpha\dfrac{\lambda}{m})}-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}$$\qquad\qquad\qquad\qquad\qquad\scriptsize\color{maroon}\boxed{[注]\ 1-\alpha\dfrac{\lambda}{m}<1}$

  ​

#### · 代码实现

+ 计算代价与梯度

```octave
function [J, grad] = linearRegCostFunction(X, y, theta, lambda)
%   Compute the cost of using theta as the parameter for linear regression to fit the 
%   data points in X and y.
m = length(y); % number of training examples
%   Compute the cost (J)
h = X * theta;
reg = lambda * 0.5 / m * (theta' * theta - theta(1)^2);
J = 0.5 / m * (h - y)' * (h - y) + reg;
%   Compute the gradient (grad)
theta_x = theta; theta_x(1) = 0;
reg_grad = lambda / m * theta_x;
grad = 1.0 / m * (X' * (h - y)) + reg_grad;
grad = grad(:);
end
```

+ 训练过程

```octave
function [theta] = trainLinearReg(X, y, lambda)
%   Train linear regression given a dataset (X, y) and a regularization parameter lambda
initial_theta = zeros(size(X, 2), 1); 
% Create "short hand" for the cost function to be minimized
costFunction = @(t) linearRegCostFunction(X, y, t, lambda);
% Now, costFunction is a function that takes in only one argument
options = optimset('MaxIter', 200, 'GradObj', 'on');
% Minimize using fmincg
theta = fmincg(costFunction, initial_theta, options);
end
```

`正规方程/岭回归(ridge regression)`

$\qquad$单位矩阵中值1贯穿整个对角线，其余元素均为0，看似在0构成的平面上有一条1组成的“岭”

$\qquad\qquad\begin{equation}\begin{aligned}\theta&=\left[X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}_{(n+1)\times{(n+1)}}\right]^{-1}X^Ty\\\\&=\left[X^TX+\lambda\begin{pmatrix}  0&\vec{0}_{1\times{n}}&\\\vec{0}_{n\times{1}}&{\bf{I}}_{n\times{n}}\end{pmatrix}\right]^{-1}X^Ty\end{aligned}\end{equation}$

​

$\qquad\qquad\color{maroon}[注]\ 当\lambda>0时，可以证明X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}是可逆的$

> 建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍（增减10倍是粗调节，当确定了λ的合适的数量级后，比如λ = 0.01,再进一步地细调节，比如调节为0.02，0.03，0.009之类。）

### 9. 牛顿法

`具有二次收敛性，收敛速度快`

![牛顿法&梯度下降法](牛顿法&梯度下降法.png)

+ 在现有极小点估计值附近对$f(x)$做二阶泰勒展开，进而找到极小点的下一个估计值

迭代公式

+ `一维情况`

  $x_{k+1}=x_k-\dfrac{f'(x_k)}{f''(x_k)}$

  > 设$x_k$为当前的极小点估计值，则$f(x)$在$x_k$附近的二阶泰勒展开式为
  >
  > $\varphi(x)=f(x_k)+f'(x_k)(x-x_k)+\dfrac{1}{2}f''(x_k)(x-x_k)^2$
  >
  > 令$\varphi'(x)=0$，即$f'(x_k)+f''(x_k)(x-x_k)=0$
  >
  > 从而求得$x=x_k-\dfrac{f'(x_k)}{f''(x_k)}$

+ `多维情况`

  ${\bf{x}}_{k+1}={\bf{x}}_k-\bf{H}_k^{-1}{\bf{g}}_k$

  + 对于多维情况，二阶泰勒展开式可以推广为

    $\qquad\varphi({\bf{x}})=f({\bf{x}}_k)+\nabla f({\bf{x}}_k)\cdot{({\bf{x}}-{\bf{x}}_k)}+\dfrac{1}{2}({\bf{x}}-{\bf{x}}_k)^T\cdot{\nabla^2f({\bf{x}}_k)}\cdot{({\bf{x}}-{\bf{x}}_k)}$

    其中$\ \nabla f=\begin{pmatrix}\dfrac{\partial f}{\partial x_1}\\\dfrac{\partial f}{\partial x_2}\\...\\\dfrac{\partial f}{\partial x_n}\end{pmatrix}={\bf{g}}_k$是$f$的**梯度向量**

    $\qquad\nabla^2f=\begin{pmatrix}\dfrac{\partial^2f}{\partial x_1^2}&\dfrac{\partial^2f}{\partial x_1\partial x_2}&...&\dfrac{\partial^2f}{\partial x_1\partial x_n}\\\dfrac{\partial^2f}{\partial x_2\partial x_1}&\dfrac{\partial^2f}{\partial x_2^2}&...&\dfrac{\partial^2f}{\partial x_2\partial x_n}\\...&...&...&...\\\dfrac{\partial^2f}{\partial x_n\partial x_1}&\dfrac{\partial^2f}{\partial x_n\partial x_2}&...&\dfrac{\partial^2f}{\partial x_n^2}\end{pmatrix}_{n\times{n}}={\bf{H}}_k$是$f$的**[海森矩阵](#3. 海森矩阵(Hessian Matrix))**

    同理，由于是求极小点，即求$\varphi({\bf{x}})$的驻点，令$\nabla\varphi({\bf{x}})=0$，即

    $\qquad{\bf{g}}_k+{\bf{H}}_k({\bf{x}}-{\bf{x}}_k)=0$

    ​若矩阵$\bf{H}_k$非奇异，则可解得

    $\qquad{\bf{x}}={\bf{x}}_k-\bf{H}_k^{-1}{\bf{g}}_k$

    其迭代公式中的搜索方向${\bf{d_k}}=-{\bf{H}}_k^{-1}{\bf{g}}_k$称为**牛顿方向**

    > 梯度下降法的搜索方向为${\bf{d}}_k=-{\bf{g}}_k$

+ 由于原始牛顿法的迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数有可能出现$f({\bf{x}}_{k+1})\gt f({\bf{x}}_k)$的情况，即不能保证函数值稳定地下降，甚至可能导致无法收敛

#### · 阻尼牛顿法

+ 对每次迭代步长引入步长因子$\lambda_k$，即$\lambda_k=arg\ \underset{\lambda\in\mathbb{R}}{\min}f({\bf{x}}+\lambda{\bf{d}}_k)$

#### · 拟牛顿法

+ 不用二阶偏导数而构造出可以近似海森矩阵（或海森矩阵的逆）的正定对称阵，在“拟牛顿”的条件下优化目标函数

  > 克服求解海森矩阵的逆计算量大的问题
  >
  > 克服目标函数的海森矩阵无法保持正定而使牛顿法失效的问题

  + 拟牛顿条件

    > 设经过k+1次迭代后得到$x_{k+1}$，此时将目标函数$f({\bf{x}})$在${\bf{x}}_{k+1}$附近作泰勒展开，取二阶近似，得到
    >
    > $\quad f({\bf{x}})\approx f({\bf{x}}_{k+1})+\nabla f({\bf{x}}_{k+1})\cdot{({\bf{x}}-{\bf{x}}_{k+1})}+\dfrac{1}{2}({\bf{x}}-{\bf{x}}_{k+1})^T\cdot{\nabla^2f({\bf{x}}_{k+1})}\cdot{({\bf{x}}-{\bf{x}}_{k+1})}$
    >
    > 两边同时作用一个梯度算子$\nabla$，可得
    >
    > $\quad\nabla f({\bf{x}})\approx\nabla f({\bf{x}}_{k+1})+{\bf{H}}_{k+1}({\bf{x}}-{\bf{x}}_{k+1})$
    >
    > 上式中取${\bf{x}}={\bf{x}}_k$，整理可得
    >
    > $\quad{\bf{g}}_{k+1}-{\bf{g}}_k\approx{\bf{H}}_{k+1}({\bf{x}}_{k+1}-{\bf{x}}_k)$
    >
    > 引入记号${\bf{s}}_k={\bf{x}}_{k+1}-{\bf{x}}_k,\ {\bf{y}}_k={\bf{g}}_{k+1}-{\bf{g}}_k$，则可表示为
    >
    > $\quad {\bf{y}}_k\approx{\bf{H}}_{k+1}{\bf{s}}_k\quad$或$\quad {\bf{s}}_k\approx{\bf{H}}_{k+1}^{-1}{\bf{y}}_k$

  + DFP算法

  + BFGS算法

  + L-BFGS算法

### 10. 共轭方向(Conjugate Directions)法

> 在N维优化问题中，选取N个正交的搜索方向(search direction)，每次沿一个方向优化得到极小值，后面再沿其他方向求极小值的时候，不会影响前面已经得到的沿那些方向上的极小值

```tex
此方法仅当已经知道结果时才能生效
```

$\vec{x}_{(i+1)}=\vec{x}_{(i)}+\alpha_{(i)}\vec{d}_{(i)}\quad(i=1,2,...,N)\quad$`迭代公式`

其中$\vec{d}_{(i)}$为第$i$个搜索方向 ，$\alpha_{(i)}$为第$i$个搜索方向的步长

第$i$个搜索方向上优化之后，距离最优点$\vec{x}$的误差向量$\vec{e}_{(i+1)}\color{gray}=\vec{x}_{(i+1)}-\vec{x}$与$\vec{d}_{(i)}$正交，即

$\qquad\vec{d}_{(i)}^T\vec{e}_{(i+1)}=0$

而$\vec{e}_{(i+1)}-\vec{e}_{(i)}=\vec{x}_{(i+1)}-\vec{x}_{(i)}=\alpha_{(i)}\vec{d}_{(i)}$

则$\vec{d}_{(i)}^T\left(\vec{e}_{(i)}+\alpha_{(i)}\vec{d}_{(i)}\right)=0$

$\quad{\color{maroon}\Longrightarrow}\quad\alpha_{(i)}=-\dfrac{\vec{d}_{(i)}^T\vec{e}_{(i)}}{\vec{d}_{(i)}^T\vec{d}_{(i)}}$

但$\vec{e}_{(i)}$是未知的，因此无法直接求解$\alpha_{(i)}$

+ 令$\vec{d}_{(i)}$之间关系为**A-正交(A-orthogonal)**以代替正交(orthogonal)，那么要求变为使误差向量$\vec{e}_{(i+1)}$与$\vec{d}_{(i)}$满足**A**-正交，这等效于在$\vec{d}_{(i)}$搜索方向上找到极小值

  > `A-正交（或A-共轭）的定义`
  >
  > 若向量$\vec{d}_{(i)}$和$\vec{d}_{(j)}$满足$\vec{d}_{(i)}^T\mathbf{A}\vec{d}_{(j)}=0$，则称它们是**A**-正交（或**A**-共轭）的

  为此，令方向导数$\dfrac{d}{d\alpha}f\left(\vec{x}_{(i+1)}\right)=0$

  $\qquad\qquad\quad{\color{maroon}\Longrightarrow}\ f'\left(\vec{x}_{(i+1)}\right)^T\dfrac{d}{d\alpha}\vec{x}_{(i+1)}=0$

  $\qquad\qquad\quad{\color{maroon}\Longrightarrow}\ -\vec{r}_{(i+1)}^T\vec{d}_{(i)}=0\qquad$($\ \color{gray}\vec{r}_{(i)}=-f'(\vec{x}_{(i)})=-\mathbf{A}\vec{e}_{(i)}$ ，残差向量$\vec{r}$定义参考[最速下降法](#· 最速下降法( Steepest Descent)))

  $\qquad\qquad\quad{\color{maroon}\Longrightarrow}\ \vec{d}_{(i)}^T\mathbf{A}\vec{e}_{(i+1)}=0$

  $\qquad\qquad\quad{\color{maroon}\Longrightarrow}\ \vec{d}_{(i)}^T\mathbf{A}\left(\vec{e}_{(i)}+\alpha_{(i)}\vec{d}_{(i)}\right)=0$

  $\quad{\color{maroon}\Longrightarrow}\ \alpha_{(i)}=-\dfrac{\vec{d}_{(i)}^T\mathbf{A}\vec{e}_{(i)}}{\vec{d}_{(i)}^T\mathbf{A}\vec{d}_{(i)}}=\dfrac{\vec{d}_{(i)}^T\vec{r}_{(i)}}{\vec{d}_{(i)}^T\mathbf{A}\vec{d}_{(i)}}$

  > 当$\vec{d}_{(i)}=\vec{r}_{(i)}$时，上述公式与最速下降法相同

  将最初的误差表示为搜索方向的线性组合：$\vec{e}_{(0)}=\sum\limits_{j=0}^{n-1}\delta_{(j)}\vec{d}_{(j)}$

  则$\delta_j$的值可以通过下列公式计算得到：

  $\qquad\qquad\vec{d}_{(k)}^T\mathbf{A}\vec{e}_{(0)}=\sum\limits_j\delta_{(j)}\vec{d}_{(k)}^T\mathbf{A}\vec{d}_{(j)}$

  $\qquad{\color{maroon}\mathop{\Longrightarrow}\limits^{\mathbf{A}-正交}}\ \ \vec{d}_{(k)}^T\mathbf{A}\vec{e}_{(0)}=\delta_{(k)}\vec{d}_{(k)}^T\mathbf{A}\vec{d}_{(k)}$

  $\qquad\ {\color{maroon}\mathop{\Longrightarrow}}\quad\delta_{(k)}=\dfrac{\vec{d}_{(k)}^T\mathbf{A}\vec{e}_{(0)}}{\vec{d}_{(k)}^T\mathbf{A}\vec{d}_{(k)}}\xlongequal{\color{Bittersweet}\mathbf{A}-正交}\dfrac{\vec{d}_{(k)}^T\mathbf{A}\left(\vec{e}_{(0)}{\color{salmon}+\sum\limits_{i=0}^{k-1}\alpha_{(i)}\vec{d}_{(i)}}\right)}{\vec{d}_{(k)}^T\mathbf{A}\vec{d}_{(k)}}=\dfrac{\vec{d}_{(k)}^T\mathbf{A}\vec{e}_{(k)}}{\vec{d}_{(k)}^T\mathbf{A}\vec{d}_{(k)}}=-\alpha_{(k)}$

  则第$i$次迭代之后的误差$\vec{e}_{(i)}=\vec{e}_{(0)}+\sum\limits_{j=0}^{i-1}\alpha_{(j)}\vec{d}_{(j)}=\sum\limits_{j=0}^{n-1}\delta_{(j)}\delta_{(j)}-\sum\limits_{j=0}^{i-1}\delta_{(j)}\vec{d}_{(j)}=\sum\limits_{j=i}^{n-1}\delta_{(j)}\vec{d}_{(j)}$

  逐步迭代到最优$\vec{x}$的过程也可以看作逐步消除误差$\vec{e}_{(i)}$的过程

+ 如何得到一组**A**-正交的搜索方向$\vec{d}_{(i)}?\quad{\color{maroon}\Rightarrow}\quad$`Gram–Schmidt过程`

  ​

### 11. 共轭梯度方法(Conjugate Gradient Method, CG)

`迭代方法`

介于[梯度下降法](#4. 梯度下降(Gradient Descent) / 最速下降法)与[牛顿法](#8. 牛顿法)之间的方法，仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算海森矩阵并求逆的缺点

迭代法适合于稀疏矩阵(sparse matrices)

[An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](https://wenku.baidu.com/view/1e2c551cb4daa58da0114a47.html)

![ConjugateGradientIllustration](ConjugateGradientIllustration.png)

###12. 局部加权线性回归    `非参数学习方法` 

​	Ⅰ $\quad\ $Fit *θ* to minimize $\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$

​	Ⅱ $\quad$Output $\theta^Tx$

$w^{(i)}=e^{-\dfrac{(x^{(i)}-x)^2}{2\tau^2}}$，其中τ为常数，可调节$w^{(i)}$钟形曲线的宽度

+ 当$|x^{(i)}-x|\approx{0}$时，$w^{(i)}\approx{1}$，即预测样本距离训练样本越近，权值越大
+ 当$|x^{(i)}-x|\approx{+\infty}$时，$w^{(i)}\approx{0}$，即预测样本距离训练样本越远，权值越小

> `问题`
>
> A.  当数据规模比较大时，计算量很大，学习效率很低
>
> B.  不一定能避免欠拟合

<div STYLE="page-break-after: always;"></div>


##三、逻辑回归/对数几率回归(Logistic Regression)

直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题

###1. 模型表示

> `分类`
>
> $\quad y=0\ \mbox{or} \ 1$
>
> $\quad h_\theta(x)\ \mbox{can be}\ >1\ \mbox{or}\ <0$
>
> $\color{brown}\Downarrow逻辑函数$
>
> `逻辑回归`
>
> $\quad0\leq{h_\theta(x)}\leq{1}\quad$

假设函数$h_\theta(x)=g(\theta^Tx)=\dfrac{1}{1+e^{-\theta^Tx}}\quad$`在线性回归模型的基础上加了一个Sigmoid函数 `

其中$g(z)=\dfrac{1}{1+e^{-z}}\quad$`sigmoid函数（S型曲线）`

​        $g'(z)=\dfrac{e^{(-z)}}{(1+e^{(-z)})^2}=\dfrac{1}{1+e^{(-z)}}(1-\dfrac{1}{1+e^{(-z)}})=g(z)(1-g(z))$

###2. 代价函数(Cost function)

> 线性回归的代价函数在逻辑回归模型中是非凸函数('"non-convex"')，不容易收敛到全局最优点，故引入新的代价函数

$J(\theta)=\dfrac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$

$\begin{equation}\begin{aligned}Cost(h_\theta(x^{(i)}),y^{(i)})&=\left\{\begin{array}{lcl}-log(h_\theta(x^{(i)})),\ \ \ \ \ \ \ \ \ y^{(i)}=1\\-log(1-h_\theta(x^{(i)})),\ \ y^{(i)}=0\end{array}\right.\\ &\color{maroon}=-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))\end{aligned}\end{equation}$

###3. 梯度下降(Gradient Descent) 

+ 目标：$\mathop{minimize}\limits_{\theta}J(\theta)$

+ 公式

  $\begin{equation}\begin{aligned}\theta_j&:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)\\ &:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot{x_j^{(i)}}\end{aligned}\end{equation}$

+ 优化算法

  `不需要手动选择学习率α`

  `通常比梯度下降更快`

  `但算法更复杂`

  + [Conjugate  gradient](#9. 共轭梯度方法(Conjugate Gradient Method, CG)) 
  + [BFGS](#· 拟牛顿法) 
  + [L-­BFGS](#· 拟牛顿法) 

###4. 向量化

|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | :--------------------------------------- |
|     feature      | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|      output      | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}\quad$`每一项取值0或1` |
|        θ         | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
|    hypothesis    | $h=g(X\theta)=\begin{pmatrix}  g(\sum\limits_{i=0}^n\theta_ix_i^{(1)})\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(2)})\\...\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(m)}) \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |
|  cost function   | $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]$ |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |

###5. 正则化(Normalization)

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]{\color{salmon}+\dfrac{\lambda}{2m}\sum\limits_{j=1}^n\theta_j^2}$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

+ 向量化

  $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]+\dfrac{\lambda}{2m}(\theta^T\theta-\theta_0^2)$

  $\theta:=\theta-\dfrac{\alpha}{m}[X^T(h-y)+\lambda\theta]\quad\color{salmon}(\theta_0无正则项)$


####* 范数(norm)

- L0范数：向量中非0的元素的个数
  - L0范数可以实现稀疏
  - $\Vert\vec{x}\Vert_0$

- L1范数：向量中各个元素绝对值之和$\quad$`稀疏规则算子`

  - L1正则化会产生稀疏的特征（产生少量的特征，而其他的特征都是0）
  - 不可导
  - 可用于特征选择
  - $\Vert\vec{x}\Vert_1=\sum\limits_{i=1}^n|x_i|\quad$
  - $d_1(I_1,I_2)=\sum\limits_p|I_1^p-I_2^p|\quad$`L1距离/曼哈顿距离`

- L2范数：向量各元素的平方和然后求平方根$\quad$`权值衰减`

  - L2正则化会产生更多特征但是都会接近于0（将参数均匀化）
  - 可导
  - $\Vert\vec{x}\Vert_2=\sqrt{x_1^2+x_2^2+...+x_n^2}=\left(\sum\limits_{i=1}^n|x_i|^2\right)^{\frac{1}{2}}$
  - $d_2(I_1,I_2)=\sqrt{\sum\limits_p(I_1^p-I_2^p)^2}\quad$`L2距离/欧氏距离`

- L1&L2结合：$\lambda\left(\alpha\Vert x\Vert_1+\dfrac{1-\alpha}{2}\Vert x\Vert_2\right)\quad$(当α=0时为L2范数，当α=1时为L1范数)

  > L1正则化：$\lambda\Vert\theta\Vert\quad\ \ $`套索回归Lasso`
  >
  > L2正则化：$\lambda\Vert\theta\Vert^2\quad$`岭回归Ridge`

###6. 多元分类

> $y\in{0,1,2,...,n}$
>
> $\left\{\begin{array}{lcl}h_\theta^{(0)}(x)=P(y=0|x;\theta)\\h_\theta^{(1)}(x)=P(y=1|x;\theta)\\...\\h_\theta^{(n)}(x)=P(y=n|x;\theta)\end{array}\right.$

$\mbox{prediction}=\mbox{max}(h_\theta^{(i)}(x))$

<div STYLE="page-break-after: always;"></div>

##四、神经网络

###1. 模型表示 

+ $L=网络总层数=输入层+隐含层+输出层$

+ $s_l=第l层中的单元数（不包含偏置单元）$

+ $K=输出单元个数\mbox{ or }类别数=s_L\quad\mathop{\Longrightarrow}\limits^{二分类}\quad K=1$

+ $(h_\Theta(x^{(i)}))_k=第k个输出$

+ $a^{(j)}=第j层的激励(\mbox{activation})\ \mathop{\Longrightarrow}\limits^{维度}\ (s_j+1)\times{1}$

  $a_i^{(j)}=第j层第i个单元的激励$

+ $\Theta^{(j)}=从第j层映射到第j+1层的权重矩阵\ \mathop{\Longrightarrow}\limits^{维度}\ s_{j+1}\times{(s_j+1)}$

$\quad\ \ \ \Theta_{kn}^{(j)}=从第j层映射到第j+1层第k个单元的第n个参数(权重)$

![神经网络](神经网络.png)



####`网络架构`

+ 输入层→隐含层→输出层

![神经网络层次](神经网络层次.jpeg)

###2. 非线性分类：逻辑运算

`激活函数取sigmoid函数`

+ or运算

  $h_\Theta(x)=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![or](or.png)

+ and运算

  $h_\Theta(x)=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![and](and.png)

+ not运算

  $h_\Theta(x)=g(10-20x)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x=0\\\approx0&x=1\end{array}\right.$

  ![not](not.png)

+ xor运算

  $h_\Theta(x)=a_1^{(3)}=g(-30+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(30-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![xor](xor.png)

+ xnor运算

  $h_\Theta(x)=a_1^{(3)}=g(-10+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(10-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  ![xnor](xnor.png)

###3. 分类

#### · 二分类

$\qquad y=0\ or\ 1$

#####		· `感知机(perception)`

$\qquad$`由两层神经元组成`

$\qquad$假设空间是特征空间中的所有线性分类模型，即找到一个线性方程$x\cdot{w}+b=0$，它对应于特征空间的一个

$\qquad$超平面，能够把对应的特征空间分为两部分，位于两部分中的点分别是正负两类。

$\qquad$若存在这样的超平面，则感知机的学习过程一定收敛(converge)

> 若当前感知机输出为$\hat{y}$，则感知机权重将这样调整（设y取值0或1）：
>
> $\qquad w_i:=w_i+\Delta w_i\\\qquad \Delta w_i=\alpha(y-\hat{y})x_i\quad\color{salmon}(\alpha为学习率)$
>
> + 若感知机对训练样例$(x,y)$预测正确，即$\hat{y}=y$，则感知机不发生变化
> + 否则将根据错误的程度进行权重调整

####· 多元分类

$\qquad y^{(i)}\in one\ of\begin{pmatrix}1\\0\\0\\0\end{pmatrix},\begin{pmatrix}0\\1\\0\\0\end{pmatrix},\begin{pmatrix}0\\0\\1\\0\end{pmatrix},\begin{pmatrix}0\\0\\0\\1\end{pmatrix}$

![多元分类](多元分类.png)

###4. 代价函数(Cost function)

`逻辑回归中代价函数的一般形式`

$J(\Theta)=-\dfrac{1}{m}\sum\limits_{i=1}^m\sum\limits_{k=1}^K\left[y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-h_\Theta(x^{(i)}))_k\right]+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{j+1}}(\Theta_{ji}^{(l)})^2$

### 5. 反向传播算法(Backpropagation algorithm)

`梯度下降`$\quad$`迭代法`

![反向传播](反向传播.png)

+ Ⅰ$\quad\ $前向传播(feedforward)$\quad$`计算各层输出`

  ![神经网络](MP神经元.png)$\quad\color{maroon}[注]\ \mbox{M-P: McCulloch & Pitts}$

  > $a^{(1)}=x$
  >
  > $z^{(2)}=\Theta^{(1)}a^{(1)}$
  >
  > $a^{(2)}=g(z^{(2)})\quad(\mbox{add}\ a_0^{(2)})$
  >
  > $z^{(3)}=\Theta^{(2)}a^{(2)}$
  >
  > $a^{(3)}=g(z^{(3)})\quad(\mbox{add}\ a_0^{(3)})$
  >
  > $z^{(4)}=\Theta^{(3)}a^{(3)}$
  >
  > $a^{(4)}=h_\Theta(x)=g(z^{(4)})$
  >
  > $\dfrac{\partial a^{(l)}}{\partial z^{(l)}}=\begin{pmatrix}\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_1}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_1}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_1}\\\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_2}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_2}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_2}\\...&...&...&...\\\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_{s_l}}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_{s_l}}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_{s_l}}\end{pmatrix}=\begin{pmatrix}0&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_1}&0&...&0\\0&0&\dfrac{\partial a^{(l)}_2}{\partial z^{(l)}_2}&...&0\\...&...&...&...&...\\0&0&0&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_{s_l}}\end{pmatrix}_{s_l\times{(s_l+1)}}\ $`分母布局`

+ Ⅱ$\quad$反向传播$\quad$`误差逆传播`

  > 若输入层的实际输出$h_\Theta(x)$与期望的输出$y$不符，则进行误差的反向传播
  >
  > 直到网络输出的误差减少到了可以接受的程度（或 进行到预先设定的学习次数为止）

  `偏置单元可以计入δ，也可不计入`

由梯度下降公式$\theta:=\theta-\alpha\dfrac{\partial}{\partial\theta}J(\theta)$联想到神经网络的迭代：

$\dfrac{\partial J(\Theta)}{\partial \Theta^{(l)}}=\nabla_{\Theta^{(l)}}J(\Theta)=\dfrac{\partial J(\Theta)}{\partial z^{(l+1)}}\dfrac{\partial z^{(l+1)}}{\partial \Theta^{(l)}}=\delta^{(l+1)}(a^{(l)})^T$

$\delta^{(l)}=\dfrac{\partial J}{\partial z^{(l)}}=\dfrac{\partial a^{(l)}}{\partial z^{(l)}}\dfrac{\partial z^{(l+1)}}{\partial a^{(l)}}\dfrac{\partial J}{\partial z^{(l+1)}}=g'(z^{(l)})\times(\Theta^{(l)})^T\delta^{(l+1)}=(\Theta^{(l)})^T\delta^{(l+1)}\ .*\ {a^{(l)}(1-a^{(l)})}\quad$`分母布局`

$\delta_j^{(l)}=第l层第j个单元的{\color{blueviolet}\textbf{误差}}=\left\{\begin{array}{lcl}a_j^{(l)}-y_j=(h_\Theta(x))_j-y_j&\color{salmon}输出层&(l=L)\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i1}^{(l)}\delta_i^{(l+1)}\cdot{a_j^{(l)}}\cdot{(1-a_j^{(l)})}&\color{salmon}隐含层&(1<l<L)\end{array}\right.\quad$

$\quad\\\qquad\mathop{\Longrightarrow}\limits^{忽略正则项\lambda}\ \boxed{\quad\dfrac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}\quad}$

$\qquad\qquad\qquad\dfrac{\partial}{\partial\Theta^{(l)}}J(\Theta)=\begin{pmatrix}\delta_1^{(l+1)}a_0^{(l)}&\delta_1^{(l+1)}a_1^{(l)}&...&\delta_1^{(l+1)}a_{s_l}^{(l)}\\\delta_2^{(l+1)}a_0^{(l)}&\delta_2^{(l+1)}a_1^{(l)}&...&\delta_2^{(l+1)}a_{s_l}^{(l)}\\...&...&...&...\\\delta_{s_{l+1}}^{(l+1)}a_0^{(l)}&\delta_{s_{l+1}}^{(l+1)}a_1^{(l)}&...&\delta_{s_{l+1}}^{(l+1)}a_{s_l}^{(l)}\end{pmatrix}_{s_{l+1}\times{(s_l+1)}}$



#### · 步骤

1. Set $\ \triangle_{ij}^{(l)}=0\ $(for all $i,j,l$)$\quad\ \;$`误差矩阵`

2. For $i$=1 to $m\qquad\qquad\qquad\quad$`对每个样本单独计算误差并累加`

   $\quad$Set $\ a^{(1)}=x^{(i)}$

   $\quad$执行前向传播算法计算$a^{(l)},\ l=2,3,...,L$

   $\quad$计算$\delta^{(L)}=a^{(L)}-y^{(i)}$

   $\quad$计算$\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\color{maroon}(保留\delta_0^{(l)}项)$

   $\quad \triangle_{ij}^{(l)}:=\triangle_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}\quad\pmatrix{\ \ \triangle^{(l)}:=\triangle^{(l)}+\delta^{(l+1)}(a^{(l)})^T\ \ }\qquad\color{maroon}(忽略\delta_0^{(l)}项)$

3. 计算$J(\Theta)$的偏导数

   $\left\{\begin{array}{lcl}D^{(l)}_{ij}:=\dfrac{1}{m}\triangle_{ij}^{(l)}+\dfrac{\lambda}{m}\Theta_{ij}^{(l)}&\mbox{if}\ j\neq{0}\\\ D^{(l)}_{ij}:=\dfrac{1}{m}\triangle_{ij}^{(l)}&\mbox{if}\ j=0\end{array}\right.$

   > $\dfrac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$

4. 迭代更新权重矩阵

   $\Theta^{(l)}:=\Theta^{(l)}-\alpha D^{(l)}$

`问题`

> 隐含层数过多之后，神经网络难以直接用BP算法进行训练，因为误差在多隐层内逆传播时，往往会“发散”(diverge)而不能收敛到稳定状态

###6. 向量化

|       变量       | 向量表示（$其中m为训练样本数，K为类别数，L为网络层数，s_l为第l层单元数$） |
| :------------: | :--------------------------------------- |
|    feature     | $x=\begin{pmatrix}{\color{salmon}1}&x_1^{(1)}& x_2^{(1)}&...& x_{s_1}^{(1)}\\{\color{salmon}1}&x_1^{(2)}& x_2^{(2)}&...& x_{s_1}^{(2)}\\{\color{salmon}...}&...&...&...&...\\{\color{salmon}1}&x_1^{(m)}& x_2^{(m)}&...& x_{s_1}^{(m)}\end{pmatrix}_{m\times{(s_1+1)}}$`人为地添加一列全1，即x₀` |
|     output     | $y=\begin{pmatrix} y_1^{(1)}&y_2^{(1)}&...& y_{K}^{(1)}\\y_1^{(2)}&y_2^{(2)}&...& y_{K}^{(2)}\\...&...&...&...\\y_1^{(m)}&y_2^{(m)}&...& y_{K}^{(m)}\end{pmatrix}_{m\times{K}}\quad\color{blueviolet}(y_1^{(i)},y_2^{(i)},...,y_K^{(i)}中仅一项取值为1，其余取值为0)$ |
| $\Theta^{(l)}$ | $\qquad\quad\ \ \color{magenta}偏置单元\\\Theta^{(l)}=\begin{pmatrix}  {\color{magenta}\theta_{10}^{(l)}}&\theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_l}^{(l)}\\{\color{magenta}\theta_{20}^{(l)}}&\theta_{21}^{(1)}&\theta_{22}^{(l)}&...&\theta_{2s_l}^{(l)}\\{\color{magenta}\theta_{30}^{(l)}}&\theta_{31}^{(l)}&\theta_{32}^{(l)}&...&\theta_{3s_l}^{(l)}\\{\color{magenta}...}&...&...&...&...\\ {\color{magenta}\theta_{s_{l+1}0}^{(l)}}&\theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_l}^{(l)} \end{pmatrix}_{s_{l+1}\times{(s_l+1)}}=\begin{pmatrix}(\vec\theta_1^{(l)})^T\\(\vec\theta_2^{(l)})^T\\(\vec\theta_3^{(l)})^T\\...\\(\vec\theta_{s_{l+1}}^{(l)})^T\end{pmatrix}$ |
|       z        | $\begin{array}{lcl}z^{(l)}=\\\scriptsize{a^{(l-1)}(\Theta^{(l-1)})^T}\end{array}=\begin{pmatrix}\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(1)}\theta_{1i}^{(l-1)}&\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(1)}\theta_{2i}^{(l-1)}&...&\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(1)}\theta_{s_{l}i}^{(l-1)}\\\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(2)}\theta_{1i}^{(l-1)}&\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(2)}\theta_{2i}^{(l-1)}&...&\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(2)}\theta_{s_{l}i}^{(l-1)}\\...&...&...&...\\\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(m)}\theta_{1i}^{(l-1)}&\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(m)}\theta_{2i}^{(l-1)}&...&\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(m)}\theta_{s_{l}i}^{(l-1)}\end{pmatrix}=\begin{pmatrix}{\left(z^{(l)}\right)^T}^{(1)}\\{\left(z^{(l)}\right)^T}^{(2)}\\...\\{\left(z^{(l)}\right)^T}^{(m)}\end{pmatrix}\\\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad^{m\times{s_{l}}}\quad\color{salmon}(2\le l\le L)$ |

<div STYLE="page-break-after: always;"></div>

|         变量         | 向量表示（$其中m为训练样本数，K为类别数，L为网络层数，s_l为第l层单元数$） |
| :----------------: | :--------------------------------------- |
|     activation     | $a^{(1)}=x\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\color{salmon}(l=1)\\\quad\\\begin{equation}\begin{aligned}a^{(l)}&=g(z^{(l)})\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\color{salmon}(2\le l\le L)\\\\&=\begin{pmatrix}{\color{salmon}1}&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(1)}\theta_{1i}^{(l-1)}\right)&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(1)}\theta_{2i}^{(l-1)}\right)&...&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(1)}\theta_{s_{l}i}^{(l-1)}\right)\\{\color{salmon}1}&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(2)}\theta_{1i}^{(l-1)}\right)&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(2)}\theta_{2i}^{(l-1)}\right)&...&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(2)}\theta_{s_{l}i}^{(l-1)}\right)\\{\color{salmon}...}&...&...&...&...\\{\color{salmon}1}&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(m)}\theta_{1i}^{(l-1)}\right)&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(m)}\theta_{2i}^{(l-1)}\right)&...&g\left(\sum\limits_{i=0}^{s_{l-1}}{a_i^{(l-1)}}^{(m)}\theta_{s_{l}i}^{(l-1)}\right)\end{pmatrix}\\\\&=\begin{pmatrix}{\color{salmon}1}&{\left(a^{(l)}\right)^T}^{(1)}\\{\color{salmon}1}&{\left(a^{(l)}\right)^T}^{(2)}\\{\color{salmon}...}&...\\{\color{salmon}1}&{\left(a^{(l)}\right)^T}^{(m)}\end{pmatrix}_{m\times{(s_l+1)}}\end{aligned}\end{equation}$ |
|     hypothesis     | $h=a^{(L)}=\begin{pmatrix}{\left(a^{(L)}\right)^T}^{(1)}\\{\left(a^{(L)}\right)^T}^{(2)}\\...\\{\left(a^{(L)}\right)^T}^{(m)}\end{pmatrix}=\begin{pmatrix}{\left(a^{(L)}_1\right)}^{(1)}&{\left(a^{(L)}_2\right)}^{(1)}&...&{\left(a^{(L)}_K\right)}^{(1)}\\{\left(a^{(L)}_1\right)}^{(2)}&{\left(a^{(L)}_2\right)}^{(2)}&...&{\left(a^{(L)}_K\right)}^{(2)}\\...&...&...&...\\{\left(a^{(L)}_1\right)}^{(m)}&{\left(a^{(L)}_2\right)}^{(m)}&...&{\left(a^{(L)}_K\right)}^{(m)}\end{pmatrix}_{m\times{K}}$ |
| error<br>(对于每个样本)： | $\delta^{(L)}=a^{(L)}-y=h_\Theta(x)-y\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ \color{salmon}(l=L)\\\quad\\ \begin{equation}\begin{aligned}\delta^{(l)}&=((\Theta^{(l)})^T \delta^{(l+1)})\ .*g'(z^{(l)})=((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\\&=\begin{pmatrix}\sum\limits_{i=1}^{s_{l+1}}\Theta_{i1}^{(l)}\delta_i^{(l+1)}\cdot{a_1^{(l)}}\cdot{(1-a_1^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i2}^{(l)}\delta_i^{(l+1)}\cdot{a_2^{(l)}}\cdot{(1-a_2^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i3}^{(l)}\delta_i^{(l+1)}\cdot{a_3^{(l)}}\cdot{(1-a_3^{(l)})}\\...\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{is_l}^{(l)}\delta_i^{(l+1)}\cdot{a_{s_l}}^{(l)}\cdot{(1-a_{s_l}^{(l)})}\end{pmatrix}_{s_l\times{1}}\end{aligned}\end{equation}\color{salmon}\qquad(2\le l\le L-1)\\\color{blueviolet}(未计入偏置单元\delta_0^{(l)}，故公式中\Theta^{(l)}的维度应为s_{l+1}\times{s_l}，a^{(l)}的维度应为s_l\times{1})$ |
|  cost function(例)  | $\begin{equation}\begin{aligned}J=&-\dfrac{1}{m}[\mbox{sum}\left(y.*log(h)\right)+\mbox{sum}\left((1-y).*log(1-h)\right)]\\&+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\left[\mbox{sum}\left(\hat{\Theta}^{(l)}.*\hat{\Theta}^{(l)}\right)\right]\end{aligned}\end{equation}\\\color{blueviolet}(\mbox{sum}为对矩阵所有元素求和，\hat{\Theta}^{(l)}为第l层去除偏置单元后的参数矩阵)$ |
|  Gradient descent  | $\Theta^{(l)}:=\Theta^{(l)}-\alpha D^{(l)}$ |

### 7. 梯度检验

 + 梯度的数值检验(Numerical Gradient Checking)

   ![GradientCheck](GradientCheck.png)

>   通过估计梯度值来检验我们计算的导数值是否真的是我们要求的
>
>   + 在代价函数上沿着切线的方向选择离两个非常近的点
>   + 然后计算两个点的平均值用以估计梯度
>   + 检查估算的梯度值与反向传播算法得到的梯度DVec（将$D^{(1)},D^{(2)},...$展开为向量）是否接近
>   + **关掉梯度检验**，使用反向传播进行学习$\quad$`梯度检验速度很慢`

$\quad\dfrac{\partial}{\partial\theta_i}J(\theta)\approx\dfrac{J(\theta_1,\theta_2,...,\theta_i+\epsilon,...\theta_n)-J(\theta_1,\theta_2,...,\theta_i-\epsilon,...\theta_n)}{2\epsilon}\qquad\color{salmon}(\epsilon是很小的正数)$

```python
for i = 1 : n,
    thetaPlus = theta;
    thetaPlus(i) = thetaPlus(i) + EPSILON;
    thetaMinus = theta;
    thetaMinus(i) = thetaMinus(i) – EPSILON;
    gradApprox(i) = (J(thetaPlus) – J(thetaMinus))/(2*EPSILON);
end;
```

+ 例题

![GradientCheckQuestion](GradientCheckQuestion.png)

### 8. 随机初始化

+ 如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值
+ 同理，如果我们初始所有的参数都为同一个非 0 的数，结果也是一样的

> 我们通常初始参数为正负$\epsilon$之间的**随机值**
>
> 如果权重矩阵 $\Theta^{(l)}$ 初始化过大，矩阵相乘的输出范围会非常大（例如 -400 到 400 之间的数值），这会使得向量$a^{(l)}$上的所有输出几乎是二元的：0 或 1。但如果是这样，S 型非线性函数的局部梯度 $a^{(l)}.*(1-a^{(l)})$ 在两种情况下都会是 0（梯度消失），使得$x$和$\Theta^{(l)}$的梯度都是 0。在链式反应下，之后的反向传播相乘之后得到的都会是 0。

假设我们要随机初始一个尺寸为 10×11 的参数矩阵，代码如下：

```python
Theta1 = rand(10, 11) * (2*eps) – eps
```

选取$\epsilon$的一个有效策略是基于网络中的单元数，如对于$\Theta^{(l)},\ \epsilon=\dfrac{\sqrt{6}}{\sqrt{s_l+s_{l+1}}}$

###9. 训练神经网络的步骤

1. 选择一个网络架构$\quad$`神经元之间的连接样式`

   + 输入单元的数量：特征$x^{(i)}$的维度

   + 输出单元的数量：分类数

     > 合理的默认选择：1个隐含层，或若有多个隐含层，则在每一层中有相同数量的隐藏单元
     >
     > （通常情况下隐含层单元的个数越多越好）

2. 训练过程

   > 1. 随机初始化权重(参数)
   >
   > 2. 利用前向传播方法计算所有输入$x^{(i)}$的$h_\Theta(x^{(i)})$
   >
   > 3. 编写计算代价函数$J(\Theta)$ 的代码
   >
   > 4. 利用反向传播方法计算偏导数$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$
   >
   > 5. 利用数值检验方法比较用反向传播计算的和用$J(\Theta)$的梯度的数值估计得到的$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$
   >
   >    `然后禁用梯度检查的代码，因为梯度检查速度很慢`
   >
   > 6. 使用梯度下降或优化算法与反向传播算法相结合，来使$J(\Theta)$最小化
   >

   `J(Θ)是非凸函数，因此梯度下降可能得到一个局部最优点`

###* Geoffrey Hinton关于BP的看法



<div STYLE="page-break-after: always;"></div>

## 五、应用机器学习的建议

###1. 交叉验证(Cross Validation) 

> $J(\theta)=\dfrac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\dfrac{\lambda}{2m}\sum\limits_{j=1}^m\theta_j^2$
>
> 使用 60% 的数据作为训练集$\qquad\qquad J_{train}(\theta)=\dfrac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
>
> 使用 20% 的数据作为交叉验证集$\qquad\ J_{cv}(\theta)=\dfrac{1}{2m_{cv}}\sum\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$
>
> 使用 20% 的数据作为测试集$\qquad\qquad J_{test}(\theta)=\dfrac{1}{2m_{test}}\sum\limits_{i=1}^{m_{test}}(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^2$
>
> + k-fold交叉验证
>
>   把原始的数据随机分成k个部分，每次将其中1个作为测试集，剩下k-1个作为训练集进行训练

###2. 模型选择

> 1.  用训练集训练出 n 个模型（对应不同的多项式次数）
> 2.  对每个模型用训练集最优化$\Theta$中的参数
> 3.  分别计算 n 个模型在交叉验证集上的误差，找出误差最小的模型对应的多项式次数d
> 4.  用测试集和$J_{test}(\Theta^{(d)})$计算泛化误差(generalization error)；多项式次数d没有用测试集进行训练

![CrossValidationQuestion](CrossValidationQuestion.png)

###3. 偏差(Bias)与方差(Variance)

$\mbox{Error}=\mbox{Bias}^²+\mbox{Variance}+\mbox{Noise}^²$

$\quad\mbox{Bias}[\hat{f}(x)]=\mbox{E}[\hat{f}(x)-f(x)]$

$\quad\mbox{Var}[\hat{f}(x)]=\mbox{E}[\hat{f}(x)^2]-\mbox{E}[f(x)]^2$

$\quad\sigma^2:$不可消除的误差（噪声$\epsilon$），假设均值为0，方差为$\sigma^2$

> 由于$f(x)$是确定性的，故$\mbox{E}f(x)=f$
>
> 给定$y=f+\epsilon$，其中$\mbox{E}[\epsilon]=0$，则$\mbox{E}[y]=\mbox{E}[f+\epsilon]=\mbox{E}[f]=f$
>
> $\mbox{Var}[y]=\mbox{E}[(y-\mbox{E}[f])^2]=\mbox{E}[(y-f)^2]=\mbox{E}[(f+\epsilon-f)^2]=\mbox{E}[\epsilon^2]=\mbox{Var}[\epsilon]+\mbox{E}[\epsilon]^2=\sigma^2$
>
> $\begin{equation}\begin{aligned}\mbox{E}[(y-\hat{f})^2]&=\mbox{E}[y^2+\hat{f}^2-2y\hat{f}]\\&=\mbox{E}[y^2]+\mbox{E}[\hat{f}^2]-\mbox{E}[2y\hat{f}]\\&=\mbox{Var}[y]+\mbox{E}[y]^2+\mbox{Var}[\hat{f}]+\mbox{E}[\hat{f}]^2-2d\mbox{E}[\hat{f}]\\&=\mbox{Var}[y]+\mbox{Var}[\hat{f}]+(f^2-2f\mbox{E}[\hat{f}]+\mbox{E}[\hat{f}]^2)\\&=\mbox{Var}[y]+\mbox{Var}[\hat{f}]+(f-\mbox{E}[\hat{f}])^2\\&=\sigma^2+\mbox{Var}[\hat{f}]+\mbox{Bias}[\hat{f}]^2\end{aligned}\end{equation}$

![bias&variance](bias&variance.png)$\quad\begin{array}\\\mbox{Bias (underfit):}\\\quad \color{maroon}J_{train}(\theta)\mbox{will be high}\\\quad \color{maroon}J_{cv}(\theta)\approx J_{train}(\theta)\\\quad\\\mbox{Variance (overfit):}\\\quad \color{maroon}J_{train}(\theta)\mbox{will be low}\\\quad \color{maroon}J_{cv}(\theta)\gg J_{train}(\theta)\end{array}$

![bias&variance&lambda](bias&variance&lambda.png)$\qquad\qquad\quad$**正则化参数**$\lambda\Rightarrow\left\{\begin{array}{lcl}\lambda过大会导致欠拟合\\\quad\\\lambda偏小时可能出现过拟合\end{array}\right.$

> 选择合适的正则化参数λ
>
> + 尝试一系列λ（如0,0.001,0.003,0.01,0.03,0.1,0.3,1,3,10）
>
> + 画出$J_{cv}(\theta)$和$J_{train}(\theta)$随λ变化的曲线
>
> + 选择一个λ值使训练集和交叉验证集上的误差都较小
>
> + 代码实现
>
>   **计算$J_{cv}(\theta)$和$J_{train}(\theta)$时不使用正则化参数λ**
>
>   linearRegCostFunction函数见正则化线性回归的[代码实现](#· 代码实现)-计算代价与梯度
>
>   trainLinearReg函数见正则化线性回归的[代码实现](#· 代码实现)-训练过程
>
> ```octave
> # An Implementation of selecting lambda using octave 
> function [lambda_vec, error_train, error_val] = validationCurve(X, y, Xval, yval)
> %   Generate the train and validation errors needed to plot a validation curve that we can 
> %   use to select lambda.
> %   Return the train and validation errors (in error_train, error_val) for different values %   (X, y) : training set
> %   (Xval, yval) :  validation set
> lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]';
> for i = 1:length(lambda_vec)
>   lambda = lambda_vec(i);
>   theta = trainLinearReg(X, y, lambda);
>   lambda_x = zeros(size(lambda));
>   [error_train(i),~] = linearRegCostFunction(X, y, theta, lambda_x);
>   [error_val(i),~] = linearRegCostFunction(Xval, yval, theta, lambda_x);
> end
> end
> ```
>
> ![selectLambda](selectLambda.png)

偏差-方差权衡(Bias–variance tradeoff)$\quad$`有监督学习的核心问题`

+ 同时使偏差和方差最小

+ 高偏差会导致算法丢失特征与目标输出的相关关系$\quad$`欠拟合`

  + 增加训练数据帮助不大

    $\quad$![getMoreTrainingData](getMoreTrainingDataForBias.png)

+ 高方差会导致算法对训练集中的随机噪声建模，而不是预设的输出$\quad$`过拟合`

  + 增加训练数据可能会有帮助

    $\quad$![getMoreTrainingDataForVar](getMoreTrainingDataForVar.png)

###4. 学习曲线

对学习算法的一个**合理检验**

![learningCurve](learningCurve.png)

> + 代码实现
>
> **绘制学习曲线时不使用正则化参数λ**
>
> linearRegCostFunction函数见正则化线性回归的[代码实现](#· 代码实现)-计算代价与梯度
>
> trainLinearReg函数见正则化线性回归的[代码实现](#· 代码实现)-训练过程
>
> ```octave
> # An Implementation of Learning Curve using octave 
> function [error_train, error_val] = learningCurve(X, y, Xval, yval, lambda)
> %   returns the train and cross validation set errors for a learning curve. 
> %   In particular, it returns two vectors of the same length - error_train and error_val.
> %   Then, error_train(i) contains the training error for i examples (and similarly for 
> %   error_val(i)).
> m = size(X, 1);
> %   Set the lambda argument to 0
> lambda_x = zeros(size(lambda));
>
> %   Loop over the examples
> for i = 1:m
>   theta = trainLinearReg(X(1:i, :), y(1:i), lambda);
>   %  Evaluate the training error on the first i training examples (i.e., X(1:i, :) and y(1:i))
>   [error_train(i),~] = linearRegCostFunction(X(1:i, :), y(1:i), theta, lambda_x);
>   %  Evaluate the cross-validation error on the entire cross validation set (Xval and yval)
>   [error_val(i),~] = linearRegCostFunction(Xval, yval, theta, lambda_x);
> end
>
> %   Plot learing curves
> plot(1:m, error_train, 1:m, error_val);
> title('Learning curve for linear regression')
> legend('Train', 'Cross Validation')
> xlabel('Number of training examples')
> ylabel('Error')
> end
> ```
>
> ![learningCurveOctave](learningCurveOctave.png)

###5. 调试一个学习算法

假设实现正则化线性回归时，发现预测结果的误差很大，接下来应该怎么尝试？

+ 扩大训练集$\qquad\qquad\qquad\qquad\qquad\qquad\qquad$(解决**高方差**问题)
+ 尝试更小的特征集$\qquad\qquad\qquad\qquad\qquad\quad\ $(解决**高方差**问题)
+ 尝试增加额外特征$\qquad\qquad\qquad\qquad\qquad\quad\ $(解决**高偏差**问题)
+ 尝试增加多项式特征（$x_1^2,x_2^2,x_1x_2$等）$\qquad\ \ \ $(解决**高偏差**问题)
+ 尝试减小λ$\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ $(解决**高偏差**问题)
+ 尝试增大λ$\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ $(解决**高方差**问题)

###6. 机器学习系统设计

> 1. 从一个简单、能快速实现的算法开始实现，并用交叉验证集的数据测试它
>
> 2. 画出学习曲线来决定是否使用更多数据、更多特征会有帮助
>
> 3. 误差分析：手动检查算法在**交叉验证集**中出现误差的样例，观察其出现误差的数据样例类型是否呈现出任何系统趋势
>
>    *Error analysis may not be helpful for deciding if this is likely to improve performance.*
>
>    *Only solution is to try it and see if it works.*

![ErrorAnalysisQuestion](ErrorAnalysisQuestion.png)

####· 偏斜类(skewed classes)的误差度量

>    `训练集中有非常多的同一种类的样本数据，只有很少或没有其他类的样本数据(导致误差增大)`
>
>    + 准确率(Accuracy)
>
>      $\mbox{Accuracy}=\dfrac{\mbox{True positive}+\mbox{True negative}}{\bf\mbox{total examples}}$
>
>
>    + 精确率(Precision)与召回率(Recall)
>
>      + 精确率/查准率：对于给定的测试数据集，预测正确的(正类)样本数与所有被预测的(正类)样本数之比
>
>        $\mbox{Precision}=\dfrac{\mbox{True positive}}{\bf\mbox{predicted as positive}}=\dfrac{\color{blue}\mbox{True positive}}{{\color{blue}\mbox{True positive}}+{\color{red}\mbox{False positive}}}$
>
>        $(P=\dfrac{TP}{TP+FP})$
>
>      + 召回率/查全率：对于给定的测试数据集，预测正确的(正类)样本数与所有应该被正确分类(正类)的样本数之比
>
>        $\mbox{Recall}=\dfrac{\mbox{True positive}}{\bf\mbox{actual positive}}=\dfrac{\color{blue}\mbox{True positive}}{{\color{blue}\mbox{True positive}}+{\color{blueviolet}\mbox{False negative}}}$
>
>        $(R=\dfrac{TP}{TP+FN})$
>
>        ![precision&recall](precision&recall.png)$\quad y=1$表示我们希望检测的样本中很少的那类
>
>      + 权衡精确率与召回率
>
>        + 若希望只在非常确信的情况下预测为正类$\ \Longrightarrow\ \left\{\begin{array}{lcl}\mbox{Higher precision}\\\mbox{Lower recall}\end{array}\right.$
>
>        + 若希望提高查全率$\ \Longrightarrow\ \left\{\begin{array}{lcl}\mbox{Higher recall}\\\mbox{Lower precision}\end{array}\right.$
>
>        + 一般地，$\left\{\begin{array}{lcl}当h_\theta(x)\ge\mbox{threshold}时预测为1\\当h_\theta(x)\lt\mbox{threshold}时预测为0\end{array}\right.$
>
>        + P-R曲线
>
>          ![precision-recallCurve](precision-recallCurve.png)
>
>          + 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者
>          + 平衡点(Break-Even Point, BEP)：查准率=查全率时的取值
>            + 基于BEP比较，可认为平衡点的值较大者性能更优
>
>      + $F_1$度量
>
>        精确率和召回率的调和均值
>
>        $\dfrac{2}{F_1}=\dfrac{1}{P}+\dfrac{1}{R}$
>
>        即$F_1=\dfrac{2PR}{P+R}=\dfrac{2TP}{2TP+FP+FN}\qquad\Longrightarrow\ \left\{\begin{array}{lcl}P=0\ \mbox{or}\ R=0\ \color{maroon}\ \ \ \Rightarrow\ F=0\\P=1\ \mbox{and}\ R=1\ \color{maroon}\Rightarrow\ F=1\end{array}\right.$
>
>        `在交叉验证集上测得精确率P和召回率R，并选取使F₁最大的阈值`

#### · 使用大量数据集

> 假设使用一种需要**大量参数**的学习算法（有很多特征的逻辑回归/线性回归，有许多隐藏单元的神经网络），特征值有**足够的信息**来预测 y 值$\quad$`低偏差`
>
> $\quad J_{train}(\theta)\ $ will be small
>
> 使用很大的训练集（不太可能过拟合）`低方差`
>
> $\quad J_{train}(\theta)\approx J_{test}(\theta)$
>
> $\qquad\Longrightarrow J_{test}(\theta)\ $ will be small

<div STYLE="page-break-after: always;"></div>

##六、支持向量机(Support Vector Machine)

`适用于复杂但数据集属于中小型的分类`

`通过一个超平面将不同分类的数据分离开`

###1. 模型表示

$h_\theta(x)=g(\theta^Tx)=\left\{\begin{array}{lcl}1&\mbox{if }\theta^Tx\ge 0\\0&\mbox{otherwise}\end{array}\right.$

###2. 代价函数(Cost function)  

> 对比逻辑回归：
>
> + 用更简单的函数替代逻辑回归的cost(z)函数
> + 去掉$\dfrac{1}{m}$（不影响求解θ最优值的结果）
> + $\sum\limits_{i=1}^n\theta_j^2$前面的λ替换成$\sum\limits_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]$前面的*C*（SVM更关心第一项的优化，逻辑回归更关心后一项即θ的优化；C的取值很大意味着给前一项更大的权重，C的取值很小则意味着给后一项更大的权重，类似于逻辑回归中λ取值很大的情况）

$J(\theta)=C\sum\limits_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\dfrac{1}{2}\sum\limits_{j=1}^n\theta_j^2$

![svm_cost](svm_cost.png)

假设*C*很大：

+ if y=1, we want $\theta^Tx\ge 1(\mbox{not just} \ge 0)$
+ if y=0, we want $\theta^Tx\le -1(\mbox{not just} \lt 0)$

###3. 线性SVM

`不使用核函数("线性核函数")`$k(x_1,x_2)=x_1^Tx_2=\langle x_1,x_2\rangle$

如果 **C 非常大**，则在最小化代价函数时希望使

$\qquad\qquad\sum\limits_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]=0$

$\qquad\qquad$即$\left\{\begin{array}{lcl}\theta^Tx^{(i)}\ge 1&\mbox{if }y^{(i)}=1\\\theta^Tx^{(i)}\le -1&\mbox{if }y^{(i)}=0\end{array}\right.$

$\qquad\qquad$则$J(\theta)=\dfrac{1}{2}\sum\limits_{j=1}^n\theta_j^2$

> `向量内积`
>
> ![VectorInnerProduct](VectorInnerProduct.png)
>
> + p为$\vec{v}$投影到$\vec{u}$上的长度$\ \Rightarrow\ \left\{\begin{array}{lcl}p\gt 0&\vec{u}\mbox{和}\vec{v}\mbox{的夹角}\lt90^\circ\\p\lt 0&\vec{u}\mbox{和}\vec{v}\mbox{的夹角}\gt90^\circ\end{array}\right.$
> + $\vec{u}^T\vec{v}=p\cdot{\Vert\vec{u}\Vert}_2=u_1v_1+u_2v_2$

+ 考虑两个特征的情况$\vec{x}^{(i)}=\begin{pmatrix} x_1^{(i)}\\ x_2^{(i)}\end{pmatrix}$，忽略截距(*θ*~0~=0)，则

  $\qquad J(\theta)=\dfrac{1}{2}(\theta_1^2+\theta_2^2)=\dfrac{1}{2}(\sqrt{\theta_1^2+\theta_2^2})^2=\dfrac{1}{2}\Vert\theta\Vert_2^2$

  $\qquad\qquad\Longrightarrow$使$J(\theta)$最小，即最小化$\Vert\theta\Vert_2^2$，也即向量*θ*长度的平方

  $\qquad\theta^Tx^{(i)}=p^{(i)}\cdot{\Vert\vec{\theta}\Vert_2}=\theta_1x_1^{(i)}+\theta_2x_2^{(i)}$



  + 训练数据是**线性可分**的——Hard-margin

    `硬间隔：所有样本都必须划分正确`

    $\left\{\begin{array}{lcl}C\mbox{ is very large}\left\{\begin{array}{lcl}\theta^Tx^{(i)}\ge 1&\mbox{if }y^{(i)}=1\\\theta^Tx^{(i)}\le -1&\mbox{if }y^{(i)}=0\end{array}\right.\\\quad\\\theta^Tx=wx+b\quad(b=\theta_0)\end{array}\right.\Longrightarrow\left\{\begin{array}{lcl}wx+b\le-1\\wx+b\ge1\end{array}\right.$

    ![svmMargin](svmMargin.png)

    > + 选择两条平行的超平面(hyperplane)将两类数据分开，使得它们之间的距离尽可能大
    > + 被这两个超平面界定的区域称作"间距"(margin)
    > + 如果距离分类超平面最近的数据点的距离(margin)越大，该分类器的噪声容忍度就越大，即鲁棒性更好
    > + 具有最大间距的超平面就在这两个超平面正中间$\quad$`判定边界(Decision Boundary)`
    >
    > $\left\{\begin{array}{lcl}\theta^Tx^{(i)}\ge 1&\mbox{if }y^{(i)}=1\\\theta^Tx^{(i)}\le -1&\mbox{if }y^{(i)}=0\end{array}\right.\Longrightarrow\left\{\begin{array}{lcl}p^{(i)}\cdot{\Vert\theta\Vert}_2\ge 1&\mbox{if }y^{(i)}=1\\p^{(i)}\cdot{\Vert\theta\Vert}_2\le -1&\mbox{if }y^{(i)}=0\end{array}\right.$
    >
    > + 对于正样本，需要$p^{(i)}\cdot{\Vert\theta\Vert}_2\ge 1$，因而当$p^{(i)}$相对大时，$\theta$就会更小
    >
    > 对于硬间距SVM，支持向量是那些在margin上的数据点


`大间距分类器(Large margin classifier)`

$\quad$努力将正样本和负样本用最大间距分开

$\qquad\qquad$![svmDecisionBoundary](svmDecisionBoundary.png)


  + 训练数据**不完全线性可分**——Soft-margin

    > `重新定义`
    >
    > 为方便推导，定义$h_\theta(x)=g(\theta^Tx)=g(w^Tx+b)=\left\{\begin{array}{lcl}1&\mbox{if }w^Tx+b\ge +1\\-1&\mbox{if }w^Tx+b\le -1\end{array}\right.$，样本点$(x_i,y_i)$的$y_i$取值1或-1
    >
    > + 定义样本点$(x_i,y_i)$与超平面$(w, b)$之间的**函数间隔(functional margin)**为$\boxed{\gamma_{i} = y_{i} (w^Tx_{i} + b)}\quad\color{salmon}\begin{array}{lcl}该定义存在问题：即w和b同时缩小或放大M倍后，超平面并没有变化，\\但是函数间隔却变化了(未归一化的距离)\end{array}$
    >
    >   $\gamma_i\gt0$意味着样本点$(x_i,y_i)$被正确分类
    >
    >
    > + 定义样本点$(x_i,y_i)$与超平面$(w, b)$之间的**几何间隔(geometric margin)**为$\boxed{\gamma_{i} = y_{i} (\dfrac{w^T}{\Vert w\Vert_2}x_{i} + \dfrac{b}{\Vert w\Vert_2})}\quad\color{salmon}\begin{array}{lcl}实际上，几何距离就是点到超平面的距离，类比点(x_i,y_i)到直ax\\+by+c=0的距离公式d(x_i,y_i)=\dfrac{|ax_i+by_i+c|}{\sqrt{a^2+b^2}}\end{array}$
    >
    >   对于线性可分的训练集来说，间隔的值都将是正值
    >
    > + 假设超平面$(w,b)$能将训练样本正确分类，则有$\left\{\begin{array}{lcl}w^Tx_i+b\ge+1,&y_i=+1\\w^Tx_i+b\le-1,&y_i=-1\end{array}\right.$
    >
    >   **支持向量(Support Vector)**：距离超平面最近的几个使上式等号成立的训练样本点(函数间隔为1)
    >
    >   **间隔(margin)**：两个异类支持向量到超平面的距离之和，即$\gamma=\dfrac{2}{\Vert w\Vert_2}$
    >
    > + SVM可以表述为求解下列优化问题
    >   $\color{maroon}\underset{w, b}{\mbox{max}} \;\gamma$
    >
    >   $\quad\color{maroon}\mbox{s.t.} \;\;\; y_{i} (\dfrac{w^T}{\Vert w\Vert_2}x_{i} + \dfrac{b}{\Vert w\Vert_2})\ge \gamma,\quad i=1,2,...,m$
    >
    >   显然，为了最大化间隔，仅需最小化$\Vert w\Vert_2^2$，于是问题等效于
    >
    >   $\color{maroon}\underset{w, b}{\mbox{min}}\ \dfrac{1}{2}\Vert w\Vert_2^2$
    >
    >   $\quad\color{maroon}\mbox{s.t.} \;\;\; y_{i} (w^Tx_i+b)\ge 1,\quad i=1,2,...,m$
    >
    >   + 由于目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划(convex quadratic programming)问题，可以用现成的QP优化包进行求解
    >
    >   + 还可以通过拉格朗日对偶性 (Lagrange Duality) 变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题 (dual problem) 得到原始问题的最优解
    >
    >     对于约束条件为等式的情况，采用拉格朗日乘数法
    >
    >     对于约束条件为不等式的情况，需满足KKT条件(Karush–Kuhn–Tucker conditions)
    >
    >     > ####KKT条件
    >     >
    >     > 对于优化问题
    >     >
    >     > $\qquad\sideset{^{\color{RubineRed}\Large\min}}{_{\color{MidnightBlue}\Large\max}}{\mathop{\diagup}}\ f(x)$
    >     >
    >     > $\qquad\quad\;\;{\color{maroon}\text{s.t.}}\;\;\;g_i(x)\le0,\\\qquad\qquad\quad\ \ h_j(x)=0$
    >     >
    >     > 其中$\ g_i\ (i=1,2,\cdots,m):\mathbb{R}^n\mapsto\mathbb{R}$为$m$组不等式约束函数，
    >     > $\qquad h_j\ (j=1,2,\cdots,\ell):\mathbb{R}^n\mapsto\mathbb{R}$为$\ell$组等式约束函数
    >     > $\qquad f(x):\mathbb{R}^n\mapsto\mathbb{R}$为目标函数
    >     >
    >     > 设$x^*$为局部最优点，若优化问题满足以下条件，则存在常数$\mu_i\ (i=1,2,\cdots,m)$和$\lambda_j\ (j=1,2,\cdots,\ell)$，称为KKT乘子，使得$\ \sideset{^{\color{RubineRed}\Large-}_{\color{MidnightBlue}\Large+}}{}{\mathop{\nabla}} f(x^{*})=\sum\limits_{i=1}^{m}\mu _{i}\nabla g_{i}(x^{*})+\sum\limits_{j=1}^{\ell }\lambda _{j}\nabla h_{j}(x^{*})$
    >     >
    >     > $\qquad$需满足的条件：$\left\{\begin{array}{cl}g_i(x^*)\le0,&\text{for }i=1,2,\cdots,m\\h_j(x^*)=0,&\text{for }j=1,2,\cdots,\ell\\\mu_i\ge0,&\text{for }i=1,2,\cdots,m\\\mu_ig_i(x^*)=0,&\text{for }i=1,2,\cdots,m&{\color{Bittersweet}(互补松弛性)}\end{array}\right.$
    >     >
    >     > 当$m=0$时，KKT条件即为拉格朗日条件，KKT乘子即为拉格朗日乘子
    >
    >     + 构造拉格朗日函数$L(w,b,\alpha)=\dfrac{1}{2}\Vert w\Vert_2^2-\sum\limits_{i=1}^m\alpha_i\left(y_i(\vec{w}^T\vec{x}_i+b)-1\right)$
    >
    >     + 对$w$和$b$求偏导数，令偏导数为0
    >
    >       $\dfrac{\partial L}{\partial w}=\Vert w\Vert_2-\sum\limits_{i=1}^m\alpha_iy_ix_i{\color{violet}=0}\ \Longrightarrow\ \color{violet}\Vert w\Vert_2=\sum\limits_{i=1}^m\alpha_iy_ix_i$
    >
    >       $\dfrac{\partial L}{\partial b}=-\sum\limits_{i=1}^m\alpha_iy_i{\color{violet}=0},\ \alpha_i\ge0$为拉格朗日乘子
    >
    >     + 将上式代回拉格朗日函数，得到拉格朗日对偶问题
    >
    >       $\begin{equation}\begin{aligned}L&=\dfrac{1}{2}\Vert w\Vert_2^2-\left(\sum\limits_{i=1}^m\alpha_iy_ix_i\right)w-\left(\sum\limits_{i=1}^m\alpha_iy_i\right)b+\sum\limits_{i=1}^m\alpha_i\\&=\sum\limits_{i=1}^m\alpha_i-\dfrac{1}{2}\Vert w\Vert_2^2=\sum\limits_{i=1}^m\alpha_i-\dfrac{1}{2}\left(\sum\limits_{i=1}^m\alpha_iy_ix_i\right)^2\\&=\sum\limits_{i=1}^m\alpha_i-\dfrac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_jx_ix_j,\quad\alpha_i\ge0\end{aligned}\end{equation}$
    >
    >     + 则原问题转化为求解满足下列等式的$\alpha_i$:
    >
    >       $\min\left(\dfrac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum\limits_{i=1}^m\alpha_i\right)$
    >
    >       $\quad\mbox{s.t.} \;\;\; \sum\limits_{i=1}^m\alpha_iy_i=0,\\\qquad\quad\alpha_i\ge 0,\;\; i=1,2,...,m$
    >
    >     + 解出$\alpha$后（可用[SMO算法](#* SMO(Sequential Minimal Optimization)算法)求解），求出$w$与$b$即可得到模型$f(x)=w^Tx+b=\sum\limits_{i=1}^m\alpha_iy_ix_i^Tx+b$
    >
    >     + 从对偶问题中解出的$\alpha_i$是拉格朗日乘子，需满足的[KKT条件](#KKT条件)为
    >       $\left\{\begin{array}{lcl}\alpha_i\ge0,\\y_if(x_i)-1\ge0,\\\alpha_i\left(y_if(x_i)-1\right)=0.\end{array}\right.$
    >
    >       于是，对任意训练样本$(x_i,y_i)$，总有$\alpha_i=0$或$y_if(x_i)=1$
    >
    >       + 若$\alpha_i=0$，则该样本将不会出现在最终模型$f(x)=\sum\limits_{i=1}^m\alpha_iy_ix_i^Tx+b\ $中
    >       + 若$\alpha_i\gt0$，则必有$y_if(x_i)=1$，所对应的样本点位于最大间隔边界上，是一个**支持向量**
    >
    >       $\ \color{maroon}\Longrightarrow\ $**训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关**
    >
    > #### * SMO(Sequential Minimal Optimization)算法
    >
    > [Sequential Minimal Optimization : A Fast Algorithm for Training Support Vector Machines](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf)
    >
    > + 基本思路：先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值$\quad$`高效`
    >
    >   但若每次固定**一个**变量$\alpha_i$外的所有参数，则该$\alpha_i$可由约束条件$\sum\limits_{i=1}^m\alpha_iy_i=0$推出，即$\alpha_i$不再是变量；
    >
    >   因此每次选择**两个**变量$\alpha_i$和$\alpha_j$，并固定其他参数，在参数初始化后，不断执行如下两个步骤直至收敛：
    >
    >   1. 选取一对需更新的变量$\alpha_i$和$\alpha_j$ ；
    >
    >   2. 固定$\alpha_i$和$\alpha_j$以外的参数，求解$\min\left(\dfrac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j-\sum\limits_{i=1}^m\alpha_i\right)$，获得更新后的$\alpha_i$和$\alpha_j$
    >
    >      此时约束条件可重写为$a_iy_i+\alpha_jy_j=-\sum\limits_{k\neq i,j}\alpha_ky_k=$常数，代回对偶问题中，即可得到关于$\alpha_i$或$\alpha_j$的单变量二次规划问题，仅有的约束是$\alpha_i\ge0$，该问题具有闭式解(closed-form)/解析形式的解，容易计算
    >
    >   3. 偏移项b可由$\;\;\forall(x_s,y_s),\ \ y_sf(x_s)=y_s\left(\alpha_iy_ix_i^Tx_s+b\right)=1\;\;$确定（其中$(x_s,y_s)$为支持向量）
    >
    >      理论上，任意选取一个支持向量，则$b=\dfrac{1}{y_s}-\alpha_iy_ix_i^Tx_s$
    >
    >      通常取所有支持向量求解的平均值：$b=\dfrac{1}{|S|}\sum\limits_{s\in S}\left(b=\dfrac{a}{y_s}-\sum\limits_{i\in S}\alpha_iy_ix_i^Tx_s\right)$

    `软间距：容许一些误分类点`

    ![soft-marginSVM](soft-marginSVM.png)

    >
    > + 线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件$y_{i}(w^{T}x_{i}+b) \ge 1$
    >
    >   软间距SVM的优化目标是在最大化间隔的同时，不满足约束的样本应尽可能少，即
    >
    >   $\color{maroon}\underset{w, b}{\mbox{min}}\ \dfrac{1}{2}\lVert w \rVert^{2}_2 + C\sum\limits_{i=1}^{m}\ell_{0/1}(y_i(w^Tx_i+b)-1)$
    >
    >   其中C>0是一个常熟，$\ell_{0/1}$是0/1损失函数：$\ell_{0/1}(z)=\left\{\begin{array}{lcl}1,&\mbox{if}\ z\lt0\\0,&\mbox{otherwise}\end{array}\right.$
    >
    >   $\ell_{0/1}$非凸、非连续，数学性质不太好，使得优化目标不易直接求解，人们常用其他函数代替$\ell_{0/1}$，称为"替代损失"：
    >
    >   $\qquad\begin{array}{lcl}hinge损失:\ \ell_{hinge}(z)=\mbox{max}(0,1-z)\\指数损失:\ \ell_{exp}(z)=exp(-z)\\对率损失:\ \ell_{log}(z)=log(1+exp(-z))\end{array}$
    >
    >   若采用hinge损失，则优化目标变为
    >
    >   $\color{maroon}\underset{w, b}{\mbox{min}}\ \dfrac{1}{2}\lVert w \rVert^{2}_2 + C\sum\limits_{i=1}^{m}\mbox{max}(0,1-y_i(w^Tx_i+b))$
    >
    >+ 对每个样本点，引入一个**松弛变量(slack variable)**$\ \xi_{i} \ge 0$, 使函数间隔加上松弛变量大于等于1，这样约束条件变为：$y_{i}(w^{T}x_{i}+b) \ge 1 - \xi_{i}$
    >
    >  同时，对于每个松弛变量$\xi_{i}$，要付出一个代价，目标函数由原来的$\frac{1}{2}\lVert w \rVert^{2}_2$变为：
    >
    >  $\color{maroon}\underset{w, b, \xi_i}{\mbox{min}}\ \dfrac{1}{2}\lVert w \rVert^{2}_2 + C\sum\limits_{i=1}^{m}\xi_{i}$
    >
    >  $\quad\color{maroon}\begin{array}{lcl}\mbox{s.t.} \;\;\;&y_{i} (w^Tx_i+b)\ge 1-\xi_i\\\;&\xi_i\ge0,\ i=1,2,...,m\end{array}$
    >
    >  其中，C>0为惩罚参数。C越大表示对错误分类的惩罚力度越大，越小则表示惩罚力度越小
    >
    >  > + 引入对偶变量$u,v\ge0$，构建拉格朗日函数$\color{tan}L(w,b,\xi,u,v)=\dfrac{1}{2}\Vert\vec{w}\Vert^{2}_2 + C\sum\limits_{i=1}^{m}\xi_{i}-\sum\limits_{i=1}^{m}u_i\xi_{i}+\sum\limits_{i=1}^{m}v_i\left(1-\xi_{i}-y_i(\vec{w}^T\vec{x}_i+b)\right)$
    >  >
    >  > + 对$w$ 、$b$和$\xi$求偏导数，令偏导数为0
    >  >
    >  >   $\nabla_w L(w,b,\xi,u,v)=\nabla_w\left(w^Tw-\sum\limits_{i=1}^mv_iy_iw^Tx_i\right){\color{violet}=0}\ \Longrightarrow\ \color{violet}\Vert w\Vert_2=\sum\limits_{i=1}^m v_iy_ix_i$
    >  >
    >  >   $\nabla_b L(w,b,\xi,u,v)=\sum\limits_{i=1}^m v_iy_i=\vec{v}^T\vec{y}{\color{violet}=0}$
    >  >
    >  >   $\nabla_{\xi} L(w,b,\xi,u,v)=C\mathbf{I}-\vec{u}-\vec{v}{\color{violet}=0}\ \Longrightarrow\ \color{violet}\vec{v}=C\mathbf{I}-\vec{u}$
    >  >
    >  > + 通过最小化$w,b,\xi$，得到拉格朗日对偶函数
    >  >
    >  >   $\begin{equation} g(u,v) = \begin{cases} -\frac{1}{2}v^T(diag(Y)X)(diag(Y)X)^Tv +I^Tv, & \text{if} \ v^Ty=0,\, v=C\mathbf{I} – u \\ -\infty \quad & \text{otherwise} \end{cases} \end{equation}$
    >

###4. 非线性SVM —— 核函数(kernel)

`kernel trick / kernel method`

对于线性不可分的数据，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题

[SVM可视化](https://v.qq.com/x/page/k05170ntgzc.html)

![kernel](kernel.png)

给定一个样本点，计算其到标记点(landmark)$\ l^{(1)},l^{(2)},l^{(3)},...l^{(n)}$的距离：

$\quad$相似度函数$f_i=\mbox{similarity}(x,l^{(i)})=k(x,l^{(i)})\ \color{maroon}\Longleftrightarrow\ 核函数\ \Longleftrightarrow\ 内积$

![landmarks](landmarks.png)

> 核函数不是从低维度到高维度的映射，而是对应于输入的两个向量映射到特征空间之后的向量内积，相关数学参见[泛函分析](#二、泛函分析(functional analysis))
>
> 将输入空间$X$映射到一个新的空间$F=\{\phi(x)|x\in X\}$，$F$称为特征空间
>
> 核函数对应于特征空间上的内积，即$K(x,y)=\langle\phi(x),\phi(y)\rangle$
>
> $\phi(x)​$表示将$x​$映射后的特征向量，则在特征空间中划分超平面所对应的模型可表示为$f(x)=w^T\phi(x)+b​$
>
> 优化目标变为$\begin{array}{lcl}\underset{w,b}{\mbox{min}}\;\;\dfrac{1}{2}\Vert w\Vert^2_2\\s.t.\;\;\;y_i(w^T\phi(x_i)+b)\ge1,&i=1,2,...,m\end{array}$
>
> 其对偶问题是$\color{violet}\underset{\alpha}{\mbox{max}}\;\;\sum\limits_{i=1}^m-\dfrac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m\alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j)\Vert w\Vert^2_2\\\color{violet}s.t.\;\;\;\sum\limits_{i=1}^m\alpha_iy_i=0,\\\color{violet}\qquad\ \ \alpha_i\ge0,\quad i=1,2,...,m$
>
> + 求解上述问题涉及到计算$\phi(x_i)^T\phi(x_j)$，即样本$x_i$与$x_j$映射到特征空间后的内积
>
> + 由于特征空间维度可能很高，甚至可能是无穷维，因此直接计算$\phi(x_i)^T\phi(x_j)$通常比较困难
>
> + 由此引入核函数$K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle=\phi(x_i)^T\phi(x_j)$，可在原始样本空间通过其计算$x_i$与$x_j$在特征空间的内积
>
> + 将$K(x_i,x_j)$代入对偶问题，求解后即可得到
>
>   $\qquad\qquad\qquad \begin{equation}\begin{aligned}f(x)&=w^T\phi(x)+b\\&=\sum\limits_{i=1}^m\alpha_iy_i\phi(x_i)^T\phi(x)+b\\&=\sum_{i=1}^m\alpha_iy_iK(x,x_i)+b\end{aligned}\end{equation}$

显然，若已知合适映射$\phi(\cdot)$的具体形式，则可写出核函数$K(\cdot,\cdot)$

若不知道映射的形式，则使用以下定理帮助确定合适的核函数：

> 对于输入数据集$\{x^{(1)},x^{(2)},...,x^{(m)}\}$，定义**核矩阵(kernel matrix)**$\ \mathbf{K}$为
>
> $\qquad\mathbf{K}=\begin{pmatrix}K(x^{(1)},x^{(1)})&K(x^{(1)},x^{(2)})&\cdots&K(x^{(1)},x^{(m)})\\K(x^{(2)},x^{(1)})&K(x^{(2)},x^{(2)})&\cdots&K(x^{(2)},x^{(m)})\\\cdots&\cdots&\cdots&\cdots\\K(x^{(m)},x^{(1)})&K(x^{(m)},x^{(2)})&\cdots&K(x^{(m)},x^{(m)})\end{pmatrix}_{m\times{m}}$
>
> 若$K$是合法的核函数，则$\mathbf{K}_{ij}=K(x^{(i)},x^{(j)})=\phi(x^{(i)})^T\phi(x^{(j)})=\phi(x^{(j)})^T\phi(x^{(i)})=K(x^{(j)},x^{(i)})$<br>$=\mathbf{K}_{ji}$，即$\mathbf{K}$是**对称**矩阵
>
> 对于任意向量$\vec{z}$，有
> $\qquad\begin{equation}\begin{aligned}\vec{z}^T\mathbf{K}\vec{z}&=\sum\limits_i\sum\limits_j\vec{z}_i\mathbf{K}_{ij}\vec{z}_j=\sum\limits_i\sum\limits_j\vec{z}_i\phi(x^{(i)})^T\phi(x^{(j)})\vec{z}_j=\sum\limits_i\sum\limits_j\vec{z}_i\left[\sum\limits_k\phi_k(x^{(i)})^T\phi_k(x^{(j)})\right]\vec{z}_j\\&=\sum\limits_i\sum\limits_j\sum\limits_k\vec{z}_i\phi_k(x^{(i)})^T\phi_k(x^{(j)})\vec{z}_j=\sum\limits_k\left(\sum\limits_i\vec{z}_i\phi_k(x^{(i)})\right)^2{\color{maroon}\ge0}\end{aligned}\end{equation}$
> 故核矩阵$\mathbf{K}$是**半正定**矩阵
>
> ${\color{maroon}\Longrightarrow}\quad$若$K$是合法的核函数，则对应的核矩阵$\mathbf{K}$为半正定对称阵$\quad$`判断K为合法核函数的充分必要条件`

**在使用核函数之前须做特征缩放**

+ 高斯核函数(Gaussian kernel) / 径向基函数(Radial Basis Function)

  ![GaussianKernel](GaussianKernel.png)

  $\begin{equation}\begin{aligned}k(x,l^{(i)})&=\exp\left(-\dfrac{\Vert x-l^{(i)}\Vert^2}{2\sigma^2}\right)\qquad\begin{array}{lcl}(\sigma控制核函数随x改变的速率\\\ \ \sigma越大，核函数随x变化速率越慢)\end{array}\\&=\exp\left(-\dfrac{\sum\limits_{k=1}^n(x_k-l_k^{(i)})^2}{2\sigma^2}\right)\end{aligned}\end{equation}$

  + 若$x$接近$l^{(i)}$，则$f_i\approx1$

  + 若$x$远离$l^{(i)}$，则$f_i\approx0$

  + 向量化

    $K=\exp\left(-\dfrac{(x-l)^T(x-l)}{2\sigma^2}\right)\qquad\color{maroon}(x、l均为列向量)$

  **predict “y=1” when** $\theta_0+\theta_1f_1+\theta_2f_2+...+\theta_nf_n\ge0$

  + 选取landmarks

    > 给定训练样本$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$
    >
    > 选择$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},...,l^{(m)}=x^{(m)}\quad$(根据训练集的数量选择landmark的数量)
    >
    > $f^{(i)}=\begin{pmatrix} f_0^{(i)}\\ f_1^{(i)}\\ f_2^{(i)}\\...\\ f_{m}^{(i)}\end{pmatrix}=\begin{pmatrix} 1\\ sim(x^{(i)},l^{(1})\\ sim(x^{(i)},l^{(2})\\...\\\color{blueviolet}sim(x^{(i)},l^{(i)})=e^0=1\\...\\sim(x^{(i)},l^{(m)})\end{pmatrix}_{m\times{1}}$
    >
    > 假设函数：$h_\theta(x)=\theta^Tf(x)$
    >
    > $\underset{\theta}{\mbox{min}}\ C\sum\limits_{i=1}^my^{(i)}\mbox{cost}_1(\theta^Tf^{(i)})+(1-y^{(i)})\mbox{cost}_0(\theta^Tf^{(i)})+\dfrac{1}{2}\sum\limits_{j=1}^m\theta_j^2$

  + 高斯核函数能把向量映射到无穷维

    > 假设有n条k维的数据，那么高斯核函数把k维成功升到了n维，这时再加一条新的与原始数据不重合的样本，那么数据就可以映射到n+1维空间，与之前结论矛盾，因此高斯核函数能将数据升到无穷维
    >
    > $\quad{e^x} = 1 + x + \frac{{{x^2}}}{{2!}} + \frac{{{x^3}}}{{3!}} + \frac{{{x^4}}}{{4!}} + \cdots + \frac{{{x^n}}}{{n!}}+\cdots=\sum\limits_{j=0}^\infty\dfrac{x^j}{j!}$
    >
    > $\begin{equation}\begin{aligned}\quad\ {\color{maroon}\Longrightarrow}\ K({x_1},{x_2})&=\exp\left(-\frac{{{{\left\| {{x_1} - {x_2}} \right\|}^2}}}{{2{\sigma ^2}}}\right)=\exp\left(-\dfrac{x_1^2+x_2^2}{2}+x_1^Tx_2\right)\\&=\exp\left(-\dfrac{x_1^2+x_2^2}{2}\right)\exp\left(x_1^Tx_2\right)\\&\xlongequal[对后一项泰勒展开]{前一项为常数}\exp\left(-\dfrac{x_1^2+x_2^2}{2}\right)\sum\limits_{j=0}^\infty\dfrac{(x_1^Tx_2)^j}{j!}\\&=\exp\left(-\dfrac{x_1^2+x_2^2}{2}\right){\color{violet}\left[\cdots,\dfrac{(x_1^T)^j}{\sqrt{j!}},\cdots\right]^T\left[\cdots,\dfrac{(x_2^T)^j}{\sqrt{j!}},\cdots\right]}\qquad(内积形式)\end{aligned}\end{equation}$

  + 用高斯核函数的SVM训练出来的模型是否需要存储所有的原始数据？

    > 不需要。由于$w=\sum\limits_{i=1}^m\alpha_iy_ix_i$，非支持向量对应的$\alpha_i$都是0，所以只需要保存支持向量的数据即可

+ 核函数的选择

  需要满足Mercer’s Theorem

  > `Mercer's Theorem`
  > 设核函数$K:\mathbb{R}^n\times\mathbb{R}^n\mapsto\mathbb{R}$，则$K$为合法(Mercel)核函数的充分必要条件是
  > $\qquad$对于任何输入数据集$\{x^{(1)},x^{(2)},...,x^{(m)}\}\ (m\lt\infty)$，其对应的核矩阵$\mathbf{K}$是半正定对称矩阵

####· 常用核函数

+ 多项式核函数(Polynomial kernel)

  > $K(x,y) = (x^\mathsf{T} y + c)^{d}\qquad\begin{array}{lcl}\color{tan}(x,y是输入空间的n维向量，d是多项式次数，c\ge0\\\color{tan}是平衡多项式中高次项对低次项影响的自由参数)\end{array}$
  >
  > 当c=0时，称作齐次多项式核函数
  >
  > $\begin{equation}\begin{aligned}d=2时，K(x,y)&=\left(\sum_{i=1}^n x_i y_i + c\right)^2\\&=\sum_{i=1}^n \left(x_i^2\right) \left(y_i^2 \right) + \sum_{i=2}^n \sum_{j=1}^{i-1} \left( \sqrt{2} x_i x_j \right) \left( \sqrt{2} y_i y_j \right)+\sum_{i=1}^n \left( \sqrt{2c} x_i \right) \left( \sqrt{2c} y_i \right) + c^2\\对应的映射是\phi(x) &= \langle x_n^2, \ldots, x_1^2, \sqrt{2} x_n x_{n-1}, \ldots, \sqrt{2} x_n x_1, \sqrt{2} x_{n-1} x_{n-2}, \ldots, \sqrt{2} x_{n-1} x_{1}, \ldots,\\ &\sqrt{2} x_{2} x_{1}, \sqrt{2c} x_n, \ldots, \sqrt{2c} x_1, c \rangle\end{aligned}\end{equation}$

+ String kernel

+ chi-square kernel

+ histogram intersection kernel

两个核函数的线性组合也是核函数

两个核函数的乘积也是核函数

若$K_1$为核函数，则对于任意函数$g(\vec{x})$，$K(\vec{x},\vec{z})=g(\vec{x})K_1(\vec{x},\vec{z})g(\vec{z})$也是核函数

### 5. 向量化

|   变量    | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :-----: | :--------------------------------------- |
| feature | $x=\begin{pmatrix}x_1^{(1)}& x_2^{(1)}&...& x_{n}^{(1)}\\x_1^{(2)}& x_2^{(2)}&...& x_{n}^{(2)}\\...&...&...&...\\x_1^{(m)}& x_2^{(m)}&...& x_{n}^{(m)}\end{pmatrix}_{m\times{n}}=\begin{pmatrix}\left(\vec{x}^{(1)}\right)^T\\\left(\vec{x}^{(2)}\right)^T\\...\\\left(\vec{x}^{(m)}\right)^T\end{pmatrix}$ |
| output  | $y=\begin{pmatrix} y^{(1)}\\y^{(2)}\\...\\y^{(m)}\end{pmatrix}_{m\times{1}}$ |
|   $w$   | $w=\begin{pmatrix} w_1\\w_2\\...\\w_n\end{pmatrix}_{n\times{1}}$ |

### 6. 选择参数

+ $C\Longleftrightarrow\dfrac{1}{\lambda}$

  *Large C* : Lower bias, high variance

  *Small C* : Higher bias, low variance


+ $\sigma^2$

  *Large* $\ \sigma^2$ : 特征$f_i$变化平缓

  $\qquad\qquad\ \ $Higher bias, lower variance

  *Small* $\ \sigma^2$ : 特征$f_i$变化更剧烈

  $\qquad\qquad\ \ $Lower bias, higher variance

###7. 逻辑回归 vs. SVMs

n为特征数($x\in\mathbb{R}^{n+1}$)，m为训练样本数

+ 如果维度n很大（相对于m）`如n=10000,m=10~1000`：

  使用逻辑回归，或不使用核函数的SVM(“线性核函数”)

+ 如果维度n较小，而m为中等大小`如n=1~1000,m=10~10000`：

  使用高斯核函数的SVM

+ 如果维度n较小，m很大`如n=1~1000,m=50000+`：

  创造/增加更多特征，然后使用逻辑回归或不使用核函数的SVM(“线性核函数”)

神经网络可能对以上大多数情况都能很好工作，但可能训练较慢

<div STYLE="page-break-after: always;"></div>

## 七、无监督学习(unsupervised learning)

###1. 聚类(clustering)

将未标注的样本数据中相似的分为同一类，从而将数据集划分为若干个不相交的子集，每个子集称为一个簇(cluster)

### 2. K-均值(K-Means)

`迭代算法` [`典型的EM算法`](#3. EM(Expectation-Maximization)算法)

随机选取K个点，称为聚类中心(cluster centroid)

用$\mu_1,\mu_2,...,\mu_K\in\mathbb{R}^{(n)}$来表示聚类中心，$c^{(1)},c^{(2)},...,c^{(m)}$存储与第 $i$ 个样例$x^{(i)}\in\mathbb{R}^{(n)}$距离最近的聚类中心的索引

1. 对于数据集中的数据点，按照与 K 个聚类中心的距离进行聚类，将与同一个中心点距离最近的所有数据点聚成一类

   $c^{(i)}=j\ $ that minimizes $\ \Vert x^{(i)}-\mu_j\Vert^2$

2. 然后计算每一个聚类的平均值，将该聚类的中心点移动到平均值的位置

   $\mu_k=\dfrac{1}{|C_k|}\sum\limits_{i\in C_k}x^{(i)},\qquad C_k$是属于聚类k的样本点集合，$|C_k|$是集合$C_k$的大小

3. 重复上述两步，直到各聚类中心点不再变化为止

   ```python
   Repeat {
      # 对数据点进行聚类（减小c(i)导致的代价）
      for i = 1 to m
         c(i) := index (form 1 to K) of cluster centroid closest to x(i)
      # 重新计算聚类中心的位置（减小μ(k)导致的代价）
      for k = 1 to K
         μ(k) := average (mean) of points assigned to cluster k
   }
   ```

   ​

   ![k-means](k-means.png)

####· 优化目标(cost function)

损失函数/畸变(distortion)函数$J\left(c^{(1)},c^{(2)},...,c^{(m)},\mu_1,\mu_2,...,\mu_K\right)=\dfrac{1}{m}\sum\limits_{i=1}^m\Vert x^{(i)}-\mu_{c^{(i)}}\Vert^2$
$\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\color{maroon}[注]\ \ \mu_{c^{(i)}}代表距离x^{(i)}最近的聚类中心$

${\color{maroon}\Longrightarrow}\quad\underset{c^{(1)},...c^{(m)}\\\mu_1,...,\mu_K}{\mbox{min}}J\left(c^{(1)},c^{(2)},...,c^{(m)},\mu_1,\mu_2,...,\mu_K\right)\qquad\quad$

```tex
坐标下降法(Coordinate Descent Method): 每次只更新自变量当中的一个，而其他的保持不变
    · 保持μ不变，调整c使J最小
    · 保持c不变，调整μ使J最小
  ⇒ 要求J随着不断迭代单调递减，并且一定收敛
```

+ 由于$J$是非凸(non-convex)函数，因此使用坐标下降法不一定会收敛到全局最优点，可能收敛到局部最优点（取决于$\mu_j$初始化的值）

  $\quad{\color{maroon}\Longrightarrow}\quad$运行多次K-means，每次选取不同的随机值初始化$\mu_j$，最后选用使$J$最小的模型（在K较大时可能不会有明显的改善）

`例题`:

![k-meansCostCheck](k-meansCostCheck.png)

---

![k-meansQuestion](k-meansQuestion.png)

#### · 随机初始化

1. 选择$K\lt m$，即聚类中心数应小于训练集的大小
2. 随机选择$K$个训练样例，令$\mu_1,\mu_2,...,\mu_K$等于这$K$个样例

```octave
% Randomly reorder the indices of examples
randidx = randperm(size(X, 1));
% Take the first K examples as centroids
centroids = X(randidx(1:K), :);
```

#### · 选择聚类数K

1. 大多数时候靠人工观察决定——“肘部法则”(elbow method)

   大多数情况难以找到明显的“肘部”

   ![ElbowMethod](ElbowMethod.png)

2. 根据运行K-均值的目的确定多少数量的聚类适合

   ​

#### · 向量化

|                    变量                    | 向量表示（$其中m为训练样本数，K为聚类数，n为数据维数$）           |
| :--------------------------------------: | ---------------------------------------- |
|                 feature                  | $X=\begin{pmatrix}x_1^{(1)}&x_2^{(1)}&...& x_{n}^{(1)}\\x_1^{(2)}& x_2^{(2)}&...& x_{n}^{(2)}\\...&...&...&...\\x_1^{(m)}& x_2^{(m)}&...& x_{n}^{(m)}\end{pmatrix}_{m\times{n}}$ |
|                  output                  | $y=\begin{pmatrix} y_1^{(1)}&y_2^{(1)}&...& y_{K}^{(1)}\\y_1^{(2)}&y_2^{(2)}&...& y_{K}^{(2)}\\...&...&...&...\\y_1^{(m)}&y_2^{(m)}&...& y_{K}^{(m)}\end{pmatrix}_{m\times{K}}\quad\color{blueviolet}(y_1^{(i)},y_2^{(i)},...,y_K^{(i)}中仅一项取值为1，其余取值为0)$ |
|                centroids                 | $\mu=\begin{pmatrix}\mu_{11}&\mu_{12}&...&\mu_{1n}\\\mu_{21}&\mu_{22}&...&\mu_{2n}\\...&...&...&...\\\mu_{K1}&\mu_{K2}&...&\mu_{Kn}\end{pmatrix}_{K\times{n}}=\begin{pmatrix}\vec{\mu}_1^T\\\vec{\mu}_2^T\\...\\\vec{\mu}_K^T\end{pmatrix}$ |
| index of the centroid that is closest to X | $c=\begin{pmatrix}c_1^{(1)}&c_2^{(1)}&...&c_n^{(1)}\\c_1^{(2)}&c_2^{(2)}&...&c_n^{(2)}\\...&...&...&...\\c_1^{(m)}&c_2^{(m)}&...&c_n^{(m)}\end{pmatrix}_{m\times{n}}=\begin{pmatrix}{\vec{c}^{(1)}}^T\\{\vec{c}^{(2)}}^T\\...\\{\vec{c}^{(m)}}^T\end{pmatrix}$ |



####· 收敛性



### 3. 降维(Dimensionality Reduction)

##### · 主成分分析(Principal Component Analysis, PCA)

将数据从n维降低到k维，使得n维空间中的数据到k维子平面的投影距离之和最小

实现方法$\left\{\begin{array}{lcl}\text{特征值分解}\\\text{奇异值分解(SVD)}\end{array}\right.$

1. 对输入特征进行均值归一化（使每一维特征的均值为0）和特征缩放

   $\mathbf{X}=\begin{pmatrix}\left(\vec{x}^{(1)}_{n\times{1}}\right)^T\\\left(\vec{x}^{(2)}_{n\times{1}}\right)^T\\...\\\left(\vec{x}^{(m)}_{n\times{1}}\right)^T\end{pmatrix}_{m\times{n}}$

2. 计算[协方差矩阵](#14. 协方差矩阵(covariance matrix))：$\Sigma_{n\times{n}}=\dfrac{1}{m}\sum\limits_{i=1}^m\left( x^{(i)}\right)\left( x^{(i)}\right)^T$

3. 计算矩阵$\Sigma$的特征向量：[U, S, V] =  [svd](#15. 奇异值分解(Singular value decomposition, SVD))$\left(\Sigma_{n\times{n}}\right)$

4. 选取$\mathbf{U}_n$中的前k个列向量（选取大的特征值对应的特征向量）即主成分，得到矩阵$\mathbf{U}_{n\times{k}}$，然后计算降维后的新特征$z^{(i)}_{k\times{1}}=\mathbf{U}_{n\times{k}}^T\times{\vec{x}^{(i)}_{n\times{1}}}$

   $\mathbf{U}_{n\times{k}}=\begin{pmatrix}\vec{u}^{(1)}_{n\times{1}}&\vec{u}^{(2)}_{n\times{1}}&...&\vec{u}^{(k)}_{n\times{1}}\end{pmatrix}_{n\times{k}},\quad\mathbf{Z}=\begin{pmatrix}\left(\vec{z}^{(1)}_{k\times{1}}\right)^T\\\left(\vec{z}^{(2)}_{k\times{1}}\right)^T\\...\\\left(\vec{z}^{(m)}_{k\times{1}}\right)^T\end{pmatrix}_{m\times{k}}$

```octave
# An Implementation of PCA using octave

# feature normalize
mu = mean(X);
X_norm = bsxfun(@minus, X, mu);
sigma = std(X_norm);
X_norm = bsxfun(@rdivide, X_norm, sigma);

# SVD
Sigma = 1.0 / m  * X_norm' * X_norm;
[U, S, V] = svd(Sigma);

# project data from n-D to k-D
U_reduce = U(:, 1:k);
Z = X_norm * U_reduce;

# Recover an approximation of the original data by using the projected data
X_rec = Z * U_reduce';
```

<div STYLE="page-break-after: always;"></div>

##八、生成学习算法(Generative Learning algorithms)

> $\left\{\begin{array}{}\color{Fuchsia}判别学习算法(\text{ discriminative learning algorithms})\\\qquad\bullet\ 由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型\\\qquad\bullet\ 如感知机算法(前者)，逻辑回归(后者)，决策树\\\\\color{Fuchsia}生成学习算法(\text{generative learning algorithms})\\\qquad\bullet\ 由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型:\\\qquad\quad P(Y|X)=\dfrac{P(X,Y)}{P(X)}\xlongequal{贝叶斯定理}\dfrac{P(X|Y)P(Y)}{P(X)}\\\qquad\bullet\ 如朴素贝叶斯，高斯混合模型，\text{HMM}，隐含狄利克雷分布(\text{Latent Dirichlet Allocation, LDA})\end{array}\right.$

### 1. 高斯判别分析(Gaussian Discriminant Analysis, GDA)

> 假设给定数据集$\{\left(x^{(i)},y^{(i)}\right);\ i=1,2,\cdots,m\}$由$m$个独立样本组成，
> 其中$x^{(i)}\in\mathbb{R}^n$是$n$维向量（连续值），$y^{(i)}\in\{0,1\}$，则
> $$
> p(y)=\left\{\begin{array}{}\phi&\text{if }y=1\\1-\phi&\text{if }y=0\end{array}\right.\\\ \\
> p(y|x)=\dfrac{p(x|y)p(y)}{p(x)}=\dfrac{p(x|y)p(y)}{ p(x|y = 1)p(y = 1) + p(x|y = 0)p(y=0)}\\\ \\
> \arg \underset{y}{\max}p(y|x)=\arg \underset{y}{\max}\dfrac{p(x|y)p(y)}{p(x)}=\arg \underset{y}{\max}p(x|y)p(y)
> $$
> 假设$x$服从**多元正态分布/高斯分布**，即$x\sim \mathcal{N}(\mu ,\Sigma)$
>
> $\qquad p(x;\mu,\Sigma) =\dfrac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\dfrac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$
>
> 其中参数$\Sigma\in\mathbb{R}^{n\times{n}}$为$x$的[协方差矩阵](#14. 协方差矩阵(covariance matrix))，$\mu\in\mathbb{R}^n$为均值向量
>
> + 各随机变量间不相关时，协方差矩阵$\Sigma$为对角矩阵，其对角线元素值越大，高斯分布越扁平
>
> ![MultiGaussianDistribution](MultiGaussianDistribution.png)

GDA假定$\ y\sim \text{Bernoulli}(\phi),\ x|y=0\sim \mathcal{N}(\mu_0,\Sigma),\ x|y=1\sim \mathcal{N}(\mu_1,\Sigma)$，即
$$
p(y)=\phi^y(1-\phi)^{1-y}\ \ {\color{maroon}\text{or}}\ \ p(y=1;\phi)=\phi,\ p(y=0;\phi)=1-\phi\\\ \\
p(x|y=0)=\dfrac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\dfrac{1}{2}(x-\mu_{0})^T\Sigma^{-1}(x-\mu_{0})\right)\\\ \\
p(x|y=1)=\dfrac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\dfrac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)
$$

$\begin{equation}\begin{aligned}对数似然函数\ \ell(\phi,\mu_0,\mu_1,\Sigma)&=\log\prod\limits_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\&=\log\prod\limits_{i=1}^mp(x^{(i)}|y^{(i)};\phi,\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)\end{aligned}\end{equation}$

求解$\max\ \ell(\phi,\mu_0,\mu_1,\Sigma)$，得到各参数的极大似然估计：
$$
\phi=\dfrac{1}{m}\sum\limits_{i=1}^m1\{y^{(i)}=1\}\\\ \\
\Sigma=\dfrac{1}{m}\sum\limits_{i=1}^m(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T\\\ \\
\mu_0=\dfrac{\sum\limits_{i=1}^m1\{y^{(i)}=0\}x^{(i)}}{\sum\limits_{i=1}^m1\{y^{(i)}=0\}},\ \ \mu_1=\dfrac{\sum\limits_{i=1}^m1\{y^{(i)}=1\}x^{(i)}}{\sum\limits_{i=1}^m1\{y^{(i)}=1\}}
$$
从而确定$p(y),p(x|y=0),p(x|y=1)$，然后代入贝叶斯公式，求得$p(y|x)$

![GDA](GDA.png)蓝色分割线对应$p(y=1|x)=0.5$

+ GDA与逻辑回归

  $p(y=1|x;\phi,\Sigma,\mu_0,\mu_1)=\dfrac{p(x|y=1)p(y=1)}{p(x|y = 1)p(y = 1) + p(x|y = 0)p(y=0)}\\\\=\frac{\dfrac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\dfrac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\cdot{\phi}}{\dfrac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\dfrac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\cdot{\phi}+\dfrac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\dfrac{1}{2}(x-\mu_{0})^T\Sigma^{-1}(x-\mu_{0})\right)\cdot{(1-\phi)}}\\\\=\dfrac{1}{1+\exp\left(-\theta^Tx\right)}$

  同理$\ p(y=0|x;\phi,\Sigma,\mu_0,\mu_1)$可表示为上式类似形式，即与逻辑回归的判别模型相同

  故当$\ p(x|y)$是多元高斯分布时，$\ p(y|x)$是logistic函数，但反之不成立，如泊松分布的生成模型也会演变为 logistic函数的判别模型

  这表明GDA相比逻辑回归做了更强的模型假设，因而当模型假设正确时，GDA将对数据拟合得更好，得到更佳的模型；而逻辑回归做了较弱的模型假设，因而鲁棒性更好，对于不正确的模型假设的敏感性较低

<div STYLE="page-break-after: always;"></div>

### 2. 朴素贝叶斯(Naïve Bayes)方法

[数学之美番外篇：平凡而又神奇的贝叶斯方法](http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/)

> 基于有限样本直接估计联合概率，在计算上将会遭遇**组合爆炸**问题，在数据上将会遭遇**样本稀疏**问题
>
> 特征向量$x$维度越高，问题越严重
>
> $\color{Bittersweet}\Downarrow为避开这个障碍$

“朴素” —— 假设特征向量的各个维度/属性间相互独立$\quad$`条件独立性假设`

> #### 贝叶斯公式
>
> $P(y|x) = \dfrac{P(x|y)  P(y)}{P(x)}$
>
> #### [全概率公式](#全概率公式)
>

设特征向量$x\in\mathbb{R}^n$的各随机变量为**离散值**，则

$\begin{equation}\begin{aligned}P(x_1,x_2,\cdots,x_n|c)&=P(x_1|c)P(x_2|x_1,c)\cdots P(x_n|x_1,x_2,\cdots,x_{n-1},c)\\&\xlongequal{\color{maroon}条件独立性假设}P(x_1|c)P(x_2|c)\cdots P(x_n|c)=\prod\limits_{i=1}^nP(x_i|c)\end{aligned}\end{equation}$

朴素贝叶斯分类器的判定准则：$h_{nb}(x)=\arg\underset{c\in\mathcal{Y}}{\max}P(c)\prod\limits_{i=1}^nP(x_i|c)$

> 朴素贝叶斯分类器的训练过程就是基于训练集$\mathbf{X}$来估计类先验概率(class prior)$\ P(c)$，并为每个属性估计条件概率$P(x_i|c)$

若有**充足的**独立同分布样本，则可得到估计量
$$
类先验概率P(c)=\dfrac{|\mathbf{X}_c|}{|\mathbf{X}|}\\\ \\
第i个(离散)属性的条件概率P(x_i|c)=\dfrac{|\mathbf{X}_{c,x_i}|}{|\mathbf{X}_c|}
$$
其中$\mathbf{X}_c$表示训练集$\mathbf{X}$中第$c$类样本组成的集合，$\mathbf{X}_{c,x_i}$表示$\mathbf{X}_c$中在第$i$个属性上取值为$x_i$的样本组成的集合

对于连续属性可考虑概率密度函数，假定$p(x_i|c)\sim\mathcal{N}(\mu_{c,i},\sigma_{c,i}^2)$，其中$\mu_{c,i}$和$\sigma_{c,i}^2$分别是第$c$类样本在第$i$个属性上取值的均值和方差，则有$p(x_i|c)=\dfrac{1}{\sqrt{2\pi}\sigma_{c,i}}\exp\left(-\dfrac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2}\right)$

+ 为避免其他属性携带的信息被训练集中未出现的属性值“抹去”【$P(c)\prod\limits_{i=1}^nP(x_i|c)$连乘式中任一项为0导致结果为0】，在估计概率值时通常要进行“平滑”(smoothing)，常用“拉普拉斯修正”(Laplacian correction)：
  $$
  \hat{P}(c)=\dfrac{|\mathbf{X}_c|+1}{|\mathbf{X}|+N}\\\ \\
  \hat{P}(x_i|c)=\dfrac{|\mathbf{X}_{c,x_i}|+1}{|\mathbf{X}_c|+N_i}
  $$
  其中$|\mathbf{X}|$表示训练集$\mathbf{X}$的大小，$N$表示训练集$\mathbf{X}$中可能的类别属性数，$N_i$表示第$i$个属性可能的取值数 

  > 拉普拉斯修正实际上假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入的关于数据的先验(prior)
  >
  > 随着训练集变大，修正过程引入的先验的影响也会逐渐变得可忽略，使得估值逐渐趋向实际概率值



###3. EM(Expectation-Maximization)算法

`最常见的隐变量估计方法`



### 4. 隐含狄利克雷分布(Latent Dirichlet allocation, LDA)

`主题模型：用来在一系列文档中发现抽象主题的一种统计模型`

<div STYLE="page-break-after: always;"></div>

## 九、决策树(Decision Tree)



<div STYLE="page-break-after: always;"></div>

##十、卷积神经网络(Convolutional Neural Network, CNN)

典型的深度学习模型就是很深层的神经网络。对于神经网络模型，提高容量的一个简单方法就是增加隐层数目。模型复杂度也可通过单纯增加隐层神经元数目来实现，但增加隐层数更有效。然而，多隐层（3个以上）神经网络难以直接用经典算法（如标准BP算法）进行训练，因为误差在多隐层内逆传播时，往往会“发散”(diverge)而不能收敛到稳定状态

节省训练开销的策略$\left\{\begin{array}{lcl}\text{无监督逐层训练}\\\text{权共享(weight sharing)}\end{array}\right.$

<div STYLE="page-break-after: always;"></div>

##十一、降维方法

###1. 线性判别分析(Linear Discriminant Analysis, LDA)

> 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；
>
> 在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别



###2. 主成分分析(Principal Component Analysis, PCA)

参见[此处](#· 主成分分析(Principal Component Analysis, PCA))

<div STYLE="page-break-after: always;"></div>

#附录

##一、经典参数估计(Parameter estimation)方法

### 1. 矩估计法(Moment method, MM)

用样本矩估计总体矩（如用样本均值估计总体均值），进而找出未知参数的估计

> 最简单的矩估计法是用一阶样本原点矩估计总体期望，而用二阶样本中心矩估计总体方差

只涉及总体的一些数字特征，并未用到总体的分布，因此矩法估计量实际上只集中了总体的部分信息，这样它在体现总体分布特征上往往性质较差，只有在**样本容量n较大**时，才能保障它的优良性

> ####辛钦大数定律
>
> 设$X_1,X_2,\cdots$是独立同分布的随机变量序列，且它们的期望值存在，记为$\text{E}(X_i)=μ\ \ (i=1,2,⋯)$，则对于任意的$\epsilon>0$，有$\lim\limits_{n\to\infty}P\left\{\left|\dfrac{1}{n}\sum\limits_{i=1}^nX_i-\dfrac{1}{n}\sum\limits_{i=1}^n\text{E}(X_i)\right|\le\epsilon\right\}=1$
>
> 即**用算术平均值来估计数学期望是合理的**
>
> + 辛钦大数定律并不要求随机变量序列$X_1,X_2,\cdots$的方差存在

### 2. 普通最小二乘估计(Ordinary least squares, OLS)

线性统计模型中的参数估计

> 估计量的数学期望等于被估计参数的真实值，则称此此估计量为被估计参数的**无偏估计**

![普通最小二乘估计](普通最小二乘估计.png)

###3. 极大似然估计(Maximum likelihood estimation, MLE)

先假定具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计

概率模型的训练过程就是参数估计过程

`估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布`

+ **似然函数**

  给定联合样本值$\textbf{x}$下关于(未知)参数$\theta$的函数：$L(\theta | \textbf{x}) = f(\textbf{x} | \theta)$

  $\quad\textbf{x}$是指联合样本随机变量$\textbf{X}$取到的值，即$\textbf{X} = \textbf{x}$

  $\quad\theta$是指未知参数，它属于参数空间

  $\quad f(\textbf{x}|\theta)$是一个密度函数，特别地，它表示(给定)$\theta$下关于联合样本值$\textbf{x}$的联合密度函数

  > 如果$\textbf{X}$是离散的随机向量，那么其概率密度函数$ f(\textbf{x} | \theta)$可改写为$ f(\textbf{x} | \theta) = \mathbb{P}_\theta(\textbf{X} = \textbf{x})$，即代表了在参数$\theta$下随机向量$\textbf{X}$取到值$\textbf{x}$的可能性
  >
  > 如果我们发现$L(\theta_1 | \textbf{x} ) = \mathbb{P}_{\theta_1}(\textbf{X} = \textbf{x}) > \mathbb{P}_{\theta_2}(\textbf{X} = \textbf{x}) = L(\theta_2 | \textbf{x})$，
  > 那么似然函数就反应出这样一个朴素推测：
  > 在参数$\theta_1$下随机向量$\textbf{X}$取到值$\textbf{x}$的可能性**大于**在参数$\theta_2$下随机向量$\textbf{X}$取到值$\textbf{x}$的可能性
  > 换句话说，我们更有理由相信（相对于$\theta_2$来说）$\theta_1$更有可能是真实值

综上，概率(密度)表达给定$\theta$下样本随机向量$\textbf{X} = \textbf{x}$的可能性，而似然表达了给定样本$\textbf{X} = \textbf{x}$下参数$\theta_1$(相对于另外的参数$\theta_2)$为真实值的可能性

> 极大似然估计的计算过程非常简单：
>
> 1. 由总体分布导出样本的联合概率函数（或联合密度）
>
> 2. 把样本的联合概率函数（或联合密度）中自变量看作已知常数，而把参数$\theta$看作自变量，得到似然函数
>
> 3. 求出使得似然函数取最大值时参数的值$\hat{\theta}$，这个值就是我们对概率模型中参数值的极大似然估计
>
>    若似然函数关于$\theta$可微，则令$\dfrac{d}{d \theta}L(\theta)=0$，因为$L(θ)$与$ lnL(θ) $在同一$\theta$处取得极值

> `频率学派(Frequentist)`
>
> + 试图直接为「事件」本身建模，即事件A在独立重复试验中发生的频率趋于极限p，那么这个极限就是该事件的概率
> + <u>把需要推断的参数θ视作固定且未知的常数</u>，而样本X是随机的，其着眼点在样本空间，有关的概率计算都是针对X的分布

### 4. 最大后验(Maximum A Posteriori, MAP)估计

对极大似然估计$\arg\underset{\theta}{\max}p(\textbf{X}|\theta)$的参数$\theta$考虑其先验概率$p(\theta)$，目标变为求使后验概率最大的参数估计值$\hat{\theta}$
$\begin{equation}\begin{aligned}\arg\underset{\theta}{\max}\dfrac{p(\textbf{X}|\theta)p(\theta)}{p(\textbf{X})}&\xlongequal{\color{maroon}p(\textbf{X})与参数\theta无关}\arg\underset{\theta}{\max}p(\textbf{X}|\theta)p(\theta)\\&=\arg\underset{\theta}{\max}\{L(\theta|\textbf{X})+\log p(\theta)\}\\&=\arg\underset{\theta}{\max}\left\{\sum\limits_{x\in\textbf{X}}\log p(x|\theta)+\log p(\theta)\right\}\end{aligned}\end{equation}$

###5. 贝叶斯估计法(Bayesian estimation)

贝叶斯估计是在MAP上做进一步拓展，此时不直接估计参数的值，而是允许参数服从一定概率分布

据参数的先验分布$P(\theta)$和一系列观察$\textbf{X}$，求出参数$\theta$的后验分布$P(\theta|\textbf{X})$，然后求出$\theta$的期望值，作为其最终值

> #### 全概率公式
>
> 若事件A1，A2，…，An构成一个完备事件组且都有正概率，则对任意一个事件B都有公式
>
> $\qquad P(B)=\sum\limits_{i=1}^n P(A_i)P(B|A_i)$
>
> 成立
>
> + 完备事件组
>
>   事件之间两两互斥，所有事件的并集是整个样本空间（必然事件）

现在不是要求后验概率最大，这样就需要求$P(\textbf{X})$，即观察到的证据因子的概率，由全概率公式即可求得

当新的数据被观察到时，后验概率可以自动随之调整

> `贝叶斯学派(Bayesian)`
>
> - 并不从试图刻画「事件」本身，而从「观察者」角度出发，试图描述的是观察者知识状态在新的观测发生后如何更新
>
> - 把概率解释成是对事件发生的信心（相信程度），把对一个事件*A*发生的信念记为$P(A)$，称为**先验概率**
>
> - 用$P(A|X)$表示更新后的信念，其含义为在得到证据*X*后*A*事件的概率，称为**后验概率**
>
>   $后验概率P(A|X)=\dfrac{似然函数P(X|A)\times{先验概率}P(A)}{证据因子常数P(X)}$
>
>   $\text{posterior}=\dfrac{\text{likelihood}\times\text{prior}}{\text{evidence}}$
>
> - <u>把参数θ视作随机变量</u>，而样本X是固定的，其着眼点在参数空间，重视参数θ的分布，固定的操作模式是通过参数的先验分布结合样本信息得到参数的后验分布
>
> - 认为参数空间里的每个值都有可能是真实模型使用的值，区别只是概率不同而已

<div STYLE="page-break-after: always;"></div>

## 二、泛函分析(functional analysis)

###1. 泛函 

+ 线性代数中的定义

  从一个向量空间V到它的标量域(即对偶空间V*)的**线性映射**

  + (代数)对偶空间 ((algebraic) dual space)

    任何一个向量空间V都存在对应的对偶向量空间，由V的所有线性泛函组成

    对偶空间是线性空间，即对加法和数乘封闭(任意一个元素加上/乘以任意一个元素，结果仍属于该空间)

+ 数学分析中的定义

  从一个向量空间V到实数/复数域的映射

+ 通常向量空间是函数的空间，这样泛函将函数作为其输入参数，可看作*函数的函数*