![封面](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\封面.png)

<div STYLE="page-break-after: always;"></div>

#机器学习笔记

[TOC]

<div STYLE="page-break-after: always;"></div>

##一、线性代数相关

###1.矩阵的求导 

|               表达形式               |             公式$\quad$`分子布局`              |
| :------------------------------: | :--------------------------------------: |
| $\dfrac{\partial向量}{\partial标量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\qquad\dfrac{\partial \vec{y}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x}\\\dfrac{\partial y_2}{\partial x}\\...\\\dfrac{\partial y_m}{\partial x} \end{pmatrix}$ |
| $\dfrac{\partial标量}{\partial向量}$ | $\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\qquad\dfrac{\partial y}{\partial \vec{x}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_1}&\dfrac{\partial y}{\partial x_2}&...&\dfrac{\partial y}{\partial x_n} \end{pmatrix}$ |
|         标量函数关于空间向量的方向导数          | $\nabla_{\vec{u}}f(x)\mathop{=}\limits^{\triangle}\nabla f(x)\cdot{\vec{u}}=\dfrac{\partial f(x)}{\partial x}\cdot{\vec{u}}$ |
| $\dfrac{\partial向量}{\partial向量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\quad\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\\\dfrac{\partial \vec{y}}{\partial \vec{x}}=\begin{pmatrix}\dfrac{\partial y_1}{\partial \vec{x}}\\\dfrac{\partial y_2}{\partial \vec{x}}\\...\\\dfrac{\partial y_m}{\partial \vec{x}}\end{pmatrix}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x_1}&\dfrac{\partial y_1}{\partial x_2}&...&\dfrac{\partial y_1}{\partial x_n}\\\dfrac{\partial y_2}{\partial x_1}&\dfrac{\partial y_2}{\partial x_2}&...&\dfrac{\partial y_2}{\partial x_n}\\...&...&...&...\\\dfrac{\partial y_m}{\partial x_1}&\dfrac{\partial y_m}{\partial x_2}&...&\dfrac{\partial y_m}{\partial x_n} \end{pmatrix}_{m\times{n}}$ |
| $\dfrac{\partial矩阵}{\partial标量}$ | $\dfrac{\partial \bf{Y}_{m\times{n}}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_{11}}{\partial x}&\dfrac{\partial y_{12}}{\partial x}&...&\dfrac{\partial y_{1n}}{\partial x}\\\dfrac{\partial y_{21}}{\partial x}&\dfrac{\partial y_{22}}{\partial x}&...&\dfrac{\partial y_{2n}}{\partial x}\\...&...&...&...\\\dfrac{\partial y_{m1}}{\partial x}&\dfrac{\partial y_{m2}}{\partial x}&...&\dfrac{\partial y_{mn}}{\partial x} \end{pmatrix}_{m\times{n}}$ |

<div STYLE="page-break-after: always;"></div>

|               表达形式               |             公式$\quad$`分子布局`              |
| :------------------------------: | :--------------------------------------: |
| $\dfrac{\partial标量}{\partial矩阵}$ | $\dfrac{\partial y}{\partial \bf{X}_{p\times{q}}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_{11}}&\dfrac{\partial y}{\partial x_{21}}&...&\dfrac{\partial y}{\partial x_{p1}}\\\dfrac{\partial y}{\partial x_{12}}&\dfrac{\partial y}{\partial x_{22}}&...&\dfrac{\partial y}{\partial x_{p2}}\\...&...&...&...\\\dfrac{\partial y}{\partial x_{1q}}&\dfrac{\partial y}{\partial x_{2q}}&...&\dfrac{\partial y}{\partial x_{pq}} \end{pmatrix}_{q\times{p}}\\\quad\\通常写作\nabla_{\bf{X}}y(\bf{X})=\dfrac{\partial y(\bf{X})}{\partial \bf{X}}$ |
|          标量函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}f=tr\pmatrix{\dfrac{\partial f}{\partial \bf{X}}\bf{Y}}$ |
| $\dfrac{\partial矩阵}{\partial矩阵}$ | $\dfrac{\partial \bf{F}_{p\times{q}}}{\partial \bf{X}_{n\times{m}}}=\begin{pmatrix} \dfrac{\partial \bf{F}}{\partial \bf{X_{1,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,n}}}\\\dfrac{\partial \bf{F}}{\partial \bf{X_{2,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,n}}}\\...&...&...&...\\\dfrac{\partial \bf{F}}{\partial \bf{X_{m,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,n}}} \end{pmatrix}_{mp\times{nq}}\\\quad\\其中每一项\dfrac{\partial \bf{F}}{\partial \bf{X_{i,j}}}都是p\times{q}的矩阵$ |
|          矩阵函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}\bf{F}=tr\pmatrix{\dfrac{\partial \bf{F}}{\partial \bf{X}}\bf{Y}}$ |

#### · 通用公式

$\quad(\bf{X}、\bf{Y}均为矩阵)$

> $\dfrac{\partial \bf{X}}{\partial \bf{X}}={\bf{I}}$
>
> $\dfrac{\partial a{\bf{Y}}}{\partial \bf{X}}=a\dfrac{\partial {\bf{Y}}}{\partial \bf{X}}\quad(a不是\bf{X}的函数)$
>
> $\dfrac{\partial f(g(u))}{\partial{\bf{X}}}=\dfrac{\partial f(g)}{\partial g}\dfrac{\partial g(u)}{\partial u}\dfrac{\partial u}{\partial{\bf{X}}}\quad(f,g,u均为标量函数)$
>
> $\dfrac{\partial^2 \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}^2}={\bf{A}}+{\bf{A}}^T\quad({\bf{A}}不是\vec{x}的函数，\vec{x}^T{\bf{A}}\vec{x}为标量)$
>
> $\dfrac{\partial\vec{u}^T}{\partial x}=\pmatrix{\dfrac{\partial \vec{u}}{\partial x}}^T$

`分子布局`

$\qquad$根据$\bf Y$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad$根据${\bf X}^T$来布局$\dfrac{\partial y}{\partial\bf{X}}$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为列向量，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为行向量

`分母布局`

$\qquad$根据${\bf Y}^T$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad​$根据$\bf{X}​$来布局$\dfrac{\partial y}{\partial\bf{X}}​$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为列向量，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为行向量

|          分子布局(Numerator Layout)          |         分母布局(Denominator Layout)         |
| :--------------------------------------: | :--------------------------------------: |
| $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ m\times{n}$ | $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ n\times{m}$ |
| $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\vec{u}\dfrac{\partial a}{\partial\vec{x}}$ | $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\dfrac{\partial a}{\partial\vec{x}}\vec{u}^T$ |
| $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}={\bf{A}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}({\bf{A}}_{p\times{q}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}{\bf{A}}^T$<br>$\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\vec{u}^T\dfrac{\partial \vec{v}}{\partial \vec{x}}+\vec{v}^T\dfrac{\partial \vec{u}}{\partial \vec{x}}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ | $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\dfrac{\partial \vec{v}}{\partial \vec{x}}\vec{u}+\dfrac{\partial \vec{u}}{\partial \vec{x}}\vec{v}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ |
| $\dfrac{\partial\vec{u}^T{\bf{A}}{\vec{v}}}{\partial \vec{x}}=\vec{u}^T{\bf{A}}\dfrac{\partial \vec{v}}{\partial \vec{x}}+\vec{v}^T{\bf{A}}^T\dfrac{\partial \vec{u}}{\partial \vec{x}}$<br>$\color{blueviolet}(\vec{u}_{m\times{1}}是\vec{x}的函数，\vec{v}_{n\times{1}}是\vec{x}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}的函数)$ | $\dfrac{\partial\vec{u}^T{\bf{A}}{\vec{v}}}{\partial \vec{x}}=\dfrac{\partial \vec{v}}{\partial \vec{x}}{\bf{A}}^T\vec{u}+\dfrac{\partial \vec{u}}{\partial \vec{x}}{\bf{A}}\vec{v}$<br>$\color{blueviolet}(\vec{u}^T\vec{v}是标量，{\bf{A}}不是\vec{x}的函数)$ |
| $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=\vec{x}^T({\bf{A}}+{\bf{A}}^T)\quad\color{blueviolet}({\bf{A}}_{n\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=({\bf{A}}+{\bf{A}}^T)\vec{x}\quad\color{blueviolet}({\bf{A}}_{n\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |

<div STYLE="page-break-after: always;"></div>

|          分子布局(Numerator Layout)          |         分母布局(Denominator Layout)         |
| :--------------------------------------: | :--------------------------------------: |
| $\dfrac{\partial \vec{b}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=\vec{b}^T{\bf{A}}$<br>$\color{blueviolet}(\vec{b}_{m\times{1}}不是\vec{x}_{n\times{1}}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{b}^T{\bf{A}}\vec{x}}{\partial \vec{x}}={\bf{A}}^T\vec{b}$<br>$\color{blueviolet}(\vec{b}_{m\times{1}}不是\vec{x}_{n\times{1}}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial \vec{x}^T\vec{x}}{\partial \vec{x}}=2\vec{x}^T$ | $\dfrac{\partial \vec{x}^T\vec{x}}{\partial \vec{x}}=2\vec{x}$ |
| $\dfrac{\partial \vec{a}^T\vec{x}\vec{x}^T\vec{b}}{\partial \vec{x}}=\vec{x}^T(\vec{a}\vec{b}^T+\vec{b}\vec{a}^T)$<br>$\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数，\vec{b}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{a}^T\vec{x}\vec{x}^T\vec{b}}{\partial \vec{x}}=(\vec{a}\vec{b}^T+\vec{b}\vec{a}^T)\vec{x}$<br>$\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数，\vec{b}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\ \Vert\vec{x}-\vec{a}\Vert}{\partial \vec{x}}=\dfrac{(\vec{x}-\vec{a})^T}{\Vert\vec{x}-\vec{a}\Vert}\quad\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\ \Vert\vec{x}-\vec{a}\Vert}{\partial \vec{x}}=\dfrac{\vec{x}-\vec{a}}{\Vert\vec{x}-\vec{a}\Vert}\quad\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ |

对${\bf{f}}({\bf{X}}):\mathbb{R}^{m\times{n}}\rightarrow\mathbb{R}^{p\times{q}}$求导**不存在**与标量函数完全等同的乘积法则$(f\cdot{g})'=f'\cdot{g}+f\cdot{g'}$和链式法则$\dfrac{dz}{dx}=\dfrac{dz}{dy}\cdot{\dfrac{dy}{dx}}$

> *$\ $维度相容原则$\quad$`Trick`
>
> 通过**前后换序**、**转置** 使求导结果满足矩阵乘法且结果维数满足下式，那么结果就是正确的：
>
> $\qquad如果\vec{x}\in \mathbb{R}^{m\times{n}},f(\vec{x})\in \mathbb{R}^1，那么\dfrac{\partial f(\vec{x})}{\partial\vec{x}}\in \mathbb{R}^{m\times{n}}$

###2. 雅各比矩阵(Jacobian Matrix)

> $\vec{f}(\vec{x}):\mathbb{R}^n\rightarrow\mathbb{R}^m是向量，那么其雅各比矩阵{\bf{J}}是一个m\times{n}的矩阵，定义如下:$

$\quad{\bf{J}}=\begin{pmatrix}\dfrac{\partial\bf{f}}{\partial x_1}&\dfrac{\partial\bf{f}}{\partial x_2}&...&\dfrac{\partial\bf{f}}{\partial x_n}\end{pmatrix}=\begin{pmatrix}\dfrac{\partial f_1}{\partial x_1}&\dfrac{\partial f_1}{\partial x_2}&...&\dfrac{\partial f_1}{\partial x_n}\\\dfrac{\partial f_2}{\partial x_1}&\dfrac{\partial f_2}{\partial x_2}&...&\dfrac{\partial f_2}{\partial x_n}\\...&...&...&...\\\dfrac{\partial f_m}{\partial x_1}&\dfrac{\partial f_m}{\partial x_2}&...&\dfrac{\partial f_m}{\partial x_n}\end{pmatrix}$

$\quad{\bf{J}}_{i,j}=\dfrac{\partial f_i}{\partial x_j}$

> 如果$\vec{f}$在$\vec{x}$点处可微，则雅各比矩阵${\bf{J}}$定义了一个线性映射$\mathbb{R}^n\rightarrow\mathbb{R}^m$，这是对函数$\vec{f}$在$\vec{x}$点处最优（逐点的）线性估计，被称作$\vec{f}$在$\vec{x}$点处的导数或微分

如果$m=n$，那么雅各比矩阵${\bf{J}}$为一个方阵，其行列式称为**雅各比行列式 (Jacobian Determinant)**

+ 当且仅当雅各比行列式在$\vec{x}$点处不等于零时，函数$\vec{f}$在$\vec{x}$点附近有反函数 (inverse function)`反函数定理(Inverse function theorem)`
+ 若雅各比行列式的值为正，则$\vec{f}$在$\vec{x}$点附近的取向不变；若为负，则取向相反
+ 若雅各比行列式恒等于零，则函数组$\pmatrix{f_1&f_2&...&f_m}$是函数相关的，其中至少有一个函数是其余函数的一个连续可微的函数

###3. 海森矩阵(Hessian Matrix)

> $f(\vec{x}):\mathbb{R}^n\rightarrow\mathbb{R}是标量，如果f的所有二阶导数存在且在函数定义域内连续，则f的海森矩阵{\bf{H}}是一个n\times{n}的方阵，定义如下:$

$\quad{\bf{H}}=\begin{pmatrix}\dfrac{\partial^2f}{\partial x_1^2}&\dfrac{\partial^2f}{\partial x_1\partial x_2}&...&\dfrac{\partial^2f}{\partial x_1\partial x_n}\\\dfrac{\partial^2f}{\partial x_2\partial x_1}&\dfrac{\partial^2f}{\partial x_2^2}&...&\dfrac{\partial^2f}{\partial x_2\partial x_n}\\...&...&...&...\\\dfrac{\partial^2f}{\partial x_n\partial x_1}&\dfrac{\partial^2f}{\partial x_n\partial x_2}&...&\dfrac{\partial^2f}{\partial x_n^2}\end{pmatrix}$

$\quad{\bf{H}}_{i,j}=\dfrac{\partial^2f}{\partial x_i\partial x_j}$

> 高维情况下的牛顿法迭代公式：
>
> $\qquad x_{n+1}=x_n-[{\bf{H}}f(\vec{x})]^{-1}\nabla f(x_n),n\ge{0}$

<div STYLE="page-break-after: always;"></div>

##二、线性回归(Linear Regression)

###1. 模型表示

​	假设函数

​		$ h_θ(x)=θ_0+θ_1x\qquad\qquad\qquad\qquad\qquad$`单个特征`

​		$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\quad\ $`多个特征`


![线性回归](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\线性回归.jpg)

###2. 代价函数(Cost function)

   $J(\theta_0,\theta_1)=\dfrac{1}{2m}\sum_{i=1}\limits^n(h_\theta(x^{(i)})^2-y^{(i)})^2\quad$`最小二乘代价函数`

   + [为什么线性回归使用最小二乘代价函数$J(\theta_0,\theta_1)?$](http://blog.csdn.net/Eric2016_Lv/article/details/52836182?locationNum=3&fps=1)

###3. 最小二乘法(LSE)

   $\hat{y}=\hat{a}+\hat{b}x$

   $\hat{b}=\dfrac{\sum_{i=1}\limits^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}\limits^n(x_i-\bar{x})^2}=\dfrac{\sum_{i=1}\limits^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}\limits^nx_i^2-n\bar{x}^2}$

   $\hat{a}=\bar{y}-\hat{b}\bar{x}$

   $其中\bar{x}、\bar{y}为x_i、y_i的均值$

   + [ 求回归直线方程的推导过程](http://blog.csdn.net/marsjohn/article/details/54911788)
   + [最小二乘法？为神马不是差的绝对值](http://blog.sciencenet.cn/blog-430956-621997.html)
   + `求解全局最优`
   + `数据量很大时，计算量大`

###4. 梯度下降(Gradient Descent)

   + 目标：$\mathop{minimize}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)$


   + `迭代法`

     + $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\quad$`α为学习率`

       $\mathop{\Longrightarrow}\limits^{线性回归}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_1:=\theta_1-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x^{(i)}}\end{array}\right.\quad$`同时更新θ₀和θ₁`

   + `每一步更新未知量，逐渐逼近局部最优解`

        ![梯度下降1](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\梯度下降1.png)

        ![梯度下降2](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\梯度下降2.png)

   + 如何确保梯度下降能正常工作

         + J(θ)应随着迭代不断减小
         + 使用较小的学习率*α*

   + 如何选取学习率*α*

         + 尝试：

              ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...

        + 取值范围

             $0<\alpha<2(|X^TX|^{-1})$

   + 分类

        + Batch Gradient Descent

          $\qquad$每一步都使用**整个**训练集的数据计算梯度

        + Stochastic Gradient Descent

          $\Downarrow\quad\ $每一步仅用训练集中的**单个**计算梯度

        + Mini-batch Gradient Descent

          $\qquad$每一步仅用训练集中的**一小部分**（32/64/128/256）计算梯度

          > The way to go is therefore to use some acceptable (but not necessarily optimal) values for the other hyper-parameters, and then trial a number of different mini-batch sizes, scaling α as above. Plot the validation accuracy versus time (as in, real elapsed time, not epoch!), and choose whichever mini-batch size gives you the most rapid improvement in performance. With the mini-batch size chosen you can then proceed to optimize the other hyper-parameters.

          + 适用于存在较多局部最优解的情况，能跳出局部最优解

          + 计算量更小，计算速度更快

          + [BGD与SGD的不同](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent)

          > The applicability of batch or stochastic gradient descent really depends on the error manifold expected.
          >
          > Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.
          >
          > Stochastic gradient descent (SGD) computes the gradient using a single sample. Most applications of SGD actually use a minibatch of several samples, for reasons that will be explained a bit later. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. Single samples are really noisy, while minibatches tend to average a little of the noise out. Thus, the amount of jerk is reduced when using minibatches. A good balance is struck when the minibatch size is small enough to avoid some of the poor local minima, but large enough that it doesn't avoid the global minima or better-performing local minima. (Incidently, this assumes that the best minima have a larger and deeper basin of attraction, and are therefore easier to fall into.)
          >
          > One benefit of SGD is that it's computationally a whole lot faster. Large datasets often can't be held in RAM, which makes vectorization much less efficient. Rather, each sample or batch of samples must be loaded, worked with, the results stored, and so on. Minibatch SGD, on the other hand, is usually intentionally made small enough to be computationally tractable.
          >
          > Usually, this computational advantage is leveraged by performing many more iterations of SGD, making many more steps than conventional batch gradient descent. This usually results in a model that is very close to that which would be found via batch gradient descent, or better.
          >
          > The way I like to think of how SGD works is to imagine that I have one point that represents my input distribution. My model is attempting to learn that input distribution. Surrounding the input distribution is a shaded area that represents the input distributions of all of the possible minibatches I could sample. It's usually a fair assumption that the minibatch input distributions are close in proximity to the true input distribution. Batch gradient descent, at all steps, takes the steepest route to reach the true input distribution. SGD, on the other hand, chooses a random point within the shaded area, and takes the steepest route towards this point. At each iteration, though, it chooses a new point. The average of all of these steps will approximate the true input distribution, usually quite well.

###5. 向量化
|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | ---------------------------------------- |
|     feature      | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|      output      | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}$ |
|        θ         | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
|    hypothesis    | $h=X\theta=\begin{pmatrix}  \sum\limits_{i=0}^n\theta_ix_i^{(1)}\\ \sum\limits_{i=0}^n\theta_ix_i^{(2)}\\...\\ \sum\limits_{i=0}^n\theta_ix_i^{(m)} \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |
|  cost function   | $J=\dfrac{1}{2m}(h-y)^T(h-y)$            |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |
###6. 特征缩放(Feature Scaling) 

+ 确保所有特征在相似的规模上（$-1\leq{x_i}\leq{1}$）

+ 提升梯度下降速度

  ![特征缩放](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\特征缩放.png)

+ 方法

  + **mean normalization**

    $x_i'=\dfrac{x_i-\mu_i}{max(x)-min(x)}$

    $其中\mu_i为x的均值$

  + **Standardization**

    $x_i'=\dfrac{x-\bar{x}}{\sigma}$

    $其中\sigma为x的标准差$
###6. 正规方程(Normal Equation) 

+ 公式

  $\theta=(X^TX)^{-1}X^Ty$

  推导过程

+ 直接求解参数*θ*的最优值，不需要多次迭代 $\quad$`解析方法`

+ 不需要特征缩放

+ 不需要选择学习率*α*



> `问题`
>
> A.  特征数很大时计算量大（n≥10000) $\quad$`求解逆矩阵的时间复杂度O(n³)`
>
> B.  只适用于线性模型
>
> C.  $X^TX$可能不可逆
>
> + 冗余特征：存在线性相关的特征
>
> + 太多特征：特征数>样本数，导致过拟合
>
>   $\mathop{\Longrightarrow}\limits^{处理方法}\left\{\begin{array}{lcl}删除部分特征\left\{\begin{array}{lcl}手动选择\\模型选择算法\end{array}\right.\\正则化(\mbox{Regularization})\end{array}\right.$

### 7. 正则化(Normalization)

`针对过拟合提出`

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=\dfrac{1}{2m}[\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2{\color{salmon}+\lambda\sum\limits_{j=1}^n\theta_j^2}]$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

  $\qquad\qquad\qquad\qquad\qquad\qquad\Downarrow$

  $\qquad\qquad\theta_j:=\theta_j{\color{salmon}(1-\alpha\dfrac{\lambda}{m})}-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}$$\qquad\qquad\qquad\qquad\qquad\scriptsize\color{maroon}\boxed{[注]\ 1-\alpha\dfrac{\lambda}{m}<1}$

  ​

  `正规方程`

  $\qquad\qquad\theta=\Bigg[X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}_{(n+1)\times{(n+1)}}\Bigg]^{-1}X^Ty$

  ​

  $\qquad\qquad\color{maroon}[注]\ 当\lambda>0时，可以证明X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}是可逆的$

  > 建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍（增减10倍是粗调节，当确定了λ的合适的数量级后，比如λ = 0.01,再进一步地细调节，比如调节为0.02，0.03，0.009之类。）

### 8. 牛顿法

+ ​

### 9. 最速下降法

+ ​

###10. 局部加权线性回归    `非参数学习方法` 

​	Ⅰ $\quad\ $Fit *θ* to minimize $\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$

​	Ⅱ $\quad$Output $\theta^Tx$

$w^{(i)}=e^{-\dfrac{(x^{(i)}-x)^2}{2\tau^2}}$，其中τ为常数，可调节$w^{(i)}$钟形曲线的宽度

+ 当$|x^{(i)}-x|\approx{0}$时，$w^{(i)}\approx{1}$，即预测样本举例训练样本越近，权值越大
+ 当$|x^{(i)}-x|\approx{+\infty}​$时，$w^{(i)}\approx{0}​$，即预测样本举例训练样本越远，权值越小

> `问题`
>
> A.  当数据规模比较大时，计算量很大，学习效率很低
>
> B.  不一定能避免欠拟合

<div STYLE="page-break-after: always;"></div>


##三、逻辑回归(Logistic Regression)

###1. 模型表示

> `分类`
>
> $\quad y=0\ \mbox{or} \ 1$
>
> $\quad h_\theta(x)\ \mbox{can be}\ >1\ \mbox{or}\ <0$
>
> $\color{brown}\Downarrow逻辑函数$
>
> `逻辑回归`
>
> $\quad0\leq{h_\theta(x)}\leq{1}\quad$

假设函数$h_\theta(x)=g(\theta^Tx)=\dfrac{1}{1+e^{-\theta^Tx}}\quad$`在线性回归模型的基础上加了一个Sigmoid函数 `

其中$g(z)=\dfrac{1}{1+e^{-z}}\quad$`sigmoid函数（S型曲线）`

​        $g'(z)=\dfrac{e^{(-z)}}{(1+e^{(-z)})^2}=\dfrac{1}{1+e^{(-z)}}(1-\dfrac{1}{1+e^{(-z)}})=g(z)(1-g(z))$

###2. 代价函数(Cost function)

> 线性回归的代价函数在逻辑回归模型中是非凸函数('"non-convex"')，不容易收敛到全局最优点，故引入新的代价函数

$J(\theta)=\dfrac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$

$\begin{equation}\begin{aligned}Cost(h_\theta(x^{(i)}),y^{(i)})&=\left\{\begin{array}{lcl}-log(h_\theta(x^{(i)})),\ \ \ \ \ \ \ \ \ y^{(i)}=1\\-log(1-h_\theta(x^{(i)})),\ \ y^{(i)}=0\end{array}\right.\\ &\color{maroon}=-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))\end{aligned}\end{equation}$

###3. 梯度下降(Gradient Descent) 

+ 目标：$\mathop{minimize}\limits_{\theta}J(\theta)$

+ 公式

  $\begin{equation}\begin{aligned}\theta_j&:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)\\ &:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot{x_j^{(i)}}\end{aligned}\end{equation}$

+ 优化算法

  `不需要手动选择学习率α`

  `通常比梯度下降更快`

  `但算法更复杂`

  + Conjugate  gradient 
  + BFGS 
  + L-­BFGS 

###4. 向量化

|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | :--------------------------------------- |
|     feature      | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|      output      | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}\quad$`每一项取值0或1` |
|        θ         | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
|    hypothesis    | $h=g(X\theta)=\begin{pmatrix}  g(\sum\limits_{i=0}^n\theta_ix_i^{(1)})\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(2)})\\...\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(m)}) \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |
|  cost function   | $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]$ |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |

###5. 正则化(Normalization)

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]{\color{salmon}+\dfrac{\lambda}{2m}\sum\limits_{j=1}^n\theta_j^2}$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

+ 向量化

  $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]+\dfrac{\lambda}{2m}(\theta^T\theta-\theta_0^2)$

  $\theta:=\theta-\dfrac{\alpha}{m}[X^T(h-y)+\lambda\theta]\quad\color{salmon}(\theta_0无正则项)$


####* 范数(norm)

- L0范数：向量中非0的元素的个数
  - L0范数可以实现稀疏
  - $\Vert\vec{x}\Vert_0$

- L1范数：向量中各个元素绝对值之和$\quad$`稀疏规则算子`

  - L1正则化会产生稀疏的特征（产生少量的特征，而其他的特征都是0）
  - 可用于特征选择
  - $\Vert\vec{x}\Vert_1=\sum\lim_{i=1}^n|x_i|\quad$
  - $d_1(I_1,I_2)=\sum\limits_p|I_1^p-I_2^p|\quad$`L1距离/曼哈顿距离`

- L2范数：向量各元素的平方和然后求平方根$\quad$`权值衰减`

  - L2正则化会产生更多特征但是都会接近于0
  - $\Vert\vec{x}\Vert_2=\sqrt{x_1^2+x_2^2+...+x_n^2}\quad$
  - $d_2(I_1,I_2)=\sqrt{\sum\limits_p(I_1^p-I_2^p)^2}\quad$`L2距离/欧氏距离`

  ​

  > L1正则化：$\lambda\Vert\theta\Vert\quad\ \ $`套索回归Lasso`
  >
  > L2正则化：$\lambda\Vert\theta\Vert^2\quad$`岭回归Ridge`

###6. 多元分类

> $y\in{0,1,2,...,n}$
>
> $\left\{\begin{array}{lcl}h_\theta^{(0)}(x)=P(y=0|x;\theta)\\h_\theta^{(1)}(x)=P(y=1|x;\theta)\\...\\h_\theta^{(n)}(x)=P(y=n|x;\theta)\end{array}\right.$

$\mbox{prediction}=\mbox{max}(h_\theta^{(i)}(x))$

<div STYLE="page-break-after: always;"></div>

##四、神经网络

###1. 模型表示 

+ $L=网络总层数=输入层+隐含层+输出层$

+ $s_l=第l层中的单元数（不包含偏置单元）$

+ $K=输出单元个数/类别数\quad\mathop{\Longrightarrow}\limits^{二分类}\quad K=1$

+ $(h_\Theta(x^{(i)}))_k=第k个输出$

+ $a^{(j)}=第j层的激励(\mbox{activation})\ \mathop{\Longrightarrow}\limits^{维度}\ (s_j+1)\times{1}$

  $a_i^{(j)}=第j层第i个单元的激励$

+ $\Theta^{(j)}=从第j层映射到第j+1层的权重矩阵\ \mathop{\Longrightarrow}\limits^{维度}\ s_{j+1}\times{(s_j+1)}$

$\quad\ \ \ \Theta_{kn}^{(j)}=从第j层映射到第j+1层第k个单元的第n个参数(权重)$

![神经网络](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络.png)



####`网络架构`

+ 输入层→隐含层→输出层

![神经网络层次](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络层次.jpeg)

###2. 非线性分类：逻辑运算

`激活函数取sigmoid函数`

+ or运算

  $h_\Theta(x)=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![or](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\or.png)

+ and运算

  $h_\Theta(x)=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![and](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\and.png)

+ not运算

  $h_\Theta(x)=g(10-20x)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x=0\\\approx0&x=1\end{array}\right.$

  ![not](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\not.png)

+ xor运算

  $h_\Theta(x)=a_1^{(3)}=g(-30+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(30-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![xor](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\xor.png)

+ xnor运算

  $h_\Theta(x)=a_1^{(3)}=g(-10+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(10-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  ![xnor](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\xnor.png)

###3. 分类

#### · 二分类

$\qquad y=0\ or\ 1$

#####		· `感知机(perception)`

$\qquad$`由两层神经元组成`

$\qquad$假设空间是特征空间中的所有线性分类模型，即找到一个线性方程$x\cdot{w}+b=0$，它对应于特征空间的一个

$\qquad$超平面，能够把对应的特征空间分为两部分，位于两部分中的点分别是正负两类。

$\qquad$若存在这样的超平面，则感知机的学习过程一定收敛(converge)

> 若当前感知机输出为$\hat{y}$，则感知机权重将这样调整：
>
> $\qquad w_i:=w_i+\Delta w_i\\\qquad \Delta w_i=\alpha(y-\hat{y})x_i\quad\color{salmon}(\alpha为学习率)$
>
> + 若感知机对训练样例$(x,y)$预测正确，即$\hat{y}=y$，则感知机不发生变化
> + 否则将根据错误的程度进行权重调整

####· 多元分类

$\qquad y^{(i)}\in one\ of\begin{pmatrix}1\\0\\0\\0\end{pmatrix},\begin{pmatrix}0\\1\\0\\0\end{pmatrix},\begin{pmatrix}0\\0\\1\\0\end{pmatrix},\begin{pmatrix}0\\0\\0\\1\end{pmatrix}$

![多元分类](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\多元分类.png)

###4. 代价函数(Cost function)

`逻辑回归中代价函数的一般形式`

$J(\Theta)=-\dfrac{1}{m}[\sum\limits_{i=1}^m\sum\limits_{k=1}^Ky_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-h_\Theta(x^{(i)}))_k]+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{j+1}}(\Theta_{ji}^{(l)})^2$

### 5. 反向传播算法(Backpropagation algorithm)

`梯度下降`$\quad$`迭代法`

![反向传播](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\反向传播.png)

+ Ⅰ$\quad\ $前向传播(feedforward)$\quad$`计算各层输出`

  ![神经网络](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\MP神经元.png)$\quad\color{maroon}[注]\ \mbox{M-P: McCulloch & Pitts}$

  > $a^{(1)}=x$
  >
  > $z^{(2)}=\Theta^{(1)}a^{(1)}$
  >
  > $a^{(2)}=g(z^{(2)})\quad(\mbox{add}\ a_0^{(2)})$
  >
  > $z^{(3)}=\Theta^{(2)}a^{(2)}$
  >
  > $a^{(3)}=g(z^{(3)})\quad(\mbox{add}\ a_0^{(3)})$
  >
  > $z^{(4)}=\Theta^{(3)}a^{(3)}$
  >
  > $a^{(4)}=h_\Theta(x)=g(z^{(4)})$
  >
  > $\dfrac{\partial a^{(l)}}{\partial z^{(l)}}=\begin{pmatrix}\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_1}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_1}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_1}\\\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_2}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_2}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_2}\\...&...&...&...\\\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_{s_l}}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_{s_l}}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_{s_l}}\end{pmatrix}=\begin{pmatrix}0&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_1}&0&...&0\\0&0&\dfrac{\partial a^{(l)}_2}{\partial z^{(l)}_2}&...&0\\...&...&...&...&...\\0&0&0&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_{s_l}}\end{pmatrix}_{s_l\times{(s_l+1)}}\ $`分母布局`

+ Ⅱ$\quad$反向传播$\quad$`误差逆传播`

  > 若输入层的实际输出$h_\Theta(x)$与期望的输出$y$不符，则进行误差的反向传播
  >
  > 直到网络输出的误差减少到了可以接受的程度（或 进行到预先设定的学习次数为止）

  `偏置单元可以计入δ，也可不计入`

由梯度下降公式$\theta:=\theta-\alpha\dfrac{\partial}{\partial\theta}J(\theta)$联想到神经网络的迭代：

$\dfrac{\partial J(\Theta)}{\partial \Theta^{(l)}}=\nabla_{\Theta^{(l)}}J(\Theta)=\dfrac{\partial J(\Theta)}{\partial z^{(l+1)}}\dfrac{\partial z^{(l+1)}}{\partial \Theta^{(l)}}=\delta^{(l+1)}(a^{(l)})^T$

$\delta^{(l)}=\dfrac{\partial J}{\partial z^{(l)}}=\dfrac{\partial a^{(l)}}{\partial z^{(l)}}\dfrac{\partial z^{(l+1)}}{\partial a^{(l)}}\dfrac{\partial J}{\partial z^{(l+1)}}=g'(z^{(l)})\times(\Theta^{(l)})^T\delta^{(l+1)}=(\Theta^{(l)})^T\delta^{(l+1)}\ .*\ {a^{(l)}(1-a^{(l)})}\quad$`分母布局`

$\delta_j^{(l)}=第l层第j个单元的{\color{blueviolet}\textbf{误差}}=\left\{\begin{array}{lcl}a_j^{(l)}-y_j=(h_\Theta(x))_j-y_j&\color{salmon}输出层&(l=L)\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i1}^{(l)}\delta_i^{(l+1)}\cdot{a_j^{(l)}}\cdot{(1-a_j^{(l)})}&\color{salmon}隐含层&(1<l<L)\end{array}\right.\quad$

$\quad\\\qquad\mathop{\Longrightarrow}\limits^{忽略正则项\lambda}\ \boxed{\quad\dfrac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}\quad}$

$\qquad\qquad\qquad\dfrac{\partial}{\partial\Theta^{(l)}}J(\Theta)=\begin{pmatrix}\delta_1^{(l+1)}a_0^{(l)}&\delta_1^{(l+1)}a_1^{(l)}&...&\delta_1^{(l+1)}a_{s_l}^{(l)}\\\delta_2^{(l+1)}a_0^{(l)}&\delta_2^{(l+1)}a_1^{(l)}&...&\delta_2^{(l+1)}a_{s_l}^{(l)}\\...&...&...&...\\\delta_{s_{l+1}}^{(l+1)}a_0^{(l)}&\delta_{s_{l+1}}^{(l+1)}a_1^{(l)}&...&\delta_{s_{l+1}}^{(l+1)}a_{s_l}^{(l)}\end{pmatrix}_{s_{l+1}\times{(s_l+1)}}$



#### · 步骤

1. Set $\ \triangle_{ij}^{(l)}=0\ $(for all $i,j,l$)$\quad$`误差矩阵`

2. For $i$=1 to $m$

   $\quad$Set $\ a^{(1)}=x^{(i)}$

   $\quad$执行前向传播算法计算$a^{(l)},\ l=2,3,...,L$

   $\quad$计算$\delta^{(L)}=a^{(L)}-y^{(i)}$

   $\quad$计算$\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$

   $\quad \triangle_{ij}^{(l)}:=\triangle_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}\quad\pmatrix{\ \ \triangle^{(l)}:=\triangle^{(l)}+\delta^{(l+1)}(a^{(l)})^T\ \ }$

3. 计算$J(\Theta)​$的偏导数

   $\left\{\begin{array}{lcl}D^{(l)}_{ij}:=\dfrac{1}{m}\triangle_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}&\mbox{if}\ j\neq{0}\\\ D^{(l)}_{ij}:=\dfrac{1}{m}\triangle_{ij}^{(l)}&\mbox{if}\ j=0\end{array}\right.$

   > $\dfrac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$

4. 迭代更新权重矩阵

   $\Theta^{(l)}:=\Theta^{(l)}-\alpha D^{(l)}$

#### · 问题

> 隐含层数过多之后，神经网络难以直接用BP算法进行训练，因为误差在多隐层内逆传播时，往往会“发散”(diverge)而不能收敛到稳定状态

###6. 向量化

|       变量       | 向量表示（$其中m为训练样本数，K为类别数，L为网络层数，s_l为第l层单元数$） |
| :------------: | :--------------------------------------- |
|    feature     | $x^{(i)}=\begin{pmatrix} x_1^{(i)}\\ x_2^{(i)}\\...\\ x_{s_L}^{(i)}\end{pmatrix}_{s_L\times{1}}\quad(1\le i\le m)$ |
|     output     | $y^{(i)}=\begin{pmatrix} y_1^{(i)}\\ y_2^{(i)}\\...\\ y_{s_L}^{(i)}\end{pmatrix}_{s_L\times{1}}\quad(1\le i\le m)\quad$`每一项取值0或1` |
| $\Theta^{(l)}$ | $\Theta^{(l)}=\begin{pmatrix}  \theta_{10}^{(l)}&\theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_l}^{(l)}\\\theta_{20}^{(l)}&\theta_{21}^{(1)}&\theta_{22}^{(l)}&...&\theta_{2s_l}^{(l)}\\\theta_{30}^{(l)}&\theta_{31}^{(l)}&\theta_{32}^{(l)}&...&\theta_{3s_l}^{(l)}\\...&...&...&...&...\\ \theta_{s_{l+1}0}^{(l)}&\theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_l}^{(l)} \end{pmatrix}_{s_{l+1}\times{(s_l+1)}}=\begin{pmatrix}(\vec\theta_1^{(l)})^T\\(\vec\theta_2^{(l)})^T\\(\vec\theta_3^{(l)})^T\\...\\(\vec\theta_{s_{l+1}}^{(l)})^T\end{pmatrix}$ |

<div STYLE="page-break-after: always;"></div>

|        变量        | 向量表示（$其中m为训练样本数，K为类别数，L为网络层数，s_l为第l层单元数$） |
| :--------------: | :--------------------------------------- |
|    activation    | $a^{(1)}=x\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \color{salmon}(l=1)\\\quad\\a^{(l)}=g(\Theta^{(l-1)}a^{(l-1)})=\begin{pmatrix}1\\g(\sum\limits_{k=0}^K\Theta_{1k}^{(l-1)}a_k^{(l-1)})\\g(\sum\limits_{k=0}^K\Theta_{2k}^{(l-1)}a_k^{(l-1)})\\...\\g(\sum\limits_{k=0}^K\Theta_{s_lk}^{(l-1)}a_k^{(l-1)})\end{pmatrix}_{(s_l+1)\times{1}}=\begin{pmatrix}a_0^{(l)}\\a_1^{(l)}\\a_2^{(l)}\\...\\a_{s_l}^{(l)}\end{pmatrix}\qquad \color{salmon}(2\le l\le L)$ |
|      error       | $\delta^{(L)}=a^{(L)}-y=h_\Theta(x)-y\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ \color{salmon}(l=L)\\\quad\\ \begin{equation}\begin{aligned}\delta^{(l)}&=((\Theta^{(l)})^T \delta^{(l+1)})\ .*g'(z^{(l)})=((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\\&=\begin{pmatrix}\sum\limits_{i=1}^{s_{l+1}}\Theta_{i1}^{(l)}\delta_i^{(l+1)}\cdot{a_1^{(l)}}\cdot{(1-a_1^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i2}^{(l)}\delta_i^{(l+1)}\cdot{a_2^{(l)}}\cdot{(1-a_2^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i3}^{(l)}\delta_i^{(l+1)}\cdot{a_3^{(l)}}\cdot{(1-a_3^{(l)})}\\...\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{is_l}^{(l)}\delta_i^{(l+1)}\cdot{a_{s_l}}^{(l)}\cdot{(1-a_{s_l}^{(l)})}\end{pmatrix}_{s_l\times{1}}\end{aligned}\end{equation}\color{salmon}\qquad(2\le l\le L-1)\\\color{blueviolet}(未计入偏置单元，故公式中\Theta^{(l)}的维度应为s_{l+1}\times{s_l}，a^{(l)}的维度应为s_l\times{1})$ |
| cost function(例) | $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]$ |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |

### 7. 梯度检验

 + 梯度的数值检验(Numerical Gradient Checking)

   ![GradientCheck](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\GradientCheck.png)

>   通过估计梯度值来检验我们计算的导数值是否真的是我们要求的
>
>   + 在代价函数上沿着切线的方向选择离两个非常近的点
>   + 然后计算两个点的平均值用以估计梯度
>   + 检查估算的梯度值与反向传播算法得到的梯度DVec（将$D^{(1)},D^{(2)},...$展开为向量）是否接近
>   + **关掉梯度检验**，使用反向传播进行学习$\quad$`梯度检验速度很慢`

$\quad\dfrac{\partial}{\partial\theta_i}J(\theta)\approx\dfrac{J(\theta_1,\theta_2,...,\theta_i+\epsilon,...\theta_n)-J(\theta_1,\theta_2,...,\theta_i-\epsilon,...\theta_n)}{2\epsilon}\qquad\color{salmon}(\epsilon是很小的正数)$

```python
for i = 1 : n,
    thetaPlus = theta;
    thetaPlus(i) = thetaPlus(i) + EPSILON;
    thetaMinus = theta;
    thetaMinus(i) = thetaMinus(i) – EPSILON;
    gradApprox(i) = (J(thetaPlus) – J(thetaMinus))/(2*EPSILON);
end;
```

+ 例题

![GradientCheckQuestion](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\GradientCheckQuestion.png)

### 8. 随机初始化

+ 如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值
+ 同理，如果我们初始所有的参数都为同一个非 0 的数，结果也是一样的

> 我们通常初始参数为正负$\epsilon$之间的**随机值**
>
> 如果权重矩阵 $\Theta^{(l)}$ 初始化过大，矩阵相乘的输出范围会非常大（例如 -400 到 400 之间的数值），这会使得向量$a^{(l)}$上的所有输出几乎是二元的：0 或 1。但如果是这样，S 型非线性函数的局部梯度 $a^{(l)}.*(1-a^{(l)})$ 在两种情况下都会是 0（梯度消失），使得$x$和$\Theta^{(l)}$的梯度都是 0。在链式反应下，之后的反向传播相乘之后得到的都会是 0。

假设我们要随机初始一个尺寸为 10×11 的参数矩阵，代码如下：

```python
Theta1 = rand(10, 11) * (2*eps) – eps
```



###9. 训练神经网络的步骤

1. 选择一个网络架构$\quad$`神经元之间的连接样式`

   + 输入单元的数量：特征$x^{(i)}$的维度

   + 输出单元的数量：分类数

     > 合理的默认选择：1个隐含层，或若有多个隐含层，则在每一层中有相同数量的隐藏单元
     >
     > （通常情况下隐含层单元的个数越多越好）

2. 训练过程

   > 1. 随机初始化权重(参数)
   >
   > 2. 利用前向传播方法计算所有输入$x^{(i)}$的$h_\Theta(x^{(i)})$
   >
   > 3. 编写计算代价函数$J(\Theta)$ 的代码
   >
   > 4. 利用反向传播方法计算偏导数$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$
   >
   > 5. 利用数值检验方法比较用反向传播计算的和用$J(\Theta)$的梯度的数值估计得到的$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$
   >
   >    `然后禁用梯度检查的代码，因为梯度检查速度很慢`
   >
   > 6. 使用梯度下降或优化算法与反向传播算法相结合，来使$J(\Theta)$最小化
   >

   `J(Θ)是非凸函数，因此梯度下降可能得到一个局部最优点`

###* Geoffrey Hinton关于BP的看法



<div STYLE="page-break-after: always;"></div>

## 五、应用机器学习的建议

###1. 交叉验证(Cross Validation) 

> $J(\theta)=\dfrac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\dfrac{\lambda}{2m}\sum\limits_{j=1}^m\theta_j^2$
>
> 使用 60% 的数据作为训练集$\qquad\qquad J_{train}(\theta)=\dfrac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
>
> 使用 20% 的数据作为交叉验证集$\qquad\ J_{cv}(\theta)=\dfrac{1}{2m_{cv}}\sum\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$
>
> 使用 20% 的数据作为测试集$\qquad\qquad J_{test}(\theta)=\dfrac{1}{2m_{test}}\sum\limits_{i=1}^{m_{test}}(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^2$
>
> + k-fold交叉验证

###2. 模型选择

> 1.  用训练集训练出 n 个模型（对应不同的多项式次数）
> 2.  对每个模型用训练集最优化$\Theta$中的参数
> 3.  分别计算 n 个模型在交叉验证集上的误差，找出误差最小的模型对应的多项式次数d
> 4.  用测试集和$J_{test}(\Theta^{(d)})$计算泛化误差(generalization error)；多项式次数d没有用测试集进行训练

![CrossValidationQuestion](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\CrossValidationQuestion.png)

###3. 偏差(Bias)与方差(Variance)

$\mbox{Error}=\mbox{Bias}^²+\mbox{Variance}+\mbox{Noise}^²$

$\quad\mbox{Bias}[\hat{f}(x)]=\mbox{E}[\hat{f}(x)-f(x)]$

$\quad\mbox{Var}[\hat{f}(x)]=\mbox{E}[\hat{f}(x)^2]-\mbox{E}[f(x)]^2$

$\quad\sigma^2:$不可消除的误差（噪声$\epsilon$），假设均值为0，方差为$\sigma$

> 由于$f(x)$是确定性的，故$\mbox{E}f(x)=f$
>
> 给定$y=f+\epsilon$，其中$\mbox{E}[\epsilon]=0$，则$\mbox{E}[y]=\mbox{E}[f+\epsilon]=\mbox{E}[f]=f$
>
> $\mbox{Var}[y]=\mbox{E}[(y-\mbox{E}[f])^2]=\mbox{E}[(y-f)^2]=\mbox{E}[(f+\epsilon-f)^2]=\mbox{E}[\epsilon^2]=\mbox{Var}[\epsilon]+\mbox{E}[\epsilon]^2=\sigma^2$
>
> $\begin{equation}\begin{aligned}\mbox{E}[(y-\hat{f})^2]&=\mbox{E}[y^2+\hat{f}^2-2y\hat{f}]\\&=\mbox{E}[y^2]+\mbox{E}[\hat{f}^2]-\mbox{E}[2y\hat{f}]\\&=\mbox{Var}[y]+\mbox{E}[y]^2+\mbox{Var}[\hat{f}]+\mbox{E}[\hat{f}]^2-2d\mbox{E}[\hat{f}]\\&=\mbox{Var}[y]+\mbox{Var}[\hat{f}]+(f^2-2f\mbox{E}[\hat{f}]+\mbox{E}[\hat{f}]^2)\\&=\mbox{Var}[y]+\mbox{Var}[\hat{f}]+(f-\mbox{E}[\hat{f}])^2\\&=\sigma^2+\mbox{Var}[\hat{f}]+\mbox{Bias}[\hat{f}]^2\end{aligned}\end{equation}$

![bias&variance](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\bias&variance.png)$\quad\begin{array}\\\mbox{Bias (underfit):}\\\quad \color{maroon}J_{train}(\theta)\mbox{will be high}\\\quad \color{maroon}J_{cv}(\theta)\approx J_{train}(\theta)\\\quad\\\mbox{Variance (overfit):}\\\quad \color{maroon}J_{train}(\theta)\mbox{will be low}\\\quad \color{maroon}J_{cv}(\theta)\gg J_{train}(\theta)\end{array}$

![bias&variance&lambda](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\bias&variance&lambda.png)$\qquad\qquad\quad$**正则化参数**$\lambda\Rightarrow\left\{\begin{array}{lcl}\lambda过大会导致欠拟合\\\quad\\\lambda偏小时可能出现过拟合\end{array}\right.$

偏差-方差权衡(Bias–variance tradeoff)$\quad$`有监督学习的核心问题`

+ 同时使偏差和方差最小

+ 高偏差会导致算法丢失特征与目标输出的相关关系$\quad$`欠拟合`

  + 增加训练数据帮助不大

    $\quad$![getMoreTrainingData](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\getMoreTrainingDataForBias.png)

+ 高方差会导致算法对训练集中的随机噪声建模，而不是预设的输出$\quad$`过拟合`

  + 增加训练数据可能会有帮助

    $\quad$![getMoreTrainingDataForVar](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\getMoreTrainingDataForVar.png)

###4. 学习曲线

对学习算法的一个**合理检验**

![learningCurve](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\learningCurve.png)

###5. 调试一个学习算法

假设实现正则化线性回归时，发现预测结果的误差很大，接下来应该怎么尝试？

+ 扩大训练集$\qquad\qquad\qquad\qquad\qquad\qquad\qquad$(解决**高方差**问题)
+ 尝试更小的特征集$\qquad\qquad\qquad\qquad\qquad\quad\ $(解决**高方差**问题)
+ 尝试增加额外特征$\qquad\qquad\qquad\qquad\qquad\quad\ $(解决**高偏差**问题)
+ 尝试增加多项式特征（$x_1^2,x_2^2,x_1x_2$等）$\qquad\ \ \ $(解决**高偏差**问题)
+ 尝试减小λ$\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ $(解决**高偏差**问题)
+ 尝试增大λ$\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ $(解决**高方差**问题)

###6. 机器学习系统设计

> 1. 从一个简单、能快速实现的算法开始实现，并用交叉验证集的数据测试它
>
> 2. 画出学习曲线来决定是否使用更多数据、更多特征会有帮助
>
> 3. 误差分析：手动检查算法在**交叉验证集**中出现误差的样例，观察其出现误差的数据样例类型是否呈现出任何系统趋势
>
>    *Error analysis may not be helpful for deciding if this is likely to improve performance.*
>
>    *Only solution is to try it and see if it works.*

![ErrorAnalysisQuestion](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\ErrorAnalysisQuestion.png)

####· 偏斜类(skewed classes)的误差度量

>    `训练集中有非常多的同一种类的样本数据，只有很少或没有其他类的样本数据(导致误差增大)`
>
>    + 准确率(Accuracy)
>
>      $\mbox{Accuracy}=\dfrac{\mbox{True positive}+\mbox{True negative}}{\bf\mbox{total examples}}$
>
>
>    + 精确率(Precision)与召回率(Recall)
>
>      + 精确率/查准率：对于给定的测试数据集，预测正确的(正类)样本数与所有被预测的(正类)样本数之比
>
>        $\mbox{Precision}=\dfrac{\mbox{True positive}}{\bf\mbox{predicted as positive}}=\dfrac{\color{blue}\mbox{True positive}}{{\color{blue}\mbox{True positive}}+{\color{red}\mbox{False positive}}}$
>
>        $(P=\dfrac{TP}{TP+FP})$
>
>      + 召回率/查全率：对于给定的测试数据集，预测正确的(正类)样本数与所有应该被正确分类(正类)的样本数之比
>
>        $\mbox{Recall}=\dfrac{\mbox{True positive}}{\bf\mbox{actual positive}}=\dfrac{\color{blue}\mbox{True positive}}{{\color{blue}\mbox{True positive}}+{\color{blueviolet}\mbox{False negative}}}$
>
>        $(R=\dfrac{TP}{TP+FN})$
>
>        ![precision&recall](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\precision&recall.png)$\quad y=1$表示我们希望检测的样本中很少的那类
>
>      + 权衡精确率与召回率
>
>        + 若希望只在非常确信的情况下预测为正类$\ \Longrightarrow\ \left\{\begin{array}{lcl}\mbox{Higher precision}\\\mbox{Lower recall}\end{array}\right.$
>
>        + 若希望提高查全率$\ \Longrightarrow\ \left\{\begin{array}{lcl}\mbox{Higher recall}\\\mbox{Lower precision}\end{array}\right.$
>
>        + 一般地，$\left\{\begin{array}{lcl}当h_\theta(x)\ge\mbox{threshold}时预测为1\\当h_\theta(x)\lt\mbox{threshold}时预测为0\end{array}\right.$
>
>        + P-R曲线
>
>          ![precision-recallCurve](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\precision-recallCurve.png)
>
>          + 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者
>          + 平衡点(Break-Even Point, BEP)：查准率=查全率时的取值
>            + 基于BEP比较，可认为平衡点的值较大者性能更优
>
>      + $F_1$度量
>
>        精确率和召回率的调和均值
>
>        $\dfrac{2}{F_1}=\dfrac{1}{P}+\dfrac{1}{R}$
>
>        即$F_1=\dfrac{2PR}{P+R}=\dfrac{2TP}{2TP+FP+FN}\qquad\Longrightarrow\ \left\{\begin{array}{lcl}P=0\ \mbox{or}\ R=0\ \color{maroon}\ \ \ \Rightarrow\ F=0\\P=1\ \mbox{and}\ R=1\ \color{maroon}\Rightarrow\ F=1\end{array}\right.$
>
>        `在交叉验证集上测得精确率P和召回率R，并选取使F₁最大的阈值`

#### · 使用大量数据集

> 假设使用一种需要**大量参数**的学习算法（有很多特征的逻辑回归/线性回归，有许多隐藏单元的神经网络），特征值有**足够的信息**来预测 y 值$\quad$`低偏差`
>
> $\quad J_{train}(\theta)\ $ will be small
>
> 使用很大的训练集（不太可能过拟合）`低方差`
>
> $\quad J_{train}(\theta)\approx J_{test}(\theta)$
>
> $\qquad\Longrightarrow J_{test}(\theta)\ $ will be small

<div STYLE="page-break-after: always;"></div>

##六、支持向量机(Support Vector Machine)

`适用于复杂但数据集属于中小型的分类`

`通过一个超平面将不同分类的数据分离开`

###1. 模型表示

$h_\theta(x)=g(\theta^Tx)=\left\{\begin{array}{lcl}1&\mbox{if }\theta^Tx\ge 0\\0&\mbox{otherwise}\end{array}\right.$

###2. 代价函数(Cost function)  

> 对比逻辑回归：
>
> + 用更简单的函数替代逻辑回归的cost(z)函数
> + 去掉$\dfrac{1}{m}$（不影响求解θ最优值的结果）
> + $\sum\limits_{i=1}^n\theta_j^2$前面的λ替换成$\sum\limits_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]$前面的*C*（SVM更关心第一项的优化，逻辑回归更关心后一项即θ的优化；C的取值很大意味着给前一项更大的权重，C的取值很小则意味着给后一项更大的权重，类似于逻辑回归中λ取值很大的情况）

$J(\theta)=C\sum\limits_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\dfrac{1}{2}\sum\limits_{j=1}^n\theta_j^2$

![svm_cost](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\svm_cost.png)

假设*C*很大：

+ if y=1, we want $\theta^Tx\ge 1(\mbox{not just} \ge 0)$
+ if y=0, we want $\theta^Tx\le -1(\mbox{not just} \lt 0)$

###3. 线性SVM

`不使用核函数("线性核函数")`$k(x_1,x_2)=x_1^Tx_2$

如果 **C 非常大**，则在最小化代价函数时希望使

$\qquad\qquad\sum\limits_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]=0$

$\qquad\qquad$即$\left\{\begin{array}{lcl}\theta^Tx^{(i)}\ge 1&\mbox{if }y^{(i)}=1\\\theta^Tx^{(i)}\le -1&\mbox{if }y^{(i)}=0\end{array}\right.$

$\qquad\qquad$则$J(\theta)=\dfrac{1}{2}\sum\limits_{j=1}^n\theta_j^2$

> `向量内积`
>
> ![VectorInnerProduct](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\VectorInnerProduct.png)
>
> + p为$\vec{v}$投影到$\vec{u}$上的长度$\ \Rightarrow\ \left\{\begin{array}{lcl}p\gt 0&\vec{u}\mbox{和}\vec{v}\mbox{的夹角}\lt90^\circ\\p\lt 0&\vec{u}\mbox{和}\vec{v}\mbox{的夹角}\gt90^\circ\end{array}\right.$
> + $\vec{u}^T\vec{v}=p\cdot{\Vert\vec{u}\Vert}_2=u_1v_1+u_2v_2$

+ 考虑两个特征的情况$\vec{x}^{(i)}=\begin{pmatrix} x_1^{(i)}\\ x_2^{(i)}\end{pmatrix}$，忽略截距(*θ*~0~=0)，则

  $\qquad J(\theta)=\dfrac{1}{2}(\theta_1^2+\theta_2^2)=\dfrac{1}{2}(\sqrt{\theta_1^2+\theta_2^2})^2=\dfrac{1}{2}\Vert\theta\Vert_2^2$

  $\qquad\qquad\Longrightarrow$使$J(\theta)$最小，即最小化$\Vert\theta\Vert_2^2$，也即向量*θ*长度的平方

  $\qquad\theta^Tx^{(i)}=p^{(i)}\cdot{\Vert\vec{\theta}\Vert_2}=\theta_1x_1^{(i)}+\theta_2x_2^{(i)}$



  + 训练数据是**线性可分**的——Hard-margin

    `硬间隔：所有样本都必须划分正确`

    $\left\{\begin{array}{lcl}C\mbox{ is very large}\left\{\begin{array}{lcl}\theta^Tx^{(i)}\ge 1&\mbox{if }y^{(i)}=1\\\theta^Tx^{(i)}\le -1&\mbox{if }y^{(i)}=0\end{array}\right.\\\quad\\\theta^Tx=wx+b\quad(b=\theta_0)\end{array}\right.\Longrightarrow\left\{\begin{array}{lcl}wx+b\le-1\\wx+b\ge1\end{array}\right.$

    ![svmMargin](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\svmMargin.png)

    > + 选择两条平行的超平面(hyperplane)将两类数据分开，使得它们之间的距离尽可能大
    > + 被这两个超平面界定的区域称作"间距"(margin)
    > + 如果距离分类超平面最近的数据点的距离(margin)越大，该分类器的噪声容忍度就越大，即鲁棒性更好
    > + 具有最大间距的超平面就在这两个超平面正中间$\quad$`判定边界(Decision Boundary)`
    >
    > $\left\{\begin{array}{lcl}\theta^Tx^{(i)}\ge 1&\mbox{if }y^{(i)}=1\\\theta^Tx^{(i)}\le -1&\mbox{if }y^{(i)}=0\end{array}\right.\Longrightarrow\left\{\begin{array}{lcl}p^{(i)}\cdot{\Vert\theta\Vert}_2\ge 1&\mbox{if }y^{(i)}=1\\p^{(i)}\cdot{\Vert\theta\Vert}_2\le -1&\mbox{if }y^{(i)}=0\end{array}\right.$
    >
    > + 对于正样本，需要$p^{(i)}\cdot{\Vert\theta\Vert}_2\ge 1$，因而当$p^{(i)}$相对大时，$\theta$就会更小
    >
    > 对于硬间距SVM，支持向量是那些在margin上的数据点


`大间距分类器(Large margin classifier)`

$\quad$努力将正样本和负样本用最大间距分开

$\qquad\qquad$![svmDecisionBoundary](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\svmDecisionBoundary.png)


  + 训练数据**不完全线性可分**——Soft-margin

    > `重新定义`
    >
    > 为方便推导，定义$h_\theta(x)=g(\theta^Tx)=g(w^Tx+b)=\left\{\begin{array}{lcl}1&\mbox{if }w^Tx+b\ge +1\\-1&\mbox{if }w^Tx+b\le -1\end{array}\right.$，样本点$(x_i,y_i)$的$y_i$取值1或-1
    >
    > + 定义样本点$(x_i,y_i)$与超平面$(w, b)$之间的**函数间隔(functional margin)**为$\boxed{\gamma_{i} = y_{i} (w^Tx_{i} + b)}\quad\color{salmon}\begin{array}{lcl}该定义存在问题：即w和b同时缩小或放大M倍后，超平面并没有变化，\\但是函数间隔却变化了(未归一化的距离)\end{array}$
    >
    >
    > + 定义样本点$(x_i,y_i)$与超平面$(w, b)$之间的**几何间隔(geometric margin)**为$\boxed{\gamma_{i} = y_{i} (\dfrac{w^T}{\Vert w\Vert_2}x_{i} + \dfrac{b}{\Vert w\Vert_2})}\quad\color{salmon}\begin{array}{lcl}实际上，几何距离就是点到超平面的距离，类比点(x_i,y_i)到直ax\\+by+c=0的距离公式d(x_i,y_i)=\dfrac{|ax_i+by_i+c|}{\sqrt{a^2+b^2}}\end{array}$
    >
    > + 假设超平面$(w,b)$能将训练样本正确分类，则有$\left\{\begin{array}{lcl}w^Tx_i+b\ge+1,&y_i=+1\\w^Tx_i+b\le-1,&y_i=-1\end{array}\right.$
    >
    >   **支持向量(Support Vector)**：距离超平面最近的几个使上式等号成立的训练样本点(函数间隔为1)
    >
    >   **间隔(margin)**：两个异类支持向量到超平面的距离之和，即$\gamma=\dfrac{2}{\Vert w\Vert_2}$
    >
    > + SVM可以表述为求解下列优化问题
    >   $\color{maroon}\underset{w, b}{\mbox{max}} \;\gamma$
    >
    >   $\quad\color{maroon}\mbox{s.t.} \;\;\; y_{i} (\dfrac{w^T}{\Vert w\Vert_2}x_{i} + \dfrac{b}{\Vert w\Vert_2})\ge \gamma,\quad i=1,2,...,m$
    >
    >   显然，为了最大化间隔，仅需最小化$\Vert w\Vert_2^2$，于是问题等效于
    >
    >   $\color{maroon}\underset{w, b}{\mbox{min}}\ \dfrac{1}{2}\Vert w\Vert_2^2$
    >
    >   $\quad\color{maroon}\mbox{s.t.} \;\;\; y_{i} (w^Tx_i+b)\ge 1,\quad i=1,2,...,m$
    >
    >   + 由于目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题，可以用现成的QP优化包进行求解
    >   + 还可以通过拉格朗日对偶性 (Lagrange Duality) 变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题 (dual problem) 得到原始问题的最优解

    `软间距：容许一些误分类点`

    ![soft-marginSVM](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\soft-marginSVM.png)

    >
    > + 线性不可分意味着某些样本点不能满足函数间隔大于等于1的约束条件$y_{i}(w^{T}x_{i}+b) \ge 1$
    >
    >   软间距SVM的优化目标是在最大化间隔的同时，不满足约束的样本应尽可能少，即
    >
    >   $\color{maroon}\underset{w, b}{\mbox{min}}\ \dfrac{1}{2}\lVert w \rVert^{2}_2 + C\sum\limits_{i=1}^{m}\ell_{0/1}(y_i(w^Tx_i+b)-1)$
    >
    >   其中C>0是一个常熟，$\ell_{0/1}$是0/1损失函数：$\ell_{0/1}(z)=\left\{\begin{array}{lcl}1,&\mbox{if}\ z\lt0\\0,&\mbox{otherwise}\end{array}\right.$
    >
    >   $\ell_{0/1}$非凸、非连续，数学性质不太好，使得优化目标不易直接求解，人们常用其他函数代替$\ell_{0/1}$，称为"替代损失"：
    >
    >   $\qquad\begin{array}{lcl}hinge损失:\ \ell_{hinge}(z)=\mbox{max}(0,1-z)\\指数损失:\ \ell_{exp}(z)=exp(-z)\\对率损失:\ \ell_{log}(z)=log(1+exp(-z))\end{array}$
    >
    >   若采用hinge损失，则优化目标变为
    >
    >   $\color{maroon}\underset{w, b}{\mbox{min}}\ \dfrac{1}{2}\lVert w \rVert^{2}_2 + C\sum\limits_{i=1}^{m}\mbox{max}(0,1-y_i(w^Tx_i+b))$
    >
    >+ 对每个样本点，引入一个**松弛变量(slack variable)**$\ \xi_{i} \ge 0$, 使函数间隔加上松弛变量大于等于1，这样约束条件变为：$y_{i}(w^{T}x_{i}+b) \ge 1 - \xi_{i}$
    >
    >  同时，对于每个松弛变量$\xi_{i}$，要付出一个代价，目标函数由原来的$\frac{1}{2}\lVert w \rVert^{2}_2$变为：
    >
    >  $\color{maroon}\underset{w, b, \xi_i}{\mbox{min}}\ \dfrac{1}{2}\lVert w \rVert^{2}_2 + C\sum\limits_{i=1}^{m}\xi_{i}$
    >
    >  $\quad\color{maroon}\begin{array}{lcl}\mbox{s.t.} \;\;\;&y_{i} (w^Tx_i+b)\ge 1-\xi_i\\\;&\xi_i\ge0,\ i=1,2,...,m\end{array}$
    >
    >  其中，C>0为惩罚参数。C越大表示对错误分类的惩罚力度越大，越小则表示惩罚力度越小
    >

###4. 非线性SVM —— 核函数(kernel)

`kernel trick / kernel method`

对于线性不可分的数据，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题

[SVM可视化](https://v.qq.com/x/page/k05170ntgzc.html)

![kernel](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\kernel.png)

给定一个样本点，计算其到标记点(landmark)$\ l^{(1)},l^{(2)},l^{(3)},...l^{(n)}$的距离：

$\quad$相似度函数$f_i=\mbox{similarity}(x,l^{(i)})=k(x,l^{(i)})\ \color{maroon}\Longleftrightarrow\ 核函数\ \Longleftrightarrow\ 内积$

![landmarks](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\landmarks.png)

> 核函数不是从低维度到高维度的映射，相关数学参见[泛函分析](#一、泛函分析(functional analysis))
>
> $\phi(x)$表示将$x$映射后的特征向量，则在特征空间中划分超平面所对应的模型可表示为$w^T\phi(x)+b$
>
> 优化目标变为$\begin{array}{lcl}\underset{w,b}{\mbox{min}}\;\;\dfrac{1}{2}\Vert w\Vert^2_2\\s.t.\;\;\;y_i(w^T\phi(x_i)+b)\ge1,&i=1,2,...,m\end{array}$
>
> 如果原始空间是有限维，那么一定存在一个高维特征空间使样本线性可分

+ 高斯核函数(Gaussian kernel)

  ![GaussianKernel](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\GaussianKernel.png)

  $k(x,l^{(i)})=\exp(-\dfrac{\Vert x-l^{(i)}\Vert^2}{2\sigma^2})\qquad\begin{array}{lcl}(\sigma控制核函数随x改变的速率\\\ \ \sigma越大，核函数随x变化速率越慢)\end{array}$

  + 若$x$接近$l^{(i)}$，则$f_i\approx1$
  + 若$x$远离$l^{(i)}$，则$f_i\approx0$

  **predict “y=1” when** $\theta_0+\theta_1f_1+\theta_2f_2+...+\theta_nf_n\ge0$

  + 选取landmarks

    > 给定训练样本$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$
    >
    > 选择$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},...,l^{(m)}=x^{(m)}\quad$(根据训练集的数量选择landmark的数量)
    >
    > $f^{(i)}=\begin{pmatrix} f_0^{(i)}\\ f_1^{(i)}\\ f_2^{(i)}\\...\\ f_{m}^{(i)}\end{pmatrix}=\begin{pmatrix} 1\\ sim(x^{(i)},l^{(1})\\ sim(x^{(i)},l^{(2})\\...\\\color{blueviolet}sim(x^{(i)},l^{(i)})=e^0=1\\...\\sim(x^{(i)},l^{(m)})\end{pmatrix}_{m\times{1}}$
    >
    > 假设函数：$h_\theta(x)=\theta^Tf(x)$
    >
    > $\underset{\theta}{\mbox{min}}\ C\sum\limits_{i=1}^my^{(i)}\mbox{cost}_1(\theta^Tf^{(i)})+(1-y^{(i)})\mbox{cost}_0(\theta^Tf^{(i)})+\dfrac{1}{2}\sum\limits_{j=1}^m\theta_j^2$

  + 在使用高斯核函数之前须做**特征缩放**

+ 核函数的选择

  需要满足Mercer’s Theorem

+ 常用核函数

  + Polynomial kernel
  + String kernel
  + chi-square kernel
  + histogram intersection kernel

### 5. 选择参数

+ $C\Longleftrightarrow\dfrac{1}{\lambda}$

  *Large C* : Lower bias, high variance

  *Small C* : Higher bias, low variance


+ $\sigma^2$

  *Large* $\ \sigma^2$ : 特征$f_i$变化平缓

  $\qquad\qquad\ \ $Higher bias, lower variance

  *Small* $\ \sigma^2$ : 特征$f_i$变化更剧烈

  $\qquad\qquad\ \ $Lower bias, higher variance

###6. 逻辑回归 vs. SVMs

n为特征数($x\in\mathbb{R}^{n+1}$)，m为训练样本数

+ 如果n很大（相对于m）`如n=10000,m=10~1000`：

  使用逻辑回归，或不使用核函数的SVM(“线性核函数”)

+ 如果n较小，而m为中等大小`如n=1~1000,m=10~10000`：

  使用高斯核函数的SVM

+ 如果n较小，m很大`如n=1~1000,m=50000+`：

  创造/增加更多特征，然后使用逻辑回归或不使用核函数的SVM(“线性核函数”)

神经网络可能对以上大多数情况都能很好工作，但可能训练较慢

<div STYLE="page-break-after: always;"></div>

#附录

## 一、泛函分析(functional analysis)

###1.  