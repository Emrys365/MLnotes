![封面](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\封面.png)

<div STYLE="page-break-after: always;"></div>

#机器学习笔记

[TOC]

<div STYLE="page-break-after: always;"></div>

##一、线性代数相关

###1.矩阵的求导 

|               表达形式               |             公式$\quad$`分子布局`              |
| :------------------------------: | :--------------------------------------: |
| $\dfrac{\partial向量}{\partial标量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\qquad\dfrac{\partial \vec{y}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x}\\\dfrac{\partial y_2}{\partial x}\\...\\\dfrac{\partial y_m}{\partial x} \end{pmatrix}$ |
| $\dfrac{\partial标量}{\partial向量}$ | $\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\qquad\dfrac{\partial y}{\partial \vec{x}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_1}&\dfrac{\partial y}{\partial x_2}&...&\dfrac{\partial y}{\partial x_n} \end{pmatrix}$ |
|         标量函数关于空间向量的方向导数          | $\nabla_{\vec{u}}f(x)\mathop{=}\limits^{\triangle}\nabla f(x)\cdot{\vec{u}}=\dfrac{\partial f(x)}{\partial x}\cdot{\vec{u}}$ |
| $\dfrac{\partial向量}{\partial向量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\quad\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\\\dfrac{\partial \vec{y}}{\partial \vec{x}}=\begin{pmatrix}\dfrac{\partial y_1}{\partial \vec{x}}\\\dfrac{\partial y_2}{\partial \vec{x}}\\...\\\dfrac{\partial y_m}{\partial \vec{x}}\end{pmatrix}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x_1}&\dfrac{\partial y_1}{\partial x_2}&...&\dfrac{\partial y_1}{\partial x_n}\\\dfrac{\partial y_2}{\partial x_1}&\dfrac{\partial y_2}{\partial x_2}&...&\dfrac{\partial y_2}{\partial x_n}\\...&...&...&...\\\dfrac{\partial y_m}{\partial x_1}&\dfrac{\partial y_m}{\partial x_2}&...&\dfrac{\partial y_m}{\partial x_n} \end{pmatrix}_{m\times{n}}$ |
| $\dfrac{\partial矩阵}{\partial标量}$ | $\dfrac{\partial \bf{Y}_{m\times{n}}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_{11}}{\partial x}&\dfrac{\partial y_{12}}{\partial x}&...&\dfrac{\partial y_{1n}}{\partial x}\\\dfrac{\partial y_{21}}{\partial x}&\dfrac{\partial y_{22}}{\partial x}&...&\dfrac{\partial y_{2n}}{\partial x}\\...&...&...&...\\\dfrac{\partial y_{m1}}{\partial x}&\dfrac{\partial y_{m2}}{\partial x}&...&\dfrac{\partial y_{mn}}{\partial x} \end{pmatrix}_{m\times{n}}$ |

<div STYLE="page-break-after: always;"></div>

|               表达形式               |             公式$\quad$`分子布局`              |
| :------------------------------: | :--------------------------------------: |
| $\dfrac{\partial标量}{\partial矩阵}$ | $\dfrac{\partial y}{\partial \bf{X}_{p\times{q}}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_{11}}&\dfrac{\partial y}{\partial x_{21}}&...&\dfrac{\partial y}{\partial x_{p1}}\\\dfrac{\partial y}{\partial x_{12}}&\dfrac{\partial y}{\partial x_{22}}&...&\dfrac{\partial y}{\partial x_{p2}}\\...&...&...&...\\\dfrac{\partial y}{\partial x_{1q}}&\dfrac{\partial y}{\partial x_{2q}}&...&\dfrac{\partial y}{\partial x_{pq}} \end{pmatrix}_{q\times{p}}\\\quad\\通常写作\nabla_{\bf{X}}y(\bf{X})=\dfrac{\partial y(\bf{X})}{\partial \bf{X}}$ |
|          标量函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}f=tr\pmatrix{\dfrac{\partial f}{\partial \bf{X}}\bf{Y}}$ |
| $\dfrac{\partial矩阵}{\partial矩阵}$ | $\dfrac{\partial \bf{F}_{p\times{q}}}{\partial \bf{X}_{n\times{m}}}=\begin{pmatrix} \dfrac{\partial \bf{F}}{\partial \bf{X_{1,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,n}}}\\\dfrac{\partial \bf{F}}{\partial \bf{X_{2,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,n}}}\\...&...&...&...\\\dfrac{\partial \bf{F}}{\partial \bf{X_{m,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,n}}} \end{pmatrix}_{mp\times{nq}}\\\quad\\其中每一项\dfrac{\partial \bf{F}}{\partial \bf{X_{i,j}}}都是p\times{q}的矩阵$ |
|          矩阵函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}\bf{F}=tr\pmatrix{\dfrac{\partial \bf{F}}{\partial \bf{X}}\bf{Y}}$ |

#### · 通用公式

$\quad(\bf{X}、\bf{Y}均为矩阵)$

> $\dfrac{\partial \bf{X}}{\partial \bf{X}}={\bf{I}}$
>
> $\dfrac{\partial a{\bf{Y}}}{\partial \bf{X}}=a\dfrac{\partial {\bf{Y}}}{\partial \bf{X}}\quad(a不是\bf{X}的函数)$
>
> $\dfrac{\partial f(g(u))}{\partial{\bf{X}}}=\dfrac{\partial f(g)}{\partial g}\dfrac{\partial g(u)}{\partial u}\dfrac{\partial u}{\partial{\bf{X}}}\quad(f,g,u均为标量函数)$
>
> $\dfrac{\partial^2 \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}^2}={\bf{A}}+{\bf{A}}^T\quad({\bf{A}}不是\vec{x}的函数，\vec{x}^T{\bf{A}}\vec{x}为标量)$
>
> $\dfrac{\partial\vec{u}^T}{\partial x}=\pmatrix{\dfrac{\partial \vec{u}}{\partial x}}^T$

`分子布局`

$\qquad$根据$\bf Y$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad$根据${\bf X}^T$来布局$\dfrac{\partial y}{\partial\bf{X}}$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为列向量，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为行向量

`分母布局`

$\qquad$根据${\bf Y}^T$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad​$根据$\bf{X}​$来布局$\dfrac{\partial y}{\partial\bf{X}}​$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为列向量，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为行向量

|          分子布局(Numerator Layout)          |         分母布局(Denominator Layout)         |
| :--------------------------------------: | :--------------------------------------: |
| $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ m\times{n}$ | $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ n\times{m}$ |
| $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\vec{u}\dfrac{\partial a}{\partial\vec{x}}$ | $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\dfrac{\partial a}{\partial\vec{x}}\vec{u}^T$ |
| $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}={\bf{A}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}({\bf{A}}_{p\times{q}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}{\bf{A}}^T$<br>$\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\vec{u}^T\dfrac{\partial \vec{v}}{\partial \vec{x}}+\vec{v}^T\dfrac{\partial \vec{u}}{\partial \vec{x}}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ | $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\dfrac{\partial \vec{v}}{\partial \vec{x}}\vec{u}+\dfrac{\partial \vec{u}}{\partial \vec{x}}\vec{v}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ |
| $\dfrac{\partial\vec{u}^T{\bf{A}}{\vec{v}}}{\partial \vec{x}}=\vec{u}^T{\bf{A}}\dfrac{\partial \vec{v}}{\partial \vec{x}}+\vec{v}^T{\bf{A}}^T\dfrac{\partial \vec{u}}{\partial \vec{x}}$<br>$\color{blueviolet}(\vec{u}_{m\times{1}}是\vec{x}的函数，\vec{v}_{n\times{1}}是\vec{x}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}的函数)$ | $\dfrac{\partial\vec{u}^T{\bf{A}}{\vec{v}}}{\partial \vec{x}}=\dfrac{\partial \vec{v}}{\partial \vec{x}}{\bf{A}}^T\vec{u}+\dfrac{\partial \vec{u}}{\partial \vec{x}}{\bf{A}}\vec{v}$<br>$\color{blueviolet}(\vec{u}^T\vec{v}是标量，{\bf{A}}不是\vec{x}的函数)$ |
| $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=\vec{x}^T({\bf{A}}+{\bf{A}}^T)\quad\color{blueviolet}({\bf{A}}_{n\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=({\bf{A}}+{\bf{A}}^T)\vec{x}\quad\color{blueviolet}({\bf{A}}_{n\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |

<div STYLE="page-break-after: always;"></div>

|          分子布局(Numerator Layout)          |         分母布局(Denominator Layout)         |
| :--------------------------------------: | :--------------------------------------: |
| $\dfrac{\partial \vec{b}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=\vec{b}^T{\bf{A}}$<br>$\color{blueviolet}(\vec{b}_{m\times{1}}不是\vec{x}_{n\times{1}}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{b}^T{\bf{A}}\vec{x}}{\partial \vec{x}}={\bf{A}}^T\vec{b}$<br>$\color{blueviolet}(\vec{b}_{m\times{1}}不是\vec{x}_{n\times{1}}的函数，{\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial \vec{x}^T\vec{x}}{\partial \vec{x}}=2\vec{x}^T$ | $\dfrac{\partial \vec{x}^T\vec{x}}{\partial \vec{x}}=2\vec{x}$ |
| $\dfrac{\partial \vec{a}^T\vec{x}\vec{x}^T\vec{b}}{\partial \vec{x}}=\vec{x}^T(\vec{a}\vec{b}^T+\vec{b}\vec{a}^T)$<br>$\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数，\vec{b}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial \vec{a}^T\vec{x}\vec{x}^T\vec{b}}{\partial \vec{x}}=(\vec{a}\vec{b}^T+\vec{b}\vec{a}^T)\vec{x}$<br>$\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数，\vec{b}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\ \Vert\vec{x}-\vec{a}\Vert}{\partial \vec{x}}=\dfrac{(\vec{x}-\vec{a})^T}{\Vert\vec{x}-\vec{a}\Vert}\quad\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\ \Vert\vec{x}-\vec{a}\Vert}{\partial \vec{x}}=\dfrac{\vec{x}-\vec{a}}{\Vert\vec{x}-\vec{a}\Vert}\quad\color{blueviolet}(\vec{a}_{n\times{1}}不是\vec{x}_{n\times{1}}的函数)$ |

对${\bf{f}}({\bf{X}}):\mathbb{R}^{m\times{n}}\rightarrow\mathbb{R}^{p\times{q}}$求导**不存在**与标量函数完全等同的乘积法则$(f\cdot{g})'=f'\cdot{g}+f\cdot{g'}$和链式法则$\dfrac{dz}{dx}=\dfrac{dz}{dy}\cdot{\dfrac{dy}{dx}}$

> *$\ $维度相容原则$\quad$`Trick`
>
> 通过**前后换序**、**转置** 使求导结果满足矩阵乘法且结果维数满足下式，那么结果就是正确的：
>
> $\qquad如果\vec{x}\in \mathbb{R}^{m\times{n}},f(\vec{x})\in \mathbb{R}^1，那么\dfrac{\partial f(\vec{x})}{\partial\vec{x}}\in \mathbb{R}^{m\times{n}}​$

###2. 雅各比矩阵(Jacobian Matrix)

> $\vec{f}(\vec{x}):\mathbb{R}^n\rightarrow\mathbb{R}^m是向量，那么其雅各比矩阵{\bf{J}}是一个m\times{n}的矩阵，定义如下:$

$\quad{\bf{J}}=\begin{pmatrix}\dfrac{\partial\bf{f}}{\partial x_1}&\dfrac{\partial\bf{f}}{\partial x_2}&...&\dfrac{\partial\bf{f}}{\partial x_n}\end{pmatrix}=\begin{pmatrix}\dfrac{\partial f_1}{\partial x_1}&\dfrac{\partial f_1}{\partial x_2}&...&\dfrac{\partial f_1}{\partial x_n}\\\dfrac{\partial f_2}{\partial x_1}&\dfrac{\partial f_2}{\partial x_2}&...&\dfrac{\partial f_2}{\partial x_n}\\...&...&...&...\\\dfrac{\partial f_m}{\partial x_1}&\dfrac{\partial f_m}{\partial x_2}&...&\dfrac{\partial f_m}{\partial x_n}\end{pmatrix}$

$\quad{\bf{J}}_{i,j}=\dfrac{\partial f_i}{\partial x_j}$

> 如果$\vec{f}$在$\vec{x}$点处可微，则雅各比矩阵${\bf{J}}$定义了一个线性映射$\mathbb{R}^n\rightarrow\mathbb{R}^m$，这是对函数$\vec{f}$在$\vec{x}$点处最优（逐点的）线性估计，被称作$\vec{f}$在$\vec{x}$点处的导数或微分

如果$m=n$，那么雅各比矩阵${\bf{J}}$为一个方阵，其行列式称为**雅各比行列式 (Jacobian Determinant)**

+ 当且仅当雅各比行列式在$\vec{x}$点处不等于零时，函数$\vec{f}$在$\vec{x}$点附近有反函数 (inverse function)`反函数定理(Inverse function theorem)`
+ 若雅各比行列式的值为正，则$\vec{f}$在$\vec{x}$点附近的取向不变；若为负，则取向相反
+ 若雅各比行列式恒等于零，则函数组$\pmatrix{f_1&f_2&...&f_m}$是函数相关的，其中至少有一个函数是其余函数的一个连续可微的函数

###3. 海森矩阵(Hessian Matrix)

> $f(\vec{x}):\mathbb{R}^n\rightarrow\mathbb{R}是标量，如果f的所有二阶导数存在且在函数定义域内连续，则f的海森矩阵{\bf{H}}是一个n\times{n}的方阵，定义如下:$

$\quad{\bf{H}}=\begin{pmatrix}\dfrac{\partial^2f}{\partial x_1^2}&\dfrac{\partial^2f}{\partial x_1\partial x_2}&...&\dfrac{\partial^2f}{\partial x_1\partial x_n}\\\dfrac{\partial^2f}{\partial x_2\partial x_1}&\dfrac{\partial^2f}{\partial x_2^2}&...&\dfrac{\partial^2f}{\partial x_2\partial x_n}\\...&...&...&...\\\dfrac{\partial^2f}{\partial x_n\partial x_1}&\dfrac{\partial^2f}{\partial x_n\partial x_2}&...&\dfrac{\partial^2f}{\partial x_n^2}\end{pmatrix}$

$\quad{\bf{H}}_{i,j}=\dfrac{\partial^2f}{\partial x_i\partial x_j}$

> 高维情况下的牛顿法迭代公式：
>
> $\qquad x_{n+1}=x_n-[{\bf{H}}f(\vec{x})]^{-1}\nabla f(x_n),n\ge{0}$

<div STYLE="page-break-after: always;"></div>

##二、线性回归(Linear Regression)

###1. 模型表示

​	假设函数

​		$ h_θ(x)=θ_0+θ_1x\qquad\qquad\qquad\qquad\qquad$`单个特征`

​		$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\quad\ $`多个特征`


![线性回归](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\线性回归.jpg)

###2. 代价函数(Cost function)

   $J(\theta_0,\theta_1)=\dfrac{1}{2m}\sum_{i=1}\limits^n(h_\theta(x^{(i)})^2-y^{(i)})^2\quad$`最小二乘代价函数`

   + [为什么线性回归使用最小二乘代价函数$J(\theta_0,\theta_1)?$](http://blog.csdn.net/Eric2016_Lv/article/details/52836182?locationNum=3&fps=1)

###3. 最小二乘法(LSE)

   $\hat{y}=\hat{a}+\hat{b}x$

   $\hat{b}=\dfrac{\sum_{i=1}\limits^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}\limits^n(x_i-\bar{x})^2}=\dfrac{\sum_{i=1}\limits^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}\limits^nx_i^2-n\bar{x}^2}$

   $\hat{a}=\bar{y}-\hat{b}\bar{x}$

   $其中\bar{x}、\bar{y}为x_i、y_i的均值$

   + [ 求回归直线方程的推导过程](http://blog.csdn.net/marsjohn/article/details/54911788)
   + [最小二乘法？为神马不是差的绝对值](http://blog.sciencenet.cn/blog-430956-621997.html)
   + `求解全局最优`
   + `数据量很大时，计算量大`

###4. 梯度下降(Gradient Descent)

   + 目标：$\mathop{minimize}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)$


   + `迭代法`

     + $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\quad$`α为学习率`

       $\mathop{\Longrightarrow}\limits^{线性回归}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_1:=\theta_1-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x^{(i)}}\end{array}\right.\quad$`同时更新θ₀和θ₁`

   + `每一步更新未知量，逐渐逼近局部最优解`

        ![梯度下降1](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\梯度下降1.png)

        ![梯度下降2](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\梯度下降2.png)

   + 如何确保梯度下降能正常工作

         + J(θ)应随着迭代不断减小
         + 使用较小的学习率*α*

   + 如何选取学习率*α*

         + 尝试：

              ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...

        + 取值范围

             $0<\alpha<2(|X^TX|^{-1})$

   + 分类

        + Batch Gradient Descent

          $\qquad$每一步都使用**整个**训练集的数据计算梯度

        + Stochastic Gradient Descent

          $\Downarrow\quad\ $每一步仅用训练集中的**单个**计算梯度

        + Mini-batch Gradient Descent

          $\qquad$每一步仅用训练集中的**一小部分**（32/64/128/256）计算梯度

          > The way to go is therefore to use some acceptable (but not necessarily optimal) values for the other hyper-parameters, and then trial a number of different mini-batch sizes, scaling α as above. Plot the validation accuracy versus time (as in, real elapsed time, not epoch!), and choose whichever mini-batch size gives you the most rapid improvement in performance. With the mini-batch size chosen you can then proceed to optimize the other hyper-parameters.

          + 适用于存在较多局部最优解的情况，能跳出局部最优解

          + 计算量更小，计算速度更快

          + [BGD与SGD的不同](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent)

          > The applicability of batch or stochastic gradient descent really depends on the error manifold expected.
          >
          > Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.
          >
          > Stochastic gradient descent (SGD) computes the gradient using a single sample. Most applications of SGD actually use a minibatch of several samples, for reasons that will be explained a bit later. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. Single samples are really noisy, while minibatches tend to average a little of the noise out. Thus, the amount of jerk is reduced when using minibatches. A good balance is struck when the minibatch size is small enough to avoid some of the poor local minima, but large enough that it doesn't avoid the global minima or better-performing local minima. (Incidently, this assumes that the best minima have a larger and deeper basin of attraction, and are therefore easier to fall into.)
          >
          > One benefit of SGD is that it's computationally a whole lot faster. Large datasets often can't be held in RAM, which makes vectorization much less efficient. Rather, each sample or batch of samples must be loaded, worked with, the results stored, and so on. Minibatch SGD, on the other hand, is usually intentionally made small enough to be computationally tractable.
          >
          > Usually, this computational advantage is leveraged by performing many more iterations of SGD, making many more steps than conventional batch gradient descent. This usually results in a model that is very close to that which would be found via batch gradient descent, or better.
          >
          > The way I like to think of how SGD works is to imagine that I have one point that represents my input distribution. My model is attempting to learn that input distribution. Surrounding the input distribution is a shaded area that represents the input distributions of all of the possible minibatches I could sample. It's usually a fair assumption that the minibatch input distributions are close in proximity to the true input distribution. Batch gradient descent, at all steps, takes the steepest route to reach the true input distribution. SGD, on the other hand, chooses a random point within the shaded area, and takes the steepest route towards this point. At each iteration, though, it chooses a new point. The average of all of these steps will approximate the true input distribution, usually quite well.

###5. 向量化
|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | ---------------------------------------- |
|     feature      | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|      output      | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}$ |
|        θ         | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
|    hypothesis    | $h=X\theta=\begin{pmatrix}  \sum\limits_{i=0}^n\theta_ix_i^{(1)}\\ \sum\limits_{i=0}^n\theta_ix_i^{(2)}\\...\\ \sum\limits_{i=0}^n\theta_ix_i^{(m)} \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |
|  cost function   | $J=\dfrac{1}{2m}(h-y)^T(h-y)$            |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |
###6. 特征缩放(Feature Scaling) 

+ 确保所有特征在相似的规模上（$-1\leq{x_i}\leq{1}$）

+ 提升梯度下降速度

  ![特征缩放](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\特征缩放.png)

+ 方法

  + **mean normalization**

    $x_i'=\dfrac{x_i-\mu_i}{max(x)-min(x)}$

    $其中\mu_i为x的均值$

  + **Standardization**

    $x_i'=\dfrac{x-\bar{x}}{\sigma}$

    $其中\sigma为x的标准差$
###6. 正规方程(Normal Equation) 

+ 公式

  $\theta=(X^TX)^{-1}X^Ty$

  推导过程

+ 直接求解参数*θ*的最优值，不需要多次迭代 $\quad$`解析方法`

+ 不需要特征缩放

+ 不需要选择学习率*α*



> `问题`
>
> A.  特征数很大时计算量大（n≥10000) $\quad$`求解逆矩阵的时间复杂度O(n³)`
>
> B.  只适用于线性模型
>
> C.  $X^TX$可能不可逆
>
> + 冗余特征：存在线性相关的特征
>
> + 太多特征：特征数>样本数，导致过拟合
>
>   $\mathop{\Longrightarrow}\limits^{处理方法}\left\{\begin{array}{lcl}删除部分特征\left\{\begin{array}{lcl}手动选择\\模型选择算法\end{array}\right.\\正则化(\mbox{Regularization})\end{array}\right.$

### 7. 正则化(Normalization)

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=\dfrac{1}{2m}[\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2{\color{salmon}+\lambda\sum\limits_{j=1}^n\theta_j^2}]$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

  $\qquad\qquad\qquad\qquad\qquad\qquad\Downarrow$

  $\qquad\qquad\theta_j:=\theta_j{\color{salmon}(1-\alpha\dfrac{\lambda}{m})}-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}$$\qquad\qquad\qquad\qquad\qquad\scriptsize\color{maroon}\boxed{[注]\ 1-\alpha\dfrac{\lambda}{m}<1}$

  ​

  `正规方程`

  $\qquad\qquad\theta=\Bigg[X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}_{(n+1)\times{(n+1)}}\Bigg]^{-1}X^Ty$

  ​

  $\qquad\qquad\color{maroon}[注]\ 当\lambda>0时，可以证明X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}是可逆的$

  > 建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍（增减10倍是粗调节，当确定了λ的合适的数量级后，比如λ = 0.01,再进一步地细调节，比如调节为0.02，0.03，0.009之类。）

### 8. 牛顿法

+ ​

### 9. 最速下降法

+ ​

###10. 局部加权线性回归    `非参数学习方法` 

​	Ⅰ $\quad\ $Fit *θ* to minimize $\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$

​	Ⅱ $\quad$Output $\theta^Tx$

$w^{(i)}=e^{-\dfrac{(x^{(i)}-x)^2}{2\tau^2}}$，其中τ为常数，可调节$w^{(i)}$钟形曲线的宽度

+ 当$|x^{(i)}-x|\approx{0}$时，$w^{(i)}\approx{1}$，即预测样本举例训练样本越近，权值越大
+ 当$|x^{(i)}-x|\approx{+\infty}​$时，$w^{(i)}\approx{0}​$，即预测样本举例训练样本越远，权值越小

> `问题`
>
> A.  当数据规模比较大时，计算量很大，学习效率很低
>
> B.  不一定能避免欠拟合

<div STYLE="page-break-after: always;"></div>


##三、逻辑回归(Logistic Regression)

###1. 模型表示

> `分类`
>
> $\quad y=0\ \mbox{or} \ 1$
>
> $\quad h_\theta(x)\ \mbox{can be}\ >1\ \mbox{or}\ <0$
>
> $\color{brown}\Downarrow逻辑函数$
>
> `逻辑回归`
>
> $\quad0\leq{h_\theta(x)}\leq{1}\quad$

假设函数$h_\theta(x)=g(\theta^Tx)=\dfrac{1}{1+e^{-\theta^Tx}}\quad$`在线性回归模型的基础上加了一个Sigmoid函数 `

其中$g(z)=\dfrac{1}{1+e^{-z}}\quad$`sigmoid函数（S型曲线）`

​        $g'(z)=\dfrac{e^{(-z)}}{(1+e^{(-z)})^2}=\dfrac{1}{1+e^{(-z)}}(1-\dfrac{1}{1+e^{(-z)}})=g(z)(1-g(z))$

###2. 代价函数(Cost function)

> 线性回归的代价函数在逻辑回归模型中是非凸函数('"non-convex"')，不容易收敛到全局最优点，故引入新的代价函数

$J(\theta)=\dfrac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$

$\begin{equation}\begin{aligned}Cost(h_\theta(x^{(i)}),y^{(i)})&=\left\{\begin{array}{lcl}-log(h_\theta(x^{(i)})),\ \ \ \ \ \ \ \ \ y^{(i)}=1\\-log(1-h_\theta(x^{(i)})),\ \ y^{(i)}=0\end{array}\right.\\ &\color{maroon}=-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))\end{aligned}\end{equation}$

###3. 梯度下降(Gradient Descent) 

+ 目标：$\mathop{minimize}\limits_{\theta}J(\theta)$

+ 公式

  $\begin{equation}\begin{aligned}\theta_j&:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)\\ &:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot{x_j^{(i)}}\end{aligned}\end{equation}$

+ 优化算法

  `不需要手动选择学习率α`

  `通常比梯度下降更快`

  `但算法更复杂`

  + Conjugate  gradient 
  + BFGS 
  + L-­BFGS 

###4. 向量化

|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | :--------------------------------------- |
|     feature      | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|      output      | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}\quad$`每一项取值0或1` |
|        θ         | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
|    hypothesis    | $h=g(X\theta)=\begin{pmatrix}  g(\sum\limits_{i=0}^n\theta_ix_i^{(1)})\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(2)})\\...\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(m)}) \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |
|  cost function   | $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]$ |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |

###5. 正则化(Normalization)

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]{\color{salmon}+\dfrac{\lambda}{2m}\sum\limits_{j=1}^n\theta_j^2}$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

+ 向量化

  $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]+\dfrac{\lambda}{2m}(\theta^T\theta-\theta_0^2)$

  $\theta:=\theta-\dfrac{\alpha}{m}[X^T(h-y)+\lambda\theta]\quad\color{salmon}(\theta_0无正则项)$


####* 范数(norm)

- L0范数：向量中非0的元素的个数
  - L0范数可以实现稀疏
  - $\Vert\vec{x}\Vert_0$

- L1范数：向量中各个元素绝对值之和$\quad$`稀疏规则算子`

  - L1范数会使权值稀疏
  - $\Vert\vec{x}\Vert_1=\sum\lim_{i=1}^n|x_i|\quad$
  - $d_1(I_1,I_2)=\sum\limits_p|I_1^p-I_2^p|\quad$`L1距离/曼哈顿距离`

- L2范数：向量各元素的平方和然后求平方根$\quad$`权值衰减`

  - L2范数可以防止过拟合，提升模型的泛化能力
  - $\Vert\vec{x}\Vert_2=\sqrt{x_1^2+x_2^2+...+x_n^2}\quad$
  - $d_2(I_1,I_2)=\sqrt{\sum\limits_p(I_1^p-I_2^p)^2}\quad$`L2距离/欧氏距离`

  ​

  > L1正则化：$\lambda\Vert\theta\Vert\quad\ \ $`套索回归Lasso`
  >
  > L2正则化：$\lambda\Vert\theta\Vert^2\quad$`岭回归Ridge`

###6. 多元分类

> $y\in{0,1,2,...,n}$
>
> $\left\{\begin{array}{lcl}h_\theta^{(0)}(x)=P(y=0|x;\theta)\\h_\theta^{(1)}(x)=P(y=1|x;\theta)\\...\\h_\theta^{(n)}(x)=P(y=n|x;\theta)\end{array}\right.$

$\mbox{prediction}=\mbox{max}(h_\theta^{(i)}(x))$

<div STYLE="page-break-after: always;"></div>

##四、神经网络

###1. 模型表示 

+ $L=网络总层数=输入层+隐藏层+输出层$

+ $s_l=第l层中的单元数（不包含偏置单元）$

+ $K=输出单元个数/类别数\quad\mathop{\Longrightarrow}\limits^{二分类}\quad K=1$

+ $(h_\Theta(x^{(i)}))_k=第k个输出$

+ $a^{(j)}=第j层的激励(\mbox{activation})\ \mathop{\Longrightarrow}\limits^{维度}\ (s_j+1)\times{1}$

  $a_i^{(j)}=第j层第i个单元的激励$

+ $\Theta^{(j)}=从第j层映射到第j+1层的权重矩阵\ \mathop{\Longrightarrow}\limits^{维度}\ s_{j+1}\times{(s_j+1)}$

$\quad\ \ \ \Theta_{kn}^{(j)}=从第j层映射到第j+1层第k个单元的第n个参数(权重)$

![神经网络](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络.png)



####`网络架构`

+ 输入层→隐藏层→输出层

![神经网络层次](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络层次.jpeg)

###2. 非线性分类：逻辑运算

`激活函数取sigmoid函数`

+ or运算

  $h_\Theta(x)=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![or](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\or.png)

+ and运算

  $h_\Theta(x)=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![and](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\and.png)

+ not运算

  $h_\Theta(x)=g(10-20x)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x=0\\\approx0&x=1\end{array}\right.$

  ![not](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\not.png)

+ xor运算

  $h_\Theta(x)=a_1^{(3)}=g(-30+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(30-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![xor](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\xor.png)

+ xnor运算

  $h_\Theta(x)=a_1^{(3)}=g(-10+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(10-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  ![xnor](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\xnor.png)

###3. 分类

#### · 二分类

$\qquad y=0\ or\ 1$

#####		· `感知机(perception)`

$\qquad$假设空间是特征空间中的所有线性分类模型，即找到一个线性方程$x\cdot{w}+b=0$，它对应于特征空间的一个

$\qquad$超平面，能够把对应的特征空间分为两部分，位于两部分中的点分别是正负两类。

####· 多元分类

$\qquad y^{(i)}\in one\ of\begin{pmatrix}1\\0\\0\\0\end{pmatrix},\begin{pmatrix}0\\1\\0\\0\end{pmatrix},\begin{pmatrix}0\\0\\1\\0\end{pmatrix},\begin{pmatrix}0\\0\\0\\1\end{pmatrix}$

![多元分类](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\多元分类.png)

###4. 代价函数(Cost function)

`逻辑回归中代价函数的一般形式`

$J(\Theta)=-\dfrac{1}{m}[\sum\limits_{i=1}^m\sum\limits_{k=1}^Ky_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-h_\Theta(x^{(i)}))_k]+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{j+1}}(\Theta_{ji}^{(l)})^2$

### 5. 反向传播算法(Backpropagation algorithm)

`梯度下降`$\quad$`迭代法`

![反向传播](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\反向传播.png)

+ Ⅰ$\quad\ $前向传播(feedforward)$\quad$`计算各层输出`

  ![神经网络](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络.jpeg)

  > $a^{(1)}=x$
  >
  > $z^{(2)}=\Theta^{(1)}a^{(1)}$
  >
  > $a^{(2)}=g(z^{(2)})\quad(\mbox{add}\ a_0^{(2)})$
  >
  > $z^{(3)}=\Theta^{(2)}a^{(2)}$
  >
  > $a^{(3)}=g(z^{(3)})\quad(\mbox{add}\ a_0^{(3)})$
  >
  > $z^{(4)}=\Theta^{(3)}a^{(3)}$
  >
  > $a^{(4)}=h_\Theta(x)=g(z^{(4)})$
  >
  > $\dfrac{\partial a^{(l)}}{\partial z^{(l)}}=\begin{pmatrix}\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_1}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_1}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_1}\\\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_2}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_2}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_2}\\...&...&...&...\\\dfrac{\partial a^{(l)}_0}{\partial z^{(l)}_{s_l}}&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_{s_l}}&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_{s_l}}\end{pmatrix}=\begin{pmatrix}0&\dfrac{\partial a^{(l)}_1}{\partial z^{(l)}_1}&0&...&0\\0&0&\dfrac{\partial a^{(l)}_2}{\partial z^{(l)}_2}&...&0\\...&...&...&...&...\\0&0&0&...&\dfrac{\partial a^{(l)}_{s_l}}{\partial z^{(l)}_{s_l}}\end{pmatrix}_{s_l\times{(s_l+1)}}\ $`分母布局`

+ Ⅱ$\quad$反向传播$\quad$`误差逆传播`

  > 若输入层的实际输出$h_\Theta(x)$与期望的输出$y$不符，则进行误差的反向传播
  >
  > 直到网络输出的误差减少到了可以接受的程度（或 进行到预先设定的学习次数为止）

  `偏置单元可以计入δ，也可不计入`

由梯度下降公式$\theta:=\theta-\alpha\dfrac{\partial}{\partial\theta}J(\theta)$联想到神经网络的迭代：

$\dfrac{\partial J(\Theta)}{\partial \Theta^{(l)}}=\nabla_{\Theta^{(l)}}J(\Theta)=\dfrac{\partial J(\Theta)}{\partial z^{(l+1)}}\dfrac{\partial z^{(l+1)}}{\partial \Theta^{(l)}}=\delta^{(l+1)}(a^{(l)})^T$

$\delta^{(l)}=\dfrac{\partial J}{\partial z^{(l)}}=\dfrac{\partial a^{(l)}}{\partial z^{(l)}}\dfrac{\partial z^{(l+1)}}{\partial a^{(l)}}\dfrac{\partial J}{\partial z^{(l+1)}}=g'(z^{(l)})\times(\Theta^{(l)})^T\delta^{(l+1)}=(\Theta^{(l)})^T\delta^{(l+1)}\ .*\ {a^{(l)}(1-a^{(l)})}\quad$`分母布局`

$\delta_j^{(l)}=第l层第j个单元的{\color{blueviolet}\textbf{误差}}=\left\{\begin{array}{lcl}a_j^{(l)}-y_j=(h_\Theta(x))_j-y_j&\color{salmon}输出层&(l=L)\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i1}^{(l)}\delta_i^{(l+1)}\cdot{a_j^{(l)}}\cdot{(1-a_j^{(l)})}&\color{salmon}隐藏层&(1<l<L)\end{array}\right.\quad$

$\quad\\\qquad\mathop{\Longrightarrow}\limits^{忽略正则项\lambda}\ \boxed{\quad\dfrac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}\quad}$

$\qquad\qquad\qquad\dfrac{\partial}{\partial\Theta^{(l)}}J(\Theta)=\begin{pmatrix}\delta_1^{(l+1)}a_0^{(l)}&\delta_1^{(l+1)}a_1^{(l)}&...&\delta_1^{(l+1)}a_{s_l}^{(l)}\\\delta_2^{(l+1)}a_0^{(l)}&\delta_2^{(l+1)}a_1^{(l)}&...&\delta_2^{(l+1)}a_{s_l}^{(l)}\\...&...&...&...\\\delta_{s_{l+1}}^{(l+1)}a_0^{(l)}&\delta_{s_{l+1}}^{(l+1)}a_1^{(l)}&...&\delta_{s_{l+1}}^{(l+1)}a_{s_l}^{(l)}\end{pmatrix}_{s_{l+1}\times{(s_l+1)}}$



#### · 步骤

1. Set $\ \triangle_{ij}^{(l)}=0\ $(for all $i,j,l$)$\quad$`误差矩阵`

2. For $i$=1 to $m$

   $\quad$Set $\ a^{(1)}=x^{(i)}$

   $\quad$执行前向传播算法计算$a^{(l)},\ l=2,3,...,L$

   $\quad$计算$\delta^{(L)}=a^{(L)}-y^{(i)}$

   $\quad$计算$\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}$

   $\quad \triangle_{ij}^{(l)}:=\triangle_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}\quad\pmatrix{\ \ \triangle^{(l)}:=\triangle^{(l)}+\delta^{(l+1)}(a^{(l)})^T\ \ }$

3. 计算$J(\Theta)​$的偏导数

   $\left\{\begin{array}{lcl}D^{(l)}_{ij}:=\dfrac{1}{m}\triangle_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}&\mbox{if}\ j\neq{0}\\\ D^{(l)}_{ij}:=\dfrac{1}{m}\triangle_{ij}^{(l)}&\mbox{if}\ j=0\end{array}\right.$

   > $\dfrac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$

4. 迭代更新权重矩阵

   $\Theta^{(l)}:=\Theta^{(l)}-\alpha D^{(l)}$

###6. 向量化

|   变量    | 向量表示（$其中m为训练样本数，K为类别数，L为网络层数，s_l为第l层单元数$） |
| :-----: | :--------------------------------------- |
| feature | $x^{(i)}=\begin{pmatrix} x_1^{(i)}\\ x_2^{(i)}\\...\\ x_{s_L}^{(i)}\end{pmatrix}_{s_L\times{1}}\quad(1\le i\le m)$ |

<div STYLE="page-break-after: always;"></div>

|        变量        | 向量表示（$其中m为训练样本数，K为类别数，L为网络层数，s_l为第l层单元数$） |
| :--------------: | :--------------------------------------- |
|      output      | $y^{(i)}=\begin{pmatrix} y_1^{(i)}\\ y_2^{(i)}\\...\\ y_{s_L}^{(i)}\end{pmatrix}_{s_L\times{1}}\quad(1\le i\le m)\quad$`每一项取值0或1` |
|  $\Theta^{(l)}$  | $\Theta^{(l)}=\begin{pmatrix}  \theta_{10}^{(l)}&\theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_l}^{(l)}\\\theta_{20}^{(l)}&\theta_{21}^{(1)}&\theta_{22}^{(l)}&...&\theta_{2s_l}^{(l)}\\\theta_{30}^{(l)}&\theta_{31}^{(l)}&\theta_{32}^{(l)}&...&\theta_{3s_l}^{(l)}\\...&...&...&...&...\\ \theta_{s_{l+1}0}^{(l)}&\theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_l}^{(l)} \end{pmatrix}_{s_{l+1}\times{(s_l+1)}}=\begin{pmatrix}(\vec\theta_1^{(l)})^T\\(\vec\theta_2^{(l)})^T\\(\vec\theta_3^{(l)})^T\\...\\(\vec\theta_{s_{l+1}}^{(l)})^T\end{pmatrix}$ |
|    activation    | $a^{(1)}=x\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \color{salmon}(l=1)\\\quad\\a^{(l)}=g(\Theta^{(l-1)}a^{(l-1)})=\begin{pmatrix}1\\g(\sum\limits_{k=0}^K\Theta_{1k}^{(l-1)}a_k^{(l-1)})\\g(\sum\limits_{k=0}^K\Theta_{2k}^{(l-1)}a_k^{(l-1)})\\...\\g(\sum\limits_{k=0}^K\Theta_{s_lk}^{(l-1)}a_k^{(l-1)})\end{pmatrix}_{(s_l+1)\times{1}}=\begin{pmatrix}a_0^{(l)}\\a_1^{(l)}\\a_2^{(l)}\\...\\a_{s_l}^{(l)}\end{pmatrix}\qquad \color{salmon}(2\le l\le L)$ |
|      error       | $\delta^{(L)}=a^{(L)}-y=h_\Theta(x)-y\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ \color{salmon}(l=L)\\\quad\\ \begin{equation}\begin{aligned}\delta^{(l)}&=((\Theta^{(l)})^T \delta^{(l+1)})\ .*g'(z^{(l)})=((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\\&=\begin{pmatrix}\sum\limits_{i=1}^{s_{l+1}}\Theta_{i1}^{(l)}\delta_i^{(l+1)}\cdot{a_1^{(l)}}\cdot{(1-a_1^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i2}^{(l)}\delta_i^{(l+1)}\cdot{a_2^{(l)}}\cdot{(1-a_2^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i3}^{(l)}\delta_i^{(l+1)}\cdot{a_3^{(l)}}\cdot{(1-a_3^{(l)})}\\...\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{is_l}^{(l)}\delta_i^{(l+1)}\cdot{a_{s_l}}^{(l)}\cdot{(1-a_{s_l}^{(l)})}\end{pmatrix}_{s_l\times{1}}\end{aligned}\end{equation}\color{salmon}\qquad(2\le l\le L-1)\\\color{blueviolet}(未计入偏置单元，故公式中\Theta^{(l)}的维度应为s_{l+1}\times{s_l}，a^{(l)}的维度应为s_l\times{1})$ |
| cost function(例) | $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]$ |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |

### 7. 梯度检验

 + 梯度的数值检验(Numerical Gradient Checking)

   ![GradientCheck](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\GradientCheck.png)

>   通过估计梯度值来检验我们计算的导数值是否真的是我们要求的
>
>   + 在代价函数上沿着切线的方向选择离两个非常近的点
>   + 然后计算两个点的平均值用以估计梯度
>   + 检查估算的梯度值与反向传播算法得到的梯度DVec（将$D^{(1)},D^{(2)},...$展开为向量）是否接近
>   + **关掉梯度检验**，使用反向传播进行学习$\quad$`梯度检验速度很慢`

$\quad\dfrac{\partial}{\partial\theta_i}J(\theta)\approx\dfrac{J(\theta_1,\theta_2,...,\theta_i+\epsilon,...\theta_n)-J(\theta_1,\theta_2,...,\theta_i-\epsilon,...\theta_n)}{2\epsilon}\qquad\color{salmon}(\epsilon是很小的正数)$

```python
for i = 1 : n,
    thetaPlus = theta;
    thetaPlus(i) = thetaPlus(i) + EPSILON;
    thetaMinus = theta;
    thetaMinus(i) = thetaMinus(i) – EPSILON;
    gradApprox(i) = (J(thetaPlus) – J(thetaMinus))/(2*EPSILON);
end;
```

+ 例题

![GradientCheckQuestion](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\GradientCheckQuestion.png)

### 8. 随机初始化

+ 如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值
+ 同理，如果我们初始所有的参数都为同一个非 0 的数，结果也是一样的

> 我们通常初始参数为正负$\epsilon$之间的**随机值**
>
> 如果权重矩阵 $\Theta^{(l)}$ 初始化过大，矩阵相乘的输出范围会非常大（例如 -400 到 400 之间的数值），这会使得向量$a^{(l)}$上的所有输出几乎是二元的：0 或 1。但如果是这样，S 型非线性函数的局部梯度 $a^{(l)}.*(1-a^{(l)})$ 在两种情况下都会是 0（梯度消失），使得$x$和$\Theta^{(l)}$的梯度都是 0。在链式反应下，之后的反向传播相乘之后得到的都会是 0。

假设我们要随机初始一个尺寸为 10×11 的参数矩阵，代码如下：

```python
Theta1 = rand(10, 11) * (2*eps) – eps
```



###9. 训练神经网络的步骤

1. 选择一个网络架构$\quad$`神经元之间的连接样式`

   + 输入单元的数量：特征$x^{(i)}$的维度

   + 输出单元的数量：分类数

     > 合理的默认选择：1个隐藏层，或若有多个隐藏层，则在每一层中有相同数量的隐藏单元
     >
     > （通常情况下隐藏层单元的个数越多越好）

2. 训练过程

   > 1. 随机初始化权重(参数)
   >
   > 2. 利用前向传播方法计算所有输入$x^{(i)}$的$h_\Theta(x^{(i)})$
   >
   > 3. 编写计算代价函数$J(\Theta)$ 的代码
   >
   > 4. 利用反向传播方法计算偏导数$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$
   >
   > 5. 利用数值检验方法比较用反向传播计算的和用$J(\Theta)$的梯度的数值估计得到的$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$
   >
   >    `然后禁用梯度检查的代码，因为梯度检查速度很慢`
   >
   > 6. 使用梯度下降或优化算法与反向传播算法相结合，来使$J(\Theta)$最小化
   >

   `J(Θ)是非凸函数，因此梯度下降可能得到一个局部最优点`

###* Geoffrey Hinton关于BP的看法



<div STYLE="page-break-after: always;"></div>

## 五、应用机器学习的建议

###1. 交叉验证(Cross Validation) 

> $J(\theta)=\dfrac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\dfrac{\lambda}{2m}\sum\limits_{j=1}^m\theta_j^2$
>
> 使用 60% 的数据作为训练集$\qquad\qquad J_{train}(\theta)=\dfrac{1}{2m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$
>
> 使用 20% 的数据作为交叉验证集$\qquad\ J_{cv}(\theta)=\dfrac{1}{2m_{cv}}\sum\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2$
>
> 使用 20% 的数据作为测试集$\qquad\qquad J_{test}(\theta)=\dfrac{1}{2m_{test}}\sum\limits_{i=1}^{m_{test}}(h_\theta(x^{(i)}_{test})-y^{(i)}_{test})^2$

###2. 模型选择

> 1.  用训练集训练出 n 个模型（对应不同的多项式次数）
> 2.  对每个模型用训练集最优化$\Theta$中的参数
> 3.  分别计算 n 个模型在交叉验证集上的误差，找出误差最小的模型对应的多项式次数d
> 4.  用测试集和$J_{test}(\Theta^{(d)})$计算泛化误差(generalization error)；多项式次数d没有用测试集进行训练

![CrossValidationQuestion](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\CrossValidationQuestion.png)

###3. 偏差(Bias)与方差(Variance)

$\mbox{Error}=\mbox{Bias}^²+\mbox{Variance}+\mbox{Noise}^²$

$\quad\mbox{Bias}[\hat{f}(x)]=\mbox{E}[\hat{f}(x)-f(x)]$

$\quad\mbox{Var}[\hat{f}(x)]=\mbox{E}[\hat{f}(x)^2]-\mbox{E}[f(x)]^2$

$\quad\sigma^2:$不可消除的误差（噪声$\epsilon$），假设均值为0，方差为$\sigma$

> 由于$f(x)$是确定性的，故$\mbox{E}f(x)=f$
>
> 给定$y=f+\epsilon$，其中$\mbox{E}[\epsilon]=0$，则$\mbox{E}[y]=\mbox{E}[f+\epsilon]=\mbox{E}[f]=f$
>
> $\mbox{Var}[y]=\mbox{E}[(y-\mbox{E}[f])^2]=\mbox{E}[(y-f)^2]=\mbox{E}[(f+\epsilon-f)^2]=\mbox{E}[\epsilon^2]=\mbox{Var}[\epsilon]+\mbox{E}[\epsilon]^2=\sigma^2$
>
> $\begin{equation}\begin{aligned}\mbox{E}[(y-\hat{f})^2]&=\mbox{E}[y^2+\hat{f}^2-2y\hat{f}]\\&=\mbox{E}[y^2]+\mbox{E}[\hat{f}^2]-\mbox{E}[2y\hat{f}]\\&=\mbox{Var}[y]+\mbox{E}[y]^2+\mbox{Var}[\hat{f}]+\mbox{E}[\hat{f}]^2-2d\mbox{E}[\hat{f}]\\&=\mbox{Var}[y]+\mbox{Var}[\hat{f}]+(f^2-2f\mbox{E}[\hat{f}]+\mbox{E}[\hat{f}]^2)\\&=\mbox{Var}[y]+\mbox{Var}[\hat{f}]+(f-\mbox{E}[\hat{f}])^2\\&=\sigma^2+\mbox{Var}[\hat{f}]+\mbox{Bias}[\hat{f}]^2\end{aligned}\end{equation}$

![bias&variance](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\bias&variance.png)$\quad\begin{array}\\\mbox{Bias (underfit):}\\\quad \color{maroon}J_{train}(\theta)\mbox{will be high}\\\quad \color{maroon}J_{cv}(\theta)\approx J_{train}(\theta)\\\quad\\\mbox{Variance (overfit):}\\\quad \color{maroon}J_{train}(\theta)\mbox{will be low}\\\quad \color{maroon}J_{cv}(\theta)\gg J_{train}(\theta)\end{array}$

![bias&variance&lambda](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\bias&variance&lambda.png)$\qquad\qquad\quad$**正则化参数**$\lambda\Rightarrow\left\{\begin{array}{lcl}\lambda过大会导致欠拟合\\\quad\\\lambda偏小时可能出现过拟合\end{array}\right.$

偏差-方差权衡(Bias–variance tradeoff)$\quad$`有监督学习的核心问题`

+ 同时使偏差和方差最小

+ 高偏差会导致算法丢失特征与目标输出的相关关系$\quad$`欠拟合`

  + 增加训练数据帮助不大

    $\quad$![getMoreTrainingData](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\getMoreTrainingDataForBias.png)

+ 高方差会导致算法对训练集中的随机噪声建模，而不是预设的输出$\quad$`过拟合`

  + 增加训练数据可能会有帮助

    $\quad$![getMoreTrainingDataForVar](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\getMoreTrainingDataForVar.png)

###4. 学习曲线

对学习算法的一个**合理检验**

![learningCurve](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\learningCurveBias.png)![learningCurve](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\learningCurveVar.png)

###5. 调试一个学习算法

假设实现正则化线性回归时，发现预测结果的误差很大，接下来应该怎么尝试？

+ 扩大训练集$\qquad\qquad\qquad\qquad\qquad\qquad\qquad$(解决**高方差**问题)
+ 尝试更小的特征集$\qquad\qquad\qquad\qquad\qquad\quad\ $(解决**高方差**问题)
+ 尝试增加额外特征$\qquad\qquad\qquad\qquad\qquad\quad\ $(解决**高偏差**问题)
+ 尝试增加多项式特征（$x_1^2,x_2^2,x_1x_2$等）$\qquad\ \ \ $(解决**高偏差**问题)
+ 尝试减小λ$\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ $(解决**高偏差**问题)
+ 尝试增大λ$\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ $(解决**高方差**问题)

<div STYLE="page-break-after: always;"></div>

###6. 机器学习系统设计

> 1. 从一个简单、能快速实现的算法开始实现，并用交叉验证集的数据测试它
>
> 2. 画出学习曲线来决定是否使用更多数据、更多特征会有帮助
>
> 3. 误差分析：手动检查算法在**交叉验证集**中出现误差的样例，观察其出现误差的数据样例类型是否呈现出任何系统趋势
>
>    *Error analysis may not be helpful for deciding if this is likely to improve performance.*
>
>    *Only solution is to try it and see if it works.*

![ErrorAnalysisQuestion](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\ErrorAnalysisQuestion.png)

####· 偏斜类(skewed classes)的误差度量

>    `训练集中有非常多的同一种类的样本数据，只有很少或没有其他类的样本数据(导致误差增大)`
>
>    + 准确率(Accuracy)
>
>      $\mbox{Accuracy}=\dfrac{\mbox{True positive}+\mbox{True negative}}{\bf\mbox{all examples}}$
>
>
>    + 精确率(Precision)与召回率(Recall)
>
>      + 精确率：对于给定的测试数据集，预测正确的(正类)样本数与所有被预测的(正类)样本数之比
>
>        $\mbox{Precision}=\dfrac{\mbox{True positive}}{\bf\mbox{predicted as positive}}=\dfrac{\color{blue}\mbox{True positive}}{{\color{blue}\mbox{True positive}}+{\color{red}\mbox{False positive}}}$
>
>        $(P=\dfrac{TP}{TP+FP})$
>
>      + 召回率：对于给定的测试数据集，预测正确的(正类)样本数与所有应该被正确分类(正类)的样本数之比
>
>        $\mbox{Recall}=\dfrac{\mbox{True positive}}{\bf\mbox{actual positive}}=\dfrac{\color{blue}\mbox{True positive}}{{\color{blue}\mbox{True positive}}+{\color{blueviolet}\mbox{False negative}}}$
>
>        $(R=\dfrac{TP}{TP+FN})$
>
>        ![precision&recall](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\precision&recall.png)$\quad y=1$表示我们希望检测的样本中很少的那一类
>
>      + 权衡精确率与召回率
>
>        + 若希望只在非常确信的情况下预测为正类$\ \Longrightarrow\ \left\{\begin{array}{lcl}\mbox{Higher precision}\\\mbox{Lower recall}\end{array}\right.$
>
>        + 若希望提高查全率$\ \Longrightarrow\ \left\{\begin{array}{lcl}\mbox{Higher recall}\\\mbox{Lower precision}\end{array}\right.$
>
>        + 一般地，$\left\{\begin{array}{lcl}当h_\theta(x)\ge\mbox{threshold}时预测为1\\当h_\theta(x)\lt\mbox{threshold}时预测为0\end{array}\right.$
>
>          ![precision-recallCurve](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\precision-recallCurve.png)
>
>      + $F_1$分数
>
>        精确率和召回率的调和均值
>
>        $\dfrac{2}{F_1}=\dfrac{1}{P}+\dfrac{1}{R}$
>
>        即$F_1=\dfrac{2PR}{P+R}=\dfrac{2TP}{2TP+FP+FN}\qquad\Longrightarrow\ \left\{\begin{array}{lcl}P=0\ \mbox{or}\ R=0\ \color{maroon}\ \ \ \Rightarrow\ F=0\\P=1\ \mbox{and}\ R=1\ \color{maroon}\Rightarrow\ F=1\end{array}\right.$
>
>        `在交叉验证集上测得精确率P和召回率R，并选取使F₁最大的阈值`

#### · 使用大量数据集

> 假设使用一种需要大量参数的学习算法（有很多特征的逻辑回归/线性回归，有许多隐藏单元的神经网络），特征值有足够的信息来预测 y 值$\quad​$`低偏差`
>
> $J_{train}(\theta)\ $ will be small
>
> 使用很大的训练集（不太可能过拟合）`低方差`
>
> $\quad J_{train}(\theta)\approx J_{test}(\theta)$
>
> $\qquad\Longrightarrow J_{test}(\theta)\ $ will be small

<div STYLE="page-break-after: always;"></div>

##六、支持向量机(Support Vector Machine)

###1.  