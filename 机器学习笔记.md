$\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}^T\quad({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$![封面](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\封面.png)

<div STYLE="page-break-after: always;"></div>

#机器学习笔记

[TOC]

<div STYLE="page-break-after: always;"></div>

##一、线性代数相关

###1.矩阵的求导 

|               表达形式               |             公式$\quad$`分子布局`              |
| :------------------------------: | :--------------------------------------: |
| $\dfrac{\partial向量}{\partial标量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\qquad\dfrac{\partial \vec{y}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x}\\\dfrac{\partial y_2}{\partial x}\\...\\\dfrac{\partial y_m}{\partial x} \end{pmatrix}$ |
| $\dfrac{\partial标量}{\partial向量}$ | $\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\qquad\dfrac{\partial y}{\partial \vec{x}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_1}&\dfrac{\partial y}{\partial x_2}&...&\dfrac{\partial y}{\partial x_n} \end{pmatrix}$ |
|         标量函数关于空间向量的方向导数          | $\nabla_{\vec{u}}f(x)\mathop{=}\limits^{\triangle}\nabla f(x)\cdot{\vec{u}}=\dfrac{\partial f(x)}{\partial x}\cdot{\vec{u}}$ |
| $\dfrac{\partial向量}{\partial向量}$ | $\vec{y}=\begin{pmatrix} y_1\\y_2\\...\\y_m \end{pmatrix}\quad\vec{x}=\begin{pmatrix} x_1\\x_2\\...\\x_n \end{pmatrix}\\\dfrac{\partial \vec{y}}{\partial \vec{x}}=\begin{pmatrix}\dfrac{\partial y_1}{\partial \vec{x}}\\\dfrac{\partial y_2}{\partial \vec{x}}\\...\\\dfrac{\partial y_m}{\partial \vec{x}}\end{pmatrix}=\begin{pmatrix} \dfrac{\partial y_1}{\partial x_1}&\dfrac{\partial y_1}{\partial x_2}&...&\dfrac{\partial y_1}{\partial x_n}\\\dfrac{\partial y_2}{\partial x_1}&\dfrac{\partial y_2}{\partial x_2}&...&\dfrac{\partial y_2}{\partial x_n}\\...&...&...&...\\\dfrac{\partial y_m}{\partial x_1}&\dfrac{\partial y_m}{\partial x_2}&...&\dfrac{\partial y_m}{\partial x_n} \end{pmatrix}_{m\times{n}}$ |
| $\dfrac{\partial矩阵}{\partial标量}$ | $\dfrac{\partial \bf{Y}_{m\times{n}}}{\partial x}=\begin{pmatrix} \dfrac{\partial y_{11}}{\partial x}&\dfrac{\partial y_{12}}{\partial x}&...&\dfrac{\partial y_{1n}}{\partial x}\\\dfrac{\partial y_{21}}{\partial x}&\dfrac{\partial y_{22}}{\partial x}&...&\dfrac{\partial y_{2n}}{\partial x}\\...&...&...&...\\\dfrac{\partial y_{m1}}{\partial x}&\dfrac{\partial y_{m2}}{\partial x}&...&\dfrac{\partial y_{mn}}{\partial x} \end{pmatrix}_{m\times{n}}$ |
| $\dfrac{\partial标量}{\partial矩阵}$ | $\dfrac{\partial y}{\partial \bf{X}_{p\times{q}}}=\begin{pmatrix} \dfrac{\partial y}{\partial x_{11}}&\dfrac{\partial y}{\partial x_{21}}&...&\dfrac{\partial y}{\partial x_{p1}}\\\dfrac{\partial y}{\partial x_{12}}&\dfrac{\partial y}{\partial x_{22}}&...&\dfrac{\partial y}{\partial x_{p2}}\\...&...&...&...\\\dfrac{\partial y}{\partial x_{1q}}&\dfrac{\partial y}{\partial x_{2q}}&...&\dfrac{\partial y}{\partial x_{pq}} \end{pmatrix}_{q\times{p}}\\\quad\\通常写作\nabla_{\bf{X}}y(\bf{X})=\dfrac{\partial y(\bf{X})}{\partial \bf{X}}$ |
|          标量函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}f=tr\pmatrix{\dfrac{\partial f}{\partial \bf{X}}\bf{Y}}$ |
| $\dfrac{\partial矩阵}{\partial矩阵}$ | $\dfrac{\partial \bf{F}_{p\times{q}}}{\partial \bf{X}_{n\times{m}}}=\begin{pmatrix} \dfrac{\partial \bf{F}}{\partial \bf{X_{1,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{1,n}}}\\\dfrac{\partial \bf{F}}{\partial \bf{X_{2,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{2,n}}}\\...&...&...&...\\\dfrac{\partial \bf{F}}{\partial \bf{X_{m,1}}}&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,2}}}&...&\dfrac{\partial \bf{F}}{\partial \bf{X_{m,n}}} \end{pmatrix}_{mp\times{nq}}\\\quad\\其中每一项\dfrac{\partial \bf{F}}{\partial \bf{X_{i,j}}}都是p\times{q}的矩阵$ |
|          矩阵函数关于矩阵的方向导数           | $\nabla_{\bf{Y}}\bf{F}=tr\pmatrix{\dfrac{\partial \bf{F}}{\partial \bf{X}}\bf{Y}}$ |

#### · 通用公式

$\quad(\bf{X}、\bf{Y}均为矩阵)$

> $\dfrac{\partial \bf{X}}{\partial \bf{X}}={\bf{I}}$
>
> $\dfrac{\partial a{\bf{Y}}}{\partial \bf{X}}=a\dfrac{\partial {\bf{Y}}}{\partial \bf{X}}\quad(a不是\bf{X}的函数)$
>
> 
>
> $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=({\bf{A}}+{\bf{A}}^T)\vec{x}\quad({\bf{A}}不是\vec{x}的函数，\vec{x}^T{\bf{A}}\vec{x}为标量)$
>
> $\dfrac{\partial^2 \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}^2}={\bf{A}}+{\bf{A}}^T\quad({\bf{A}}不是\vec{x}的函数，\vec{x}^T{\bf{A}}\vec{x}为标量)$
>
> 
>
> $\dfrac{\partial\vec{u}^T}{\partial x}=\pmatrix{\dfrac{\partial \vec{u}}{\partial x}}^T$

`分子布局`

$\qquad$根据$\bf Y$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad$根据${\bf X}^T$来布局$\dfrac{\partial y}{\partial\bf{X}}$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为列向量，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为行向量

`分母布局`

$\qquad$根据${\bf Y}^T$来布局$\dfrac{\partial \bf{Y}}{\partial x}$

$\qquad$根据$\bf{X}$来布局$\dfrac{\partial y}{\partial\bf{X}}$

$\qquad$布局$\dfrac{\partial\bf{y}}{\partial\bf{x}}$时，将$\dfrac{\partial{y_i}}{\partial\bf{x}}$作为列向量，将$\dfrac{\partial\bf{y}}{\partial{x_j}}$作为行向量

|          分子布局(Numerator Layout)          |         分母布局(Denominator Layout)         |
| :--------------------------------------: | :--------------------------------------: |
| $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ m\times{n}$ | $\dfrac{\partial\vec{y}_{m\times{1}}}{\partial\vec{x}_{n\times{1}}}\ \mathop{\Longrightarrow}\limits^{维度}\ n\times{m}$ |
| $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{x}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}^T\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{x}^T\bf{A}}{\partial\vec{x}}={\bf{A}}\quad\color{blueviolet}({\bf{A}}_{n\times{m}}不是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\vec{u}\dfrac{\partial a}{\partial\vec{x}}$ | $\dfrac{\partial a(x)\vec{u}(x)}{\partial\vec{x}}=a\dfrac{\partial\vec{u}}{\partial\vec{x}}+\dfrac{\partial a}{\partial\vec{x}}\vec{u}^T$ |
| $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}={\bf{A}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}({\bf{A}}_{p\times{q}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial{\bf{A}}\vec{u}}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}{\bf{A}}^T$<br>$\color{blueviolet}({\bf{A}}_{m\times{n}}不是\vec{x}_{n\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}$<br>$\color{blueviolet}(\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{u}}{\partial\vec{x}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ | $\dfrac{\partial\vec{f}(\vec{g}(\vec{u}))}{\partial\vec{x}}=\dfrac{\partial\vec{u}}{\partial\vec{x}}\dfrac{\partial\vec{g}(\vec{u})}{\partial\vec{u}}\dfrac{\partial\vec{f}(\vec{g})}{\partial\vec{g}}$<br>$\color{blueviolet}(\vec{f}_{m\times{1}}是\vec{g}_{p\times{1}}的函数，\vec{g}_{p\times{1}}是\vec{u}_{q\times{1}}的函数，\\\color{blueviolet}\vec{u}_{q\times{1}}是\vec{x}_{n\times{1}}的函数)$ |
| $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\vec{u}^T\dfrac{\partial \vec{v}}{\partial \vec{x}}+\vec{v}^T\dfrac{\partial \vec{u}}{\partial \vec{x}}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ | $\dfrac{\partial\vec{u}^T{\vec{v}}}{\partial \vec{x}}=\dfrac{\partial \vec{v}}{\partial \vec{x}}\vec{u}+\dfrac{\partial \vec{u}}{\partial \vec{x}}\vec{v}\quad\color{blueviolet}(\vec{u}^T\vec{v}是标量)$ |
| $\dfrac{\partial \vec{x}^T{\bf{A}}\vec{x}}{\partial \vec{x}}=({\bf{A}}+{\bf{A}}^T)\vec{x}\quad\color{blueviolet}({\bf{A}}_{n\times{n}}不是\vec{x}_{n\times{1}}的函数)$ |                                          |

> *$\ $维度相容原则$\quad$`Trick`
>
> 通过**前后换序**、**转置** 使求导结果满足矩阵乘法且结果维数满足下式，那么结果就是正确的：
>
> $\qquad如果\vec{x}\in R^{m\times{n}},f(\vec{x})\in R^1，那么\dfrac{\partial f(\vec{x})}{\partial\vec{x}}\in R^{m\times{n}}$

<div STYLE="page-break-after: always;"></div>

##二、线性回归(Linear Regression)

###1. 模型表示

​	假设函数

​		$ h_θ(x)=θ_0+θ_1x\qquad\qquad\qquad\qquad\qquad$`单个特征`

​		$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n\quad\ $`多个特征`


![线性回归](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\线性回归.jpg)

###2. 代价函数(Cost function)

   $J(\theta_0,\theta_1)=\dfrac{1}{2m}\sum_{i=1}\limits^n(h_\theta(x^{(i)})^2-y^{(i)})^2\quad$`最小二乘代价函数`

   + [为什么线性回归使用最小二乘代价函数$J(\theta_0,\theta_1)?$](http://blog.csdn.net/Eric2016_Lv/article/details/52836182?locationNum=3&fps=1)

###3. 最小二乘法(LSE)

   $\hat{y}=\hat{a}+\hat{b}x$

   $\hat{b}=\dfrac{\sum_{i=1}\limits^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}\limits^n(x_i-\bar{x})^2}=\dfrac{\sum_{i=1}\limits^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}\limits^nx_i^2-n\bar{x}^2}$

   $\hat{a}=\bar{y}-\hat{b}\bar{x}$

   $其中\bar{x}、\bar{y}为x_i、y_i的均值$

   + [ 求回归直线方程的推导过程](http://blog.csdn.net/marsjohn/article/details/54911788)
   + [最小二乘法？为神马不是差的绝对值](http://blog.sciencenet.cn/blog-430956-621997.html)
   + `求解全局最优`
   + `数据量很大时，计算量大`

###4. 梯度下降(Gradient Descent)

   + 目标：$\mathop{minimize}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)$


   + `迭代法`

     + $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\quad$`α为学习率`

       $\mathop{\Longrightarrow}\limits^{线性回归}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_1:=\theta_1-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x^{(i)}}\end{array}\right.\quad$`同时更新θ₀和θ₁`

   + `每一步更新未知量，逐渐逼近局部最优解`

        ![梯度下降1](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\梯度下降1.png)

        ![梯度下降2](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\梯度下降2.png)

   + 如何确保梯度下降能正常工作

         + J(θ)应随着迭代不断减小
         + 使用较小的学习率*α*

   + 如何选取学习率*α*

         + 尝试：

              ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ...

        + 取值范围

             $0<\alpha<2(|X^TX|^{-1})$

   + 分类

        + Batch Gradient Descent

          $\qquad$每一步都使用**整个**训练集的数据计算梯度

        + Stochastic Gradient Descent

          $\Downarrow\quad\ $每一步仅用训练集中的**单个**计算梯度

        + Mini-batch Gradient Descent

          $\qquad$每一步仅用训练集中的**一小部分**（32/64/128/256）计算梯度

          > The way to go is therefore to use some acceptable (but not necessarily optimal) values for the other hyper-parameters, and then trial a number of different mini-batch sizes, scaling α as above. Plot the validation accuracy versus time (as in, real elapsed time, not epoch!), and choose whichever mini-batch size gives you the most rapid improvement in performance. With the mini-batch size chosen you can then proceed to optimize the other hyper-parameters.

          + 适用于存在较多局部最优解的情况，能跳出局部最优解

          + 计算量更小，计算速度更快

          + [BGD与SGD的不同](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent)

          > The applicability of batch or stochastic gradient descent really depends on the error manifold expected.
          >
          > Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.
          >
          > Stochastic gradient descent (SGD) computes the gradient using a single sample. Most applications of SGD actually use a minibatch of several samples, for reasons that will be explained a bit later. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. Single samples are really noisy, while minibatches tend to average a little of the noise out. Thus, the amount of jerk is reduced when using minibatches. A good balance is struck when the minibatch size is small enough to avoid some of the poor local minima, but large enough that it doesn't avoid the global minima or better-performing local minima. (Incidently, this assumes that the best minima have a larger and deeper basin of attraction, and are therefore easier to fall into.)
          >
          > One benefit of SGD is that it's computationally a whole lot faster. Large datasets often can't be held in RAM, which makes vectorization much less efficient. Rather, each sample or batch of samples must be loaded, worked with, the results stored, and so on. Minibatch SGD, on the other hand, is usually intentionally made small enough to be computationally tractable.
          >
          > Usually, this computational advantage is leveraged by performing many more iterations of SGD, making many more steps than conventional batch gradient descent. This usually results in a model that is very close to that which would be found via batch gradient descent, or better.
          >
          > The way I like to think of how SGD works is to imagine that I have one point that represents my input distribution. My model is attempting to learn that input distribution. Surrounding the input distribution is a shaded area that represents the input distributions of all of the possible minibatches I could sample. It's usually a fair assumption that the minibatch input distributions are close in proximity to the true input distribution. Batch gradient descent, at all steps, takes the steepest route to reach the true input distribution. SGD, on the other hand, chooses a random point within the shaded area, and takes the steepest route towards this point. At each iteration, though, it chooses a new point. The average of all of these steps will approximate the true input distribution, usually quite well.

###5. 向量化
|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | ---------------------------------------- |
|     feature      | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|      output      | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}$ |
|        θ         | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
|    hypothesis    | $h=X\theta=\begin{pmatrix}  \sum\limits_{i=0}^n\theta_ix_i^{(1)}\\ \sum\limits_{i=0}^n\theta_ix_i^{(2)}\\...\\ \sum\limits_{i=0}^n\theta_ix_i^{(m)} \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |
|  cost function   | $J=\dfrac{1}{2m}(h-y)^T(h-y)$            |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |
###6. 特征缩放(Feature Scaling) 

+ 确保所有特征在相似的规模上（$-1\leq{x_i}\leq{1}$）

+ 提升梯度下降速度

  ![特征缩放](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\特征缩放.png)

+ 方法

  + **mean normalization**

    $x_i'=\dfrac{x_i-\mu_i}{max(x)-min(x)}$

    $其中\mu_i为x的均值$

  + **Standardization**

    $x_i'=\dfrac{x-\bar{x}}{\sigma}$

    $其中\sigma为x的标准差$
###6. 正规方程(Normal Equation) 

+ 公式

  $\theta=(X^TX)^{-1}X^Ty$

  推导过程

+ 直接求解参数*θ*的最优值，不需要多次迭代 $\quad$`解析方法`

+ 不需要特征缩放

+ 不需要选择学习率*α*



> `问题`
>
> A.  特征数很大时计算量大（n≥10000) $\quad$`求解逆矩阵的时间复杂度O(n³)`
>
> B.  只适用于线性模型
>
> C.  $X^TX$可能不可逆
>
> + 冗余特征：存在线性相关的特征
>
> + 太多特征：特征数>样本数，导致过拟合
>
>   $\mathop{\Longrightarrow}\limits^{处理方法}\left\{\begin{array}{lcl}删除部分特征\left\{\begin{array}{lcl}手动选择\\模型选择算法\end{array}\right.\\正则化(\mbox{Regularization})\end{array}\right.$

### 7. 正则化(Normalization)

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=\dfrac{1}{2m}[\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2{\color{salmon}+\lambda\sum\limits_{j=1}^n\theta_j^2}]$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

  $\qquad\qquad\qquad\qquad\qquad\qquad\Downarrow$

  $\qquad\qquad\theta_j:=\theta_j{\color{salmon}(1-\alpha\dfrac{\lambda}{m})}-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}$$\qquad\qquad\qquad\qquad\qquad\scriptsize\color{maroon}[注]\ 1-\alpha\dfrac{\lambda}{m}<1$

  ​

  `正规方程`

  $\qquad\qquad\theta=(X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}_{(n+1)\times{(n+1)}})^{-1}X^Ty$

  ​

  $\qquad\qquad\color{maroon}[注]\ 当\lambda>0时，可以证明X^TX+\lambda\begin{pmatrix}  0&0&0&...&0\\0&1&0&...&0\\0&0&1&...&0\\...&...&...&...&...\\0&0&0&...&1 \end{pmatrix}是可逆的$

  > 建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍（增减10倍是粗调节，当确定了λ的合适的数量级后，比如λ = 0.01,再进一步地细调节，比如调节为0.02，0.03，0.009之类。）

### 8. 牛顿法

+ ​

### 9. 最速下降法

+ ​

###10. 局部加权线性回归    `非参数学习方法` 

​	Ⅰ $\quad\ $Fit *θ* to minimize $\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$

​	Ⅱ $\quad$Output $\theta^Tx$

$w^{(i)}=e^{-\dfrac{(x^{(i)}-x)^2}{2\tau^2}}$，其中τ为常数，可调节$w^{(i)}$钟形曲线的宽度

+ 当$|x^{(i)}-x|\approx{0}$时，$w^{(i)}\approx{1}$，即预测样本举例训练样本越近，权值越大
+ 当$|x^{(i)}-x|\approx{+\infty}​$时，$w^{(i)}\approx{0}​$，即预测样本举例训练样本越远，权值越小

> `问题`
>
> A.  当数据规模比较大时，计算量很大，学习效率很低
>
> B.  不一定能避免欠拟合

<div STYLE="page-break-after: always;"></div>


##三、逻辑回归(Logistic Regression)

###1. 模型表示

> `分类`
>
> $\quad y=0\ \mbox{or} \ 1$
>
> $\quad h_\theta(x)\ \mbox{can be}\ >1\ \mbox{or}\ <0$
>
> $\color{brown}\Downarrow逻辑函数$
>
> `逻辑回归`
>
> $\quad0\leq{h_\theta(x)}\leq{1}\quad$

假设函数$h_\theta(x)=g(\theta^Tx)=\dfrac{1}{1+e^{-\theta^Tx}}\quad$`在线性回归模型的基础上加了一个Sigmoid函数 `

其中$g(z)=\dfrac{1}{1+e^{-z}}\quad$`sigmoid函数（S型曲线）`

​        $g'(z)=\dfrac{e^{(-z)}}{(1+e^{(-z)})^2}=\dfrac{1}{1+e^{(-z)}}(1-\dfrac{1}{1+e^{(-z)}})=g(z)(1-g(z))$

###2. 代价函数(Cost function)

> 线性回归的代价函数在逻辑回归模型中是非凸函数('"non-convex"')，不容易收敛到全局最优点，故引入新的代价函数

$J(\theta)=\dfrac{1}{m}\sum\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)})=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$

$\begin{equation}\begin{aligned}Cost(h_\theta(x^{(i)}),y^{(i)})&=\left\{\begin{array}{lcl}-log(h_\theta(x^{(i)})),\ \ \ \ \ \ \ \ \ y^{(i)}=1\\-log(1-h_\theta(x^{(i)})),\ \ y^{(i)}=0\end{array}\right.\\ &\color{maroon}=-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))\end{aligned}\end{equation}$

###3. 梯度下降(Gradient Descent) 

+ 目标：$\mathop{minimize}\limits_{\theta}J(\theta)$

+ 公式

  $\begin{equation}\begin{aligned}\theta_j&:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)\\ &:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot{x_j^{(i)}}\end{aligned}\end{equation}$

+ 优化算法

  `不需要手动选择学习率α`

  `通常比梯度下降更快`

  `但算法更复杂`

  + Conjugate  gradient 
  + BFGS 
  + L-­BFGS 

###4. 向量化

|        变量        | 向量表示（$其中m为训练样本数，n为特征数$）                  |
| :--------------: | :--------------------------------------- |
|     feature      | $X=\begin{pmatrix}\color{salmon}1&x_1^{(1)}&x_2^{(1)}&...&x_n^{(1)}\\ \color{salmon}1&x_1^{(2)}&x_2^{(2)}&...&x_n^{(2)}\\\color{salmon}...&...&...&...&...\\\color{salmon}1&x_n^{(m)}&x_2^{(m)}&...&x_n^{(m)} \end{pmatrix}_{m\times{(n+1)}}$`人为地添加一列全1，即x₀` |
|      output      | $y=\begin{pmatrix} y^{(1)}\\ y^{(2)}\\...\\ y^{(m)} \end{pmatrix}_{m\times{1}}\quad$`每一项取值0或1` |
|        θ         | $\theta=\begin{pmatrix}  \theta_0\\\theta_1\\ \theta_2\\...\\ \theta_n \end{pmatrix}_{(n+1)\times{1}}$ |
|    hypothesis    | $h=g(X\theta)=\begin{pmatrix}  g(\sum\limits_{i=0}^n\theta_ix_i^{(1)})\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(2)})\\...\\ g(\sum\limits_{i=0}^n\theta_ix_i^{(m)}) \end{pmatrix}_{m\times{1}}=\begin{pmatrix} \hat{y}^{(1)}\\ \hat{y}{(2)}\\...\\ \hat{y}{(m)} \end{pmatrix}_{m\times{1}}$ |
|  cost function   | $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]$ |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |

###5. 正则化(Normalization)

保留所有特征，但减小参数$\theta_j$的量级/值

`引入正则化参数λ`

`特征较多时效果较好`

+ 公式

  $J(\theta)=-\dfrac{1}{m}[\sum\limits_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]{\color{salmon}+\dfrac{\lambda}{2m}\sum\limits_{j=1}^n\theta_j^2}$

  $\theta_j:=\theta_j-\alpha\dfrac{\partial}{\partial\theta_j}J(\theta)$

  $\quad\mathop{\Longrightarrow}\left\{\begin{array}{lcl}\theta_0:=\theta_0-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\\\theta_j:=\theta_j-\alpha[\dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)}))\cdot{x_j^{(i)}}{\color{salmon}+\dfrac{\lambda}{m}\theta_j}]\qquad (j=1,2,3,...,n)\end{array}\right.\quad$`同时更新所有θ`

+ 向量化

  $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]+\dfrac{\lambda}{2m}(\theta^T\theta-\theta_0^2)$

  $\theta:=\theta-\dfrac{\alpha}{m}[X^T(h-y)+\lambda\theta]$


####* 范数(norm)

- L0范数：向量中非0的元素的个数
  - L0范数可以实现稀疏

  - L1范数：向量中各个元素绝对值之和\quad$`稀疏规则算子`

  - L1范数会使权值稀疏

  - L2范数：向量各元素的平方和然后求平方根$\quad$`权值衰减`

  - L2范数可以防止过拟合，提升模型的泛化能力

    L1正则化：$\lambda\Vert\theta\Vert\quad\ \ $`套索回归Lasso`

    L2正则化：$\lambda\Vert\theta\Vert^2\quad$`岭回归Ridge`

###6. 多元分类

> $y\in{0,1,2,...,n}$
>
> $\left\{\begin{array}{lcl}h_\theta^{(0)}(x)=P(y=0|x;\theta)\\h_\theta^{(1)}(x)=P(y=1|x;\theta)\\...\\h_\theta^{(n)}(x)=P(y=n|x;\theta)\end{array}\right.$

$\mbox{prediction}=\mbox{max}(h_\theta^{(i)}(x))$

<div STYLE="page-break-after: always;"></div>

##四、神经网络

###1. 模型表示 

+ $L=网络总层数=输入层+隐藏层+输出层$

+ $s_l=第l层中的单元数（不包含偏置单元）$

+ $K=输出单元个数/类别数\quad\mathop{\Longrightarrow}\limits^{二分类}\quad K=1$

+ $(h_\Theta(x^{(i)}))_k=第k个输出$

+ $a^{(j)}=第j层的激励(\mbox{activation})\ \mathop{\Longrightarrow}\limits^{维度}\ (s_j+1)\times{1}$

  $a_i^{(j)}=第j层第i个单元的激励$

+ $\Theta^{(j)}=从第j层映射到第j+1层的权重矩阵\ \mathop{\Longrightarrow}\limits^{维度}\ s_{j+1}\times{(s_j+1)}$

$\quad\ \ \ \Theta_{kn}^{(j)}=从第j层映射到第j+1层第k个单元的第n个参数(权重)$

![神经网络](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络.png)



####`网络架构`

+ 输入层→隐藏层→输出层

![神经网络层次](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络层次.jpeg)

###2. 非线性分类：逻辑运算

`激活函数取sigmoid函数`

+ or运算

  $h_\Theta(x)=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![or](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\or.png)

+ and运算

  $h_\Theta(x)=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![and](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\and.png)

+ not运算

  $h_\Theta(x)=g(10-20x)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x=0\\\approx0&x=1\end{array}\right.$

  ![not](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\not.png)

+ xor运算

  $h_\Theta(x)=a_1^{(3)}=g(-30+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(30-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(-10+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx1&x_1=0,x_2=1\\\approx1&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  ![xor](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\xor.png)

+ xnor运算

  $h_\Theta(x)=a_1^{(3)}=g(-10+20a_1^{(2)}+20a_2^{(2)})\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_1^{(2)}=g(-30+20x_1+20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx1&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx0&x_1=0,x_2=0\end{array}\right.$

  $\qquad a_2^{(2)}=g(10-20x_1-20x_2)\Longrightarrow\left\{\begin{array}{lcl}\approx0&x_1=1,x_2=1\\\approx0&x_1=0,x_2=1\\\approx0&x_1=1,x_2=0\\\approx1&x_1=0,x_2=0\end{array}\right.$

  ![xnor](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\xnor.png)

###3. 分类

#### · 二分类

$\qquad y=0\ or\ 1$

#####		· `感知机(perception)`

$\qquad$假设空间是特征空间中的所有线性分类模型，即找到一个线性方程$x\cdot{w}+b=0$，它对应于特征空间的一个

$\qquad$超平面，能够把对应的特征空间分为两部分，位于两部分中的点分别是正负两类。

####· 多元分类

$\qquad y^{(i)}\in one\ of\begin{pmatrix}1\\0\\0\\0\end{pmatrix},\begin{pmatrix}0\\1\\0\\0\end{pmatrix},\begin{pmatrix}0\\0\\1\\0\end{pmatrix},\begin{pmatrix}0\\0\\0\\1\end{pmatrix}$

![多元分类](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\多元分类.png)

###4. 代价函数(Cost function)

`逻辑回归中代价函数的一般形式`

$J(\Theta)=-\dfrac{1}{m}[\sum\limits_{i=1}^m\sum\limits_{k=1}^Ky_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)log(1-h_\Theta(x^{(i)}))_k]+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{j+1}}(\Theta_{ji}^{(l)})^2$

### 5. 反向传播算法(Backpropagation algorithm)

`梯度下降`$\quad$`迭代法`

![反向传播](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\反向传播.png)

+ Ⅰ$\quad\ $前向传播$\quad$`计算各层输出`

  ![神经网络](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\神经网络.jpeg)

  > $a^{(1)}=x$
  >
  > $z^{(2)}=\Theta^{(1)}a^{(1)}$
  >
  > $a^{(2)}=g(z^{(2)})\quad(\mbox{add}\ a_0^{(2)})$
  >
  > $z^{(3)}=\Theta^{(2)}a^{(2)}$
  >
  > $a^{(3)}=g(z^{(3)})\quad(\mbox{add}\ a_0^{(3)})$
  >
  > $z^{(4)}=\Theta^{(3)}a^{(3)}$
  >
  > $a^{(4)}=h_\Theta(x)=g(z^{(4)})$

+ Ⅱ$\quad$反向传播$\quad$`误差逆传播`

  > 若输入层的实际输出$h_\Theta(x)$与期望的输出$y$不符，则进行 误差的反向传播
  >
  > 直到网络输出的误差减少到了可以接受的	程度（或 进行到预先设定的学习次数为止）

  `偏置单元可以计入δ，也可不计入`

$\nabla_{\Theta^{(l)}}J(\Theta)=\dfrac{\partial J(\Theta)}{\partial z^{(l+1)}}\dfrac{\partial z^{(l+1)}}{\partial \Theta^{(l)}}=\delta^{(l+1)}(a^{(l)})^T$

$\delta^{(l)}=\dfrac{\partial J}{\partial z^{(l)}}=\dfrac{\partial J}{\partial z^{(l+1)}}\dfrac{\partial z^{(l+1)}}{\partial a^{(l)}}\dfrac{\partial a^{(l)}}{\partial z^{(l)}}=(\Theta^{(l)})^T\delta^{(l+1)}\cdot{g'(z^{(l)})}=(\Theta^{(l)})^T\delta^{(l+1)}\cdot{a^{(l)}(1-a^{(l)})}$

$\delta_j^{(l)}=第l层第j个单元的{\color{blueviolet}\textbf{误差}}=\left\{\begin{array}{lcl}a_j^{(l)}-y_j=(h_\Theta(x))_j-y_j&\color{salmon}输出层&(l=L)\\a_j^{(l)}\cdot(1-a_j^{(l)})&\color{salmon}隐藏层&(1<l<L)\end{array}\right.\quad$

$\quad\\\qquad\mathop{\Longrightarrow}\limits^{忽略正则项\lambda}\ \dfrac{\partial}{\partial\Theta_{ji}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}$

$\qquad\qquad\mathop{\Longrightarrow}\limits^{二分类}\left\{\begin{array}{lcl}cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))\\\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)}} cost(t)\\\dfrac{\partial}{\partial\Theta_{ji}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}\end{array}\right.$





###6. 向量化

|        变量        | 向量表示（$其中m为训练样本数，K为类别数，L为网络层数，s_l为第l层单元数$） |
| :--------------: | :--------------------------------------- |
|     feature      | $x^{(i)}=\begin{pmatrix} x_1^{(i)}\\ x_2^{(i)}\\...\\ x_{s_L}^{(i)}\end{pmatrix}_{s_L\times{1}}\quad(1\le i\le m)$ |
|      output      | $y^{(i)}=\begin{pmatrix} y_1^{(i)}\\ y_2^{(i)}\\...\\ y_{s_L}^{(i)}\end{pmatrix}_{s_L\times{1}}\quad(1\le i\le m)\quad$`每一项取值0或1` |
|  $\Theta^{(l)}$  | $\Theta^{(l)}=\begin{pmatrix}  \theta_{10}^{(l)}&\theta_{11}^{(l)}&\theta_{12}^{(l)}&...&\theta_{1s_l}^{(l)}\\\theta_{20}^{(l)}&\theta_{21}^{(1)}&\theta_{22}^{(l)}&...&\theta_{2s_l}^{(l)}\\\theta_{30}^{(l)}&\theta_{31}^{(l)}&\theta_{32}^{(l)}&...&\theta_{3s_l}^{(l)}\\...&...&...&...&...\\ \theta_{s_{l+1}0}^{(l)}&\theta_{s_{l+1}1}^{(l)}&\theta_{s_{l+1}2}^{(l)}&...&\theta_{s_{l+1}s_l}^{(l)} \end{pmatrix}_{s_{l+1}\times{(s_l+1)}}=\begin{pmatrix}(\vec\theta_1^{(l)})^T\\(\vec\theta_2^{(l)})^T\\(\vec\theta_3^{(l)})^T\\...\\(\vec\theta_{s_{l+1}}^{(l)})^T\end{pmatrix}$ |
|    activation    | $a^{(1)}=x\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \color{salmon}(l=1)\\\quad\\a^{(l)}=g(\Theta^{(l-1)}a^{(l-1)})=\begin{pmatrix}1\\g(\sum\limits_{k=0}^K\Theta_{1k}^{(l-1)}a_k^{(l-1)})\\g(\sum\limits_{k=0}^K\Theta_{2k}^{(l-1)}a_k^{(l-1)})\\...\\g(\sum\limits_{k=0}^K\Theta_{s_lk}^{(l-1)}a_k^{(l-1)})\end{pmatrix}_{(s_l+1)\times{1}}=\begin{pmatrix}a_0^{(l)}\\a_1^{(l)}\\a_2^{(l)}\\...\\a_{s_l}^{(l)}\end{pmatrix}\qquad \color{salmon}(2\le l\le L)$ |
|      error       | $\delta^{(L)}=a^{(L)}-y=h_\Theta(x)-y\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ \ \color{salmon}(l=L)\\\quad\\ \begin{equation}\begin{aligned}\delta^{(l)}&=((\Theta^{(l)})^T \delta^{(l+1)})\ .*g'(z^{(l)})=((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\\&=\begin{pmatrix}\sum\limits_{i=1}^{s_{l+1}}\Theta_{i1}^{(l)}\delta_i^{(i+1)}\cdot{a_1^{(l)}}\cdot{(1-a_1^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i2}^{(l)}\delta_i^{(i+1)}\cdot{a_2^{(l)}}\cdot{(1-a_2^{(l)})}\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{i3}^{(l)}\delta_i^{(i+1)}\cdot{a_3^{(l)}}\cdot{(1-a_3^{(l)})}\\...\\\sum\limits_{i=1}^{s_{l+1}}\Theta_{is_l}^{(l)}\delta_i^{(i+1)}\cdot{a_{s_l}}^{(l)}\cdot{(1-a_{s_l}^{(l)})}\end{pmatrix}_{s_l\times{1}}\end{aligned}\end{equation}\color{salmon}\qquad(2\le l\le L-1)\\\color{blueviolet}(未计入偏置单元，故公式中\Theta^{(l)}的维度应为s_{l+1}\times{s_l}，a^{(l)}的维度应为s_l\times{1})$ |
| cost function(例) | $J=-\dfrac{1}{m}[y^Tlog(h)+(1-y)^Tlog(1-h)]$ |
| Gradient descent | $\theta:=\theta-\dfrac{\alpha}{m}X^T(h-y)$ |

### 7. 梯度检查

![GradientCheckQuestion](C:\Users\Emrys\Documents\Dian\Machine Learning\notes\GradientCheckQuestion.png)

### 8. 随机初始化

###9. 训练神经网络的步骤

1. 选择一个网络架构$\quad$`神经元之间的连接样式`

   + 输入单元的数量：特征$x^{(i)}$的维度

   + 输出单元的数量：分类数

     > 合理的默认选择：1个隐藏层，或若有多个隐藏层，则在每一层中有相同数量的隐藏单元（通常越多越好）

2. 训练过程

   1. 随机初始化权重(参数)

   2. 利用正向传播方法计算所有输入$x^{(i)}$的$h_\Theta(x^{(i)})$

   3. 编写计算代价函数$J(\Theta)$ 的代码

   4. 利用反向传播方法计算偏导数$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$

   5. 利用数值检验方法比较用反向传播计算的和用$J(\Theta)$的梯度的数值估计得到的$\dfrac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$

      > 然后禁用梯度检查的代码，因为梯度检查速度很慢

   6. 使用梯度下降或优化算法与反向传播算法相结合，来使$J(\Theta)$最小化

   > $J(\Theta)$是非凸函数，因此梯度下降可能得到一个局部最优点

###* Geoffrey Hinton关于BP的看法